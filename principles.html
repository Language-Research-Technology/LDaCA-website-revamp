<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>LDaCA</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/c09165cc504eb607.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c09165cc504eb607.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-f4ae3437c92c1efc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c12b0cd12edac7a3.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bpage%5D-d2184b8ab241f292.js" defer=""></script><script src="/_next/static/AWBu_R8MsSXbxYahFIoQG/_buildManifest.js" defer=""></script><script src="/_next/static/AWBu_R8MsSXbxYahFIoQG/_ssgManifest.js" defer=""></script><script src="/_next/static/AWBu_R8MsSXbxYahFIoQG/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div><div class="flex min-h-screen flex-col"><div class="sticky top-0 h-20 bg-white shadow-sm md:static md:shadow-none lg:h-36"><nav class="container flex h-full items-center justify-between text-gray-700"><a href="/"><img src="/logo.png" class="h-16 lg:h-28"/></a><div class="hidden items-center space-x-2 md:flex"><ul class="flex divide-x divide-slate-400 text-sm"><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/"><li>Home</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/posts"><li>Blog</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/background"><li>Background</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/resources"><li>Resources</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/organisation"><li>Organisation</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/contact"><li>Contact us</li></a></ul><div class="relative flex"><input placeholder="Page name" class="w-0 mr-2 border transition-all" value=""/><button><svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button></div></div><div class="md:hidden"><button class="relative h-10 w-10 text-gray-500 focus:outline-none"><span class="sr-only">Open main menu</span><div class="absolute left-1/2 top-1/2 block w-5 -translate-x-1/2 -translate-y-1/2 transform"><span aria-hidden="true" class="absolute block h-0.5 w-5 transform bg-current transition duration-500 ease-in-out rounded-sm -translate-y-1.5"></span><span aria-hidden="true" class="absolute block h-0.5 w-5 transform bg-current transition duration-500 ease-in-out rounded-sm false"></span><span aria-hidden="true" class="absolute block h-0.5 w-5 transform bg-current transition duration-500 ease-in-out rounded-sm translate-y-1.5"></span></div></button></div></nav><div></div></div><main class="container flex-1 py-8"><article class="prose max-w-none lg:prose-xl"><p>The Language Data Commons of Australia aims to ensure the long-term access to language data collections for analysis and reuse. Sustainable management of and access to these significant collections of intangible cultural heritage are underpinned by two sets of complementary guiding principles of data management and stewardship, namely the FAIR and CARE principles.
<br /></p>
<h2 id="id1">FAIR Principles</h2>
<br />
<p>The FAIR principles were <a href="https://www.nature.com/articles/sdata201618">first published</a> by a group of stakeholders representing academia, industry, funding agencies, and scholarly publishers. The principles aim to address issues surrounding data management and stewardship, focusing on four areas (which provide the FAIR acronym).
<br /></p>
<h3 id="id2">Findable</h3>
<p>Metadata and data should be easy to find for both humans and computers. Making the data findable includes:</p>
<ul>
<li>(Meta)data are assigned a globally unique and persistent identifier</li>
<li>Data are described with rich metadata</li>
<li>Metadata clearly and explicitly include the identifier of the data they
describe.</li>
<li>(Meta)data are registered or indexed in a searchable resource.
<br />
</li>
</ul>
<h3 id="id3">Accessible</h3>
<p>Once the user finds the required data, she/he/they need to know how they can be accessed, possibly including authentication and authorisation.</p>
<ul>
<li>(Meta)data are retrievable by their identifier using a standardised communications protocol
<ul>
<li>The protocol is open, free, and universally implementable</li>
<li>The protocol allows for an authentication and authorisation procedure, where necessary</li>
</ul>
</li>
<li>Metadata are accessible, even when the data are no longer available
<br />
</li>
</ul>
<h3 id="id4">Interoperable</h3>
<p>The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.</p>
<ul>
<li>(Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation</li>
<li>(Meta)data use vocabularies that follow FAIR principles</li>
<li>(Meta)data include qualified references to other (meta)data
<br />
</li>
</ul>
<h3 id="id5">Reusable</h3>
<p>The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings.</p>
<ul>
<li>(Meta)data are richly described with a plurality of accurate and relevant attributes
<ul>
<li>(Meta)data are released with a clear and accessible data usage license</li>
<li>(Meta)data are associated with detailed provenance</li>
<li>(Meta)data meet domain-relevant community standards
<br />
</li>
</ul>
</li>
</ul>
<br />
<p>The Australian Research Data Commons (<a href="https://ardc.edu.au/">ARDC</a>) supports FAIR data practices and initiatives that make data and related research outputs FAIR. At the same time, the ARDC acknowledges that the implementation of the principles will look different across disciplines and will need discipline-specific approaches and standards.</p>
<h2 id="id6">CARE Principles</h2>
<p>The <a href="https://www.gida-global.org/care">CARE Principles for Indigenous Data Governance</a> were developed by the Global Indigenous Alliance (GIDA); they are a response to the FAIR principles and aim to complement them. GIDA highlights how the FAIR principles and the open data movement focus on increasing data sharing among researchers and entities but do not take into account power differentials and historical contexts or fully engage with Indigenous Peoples’ rights and interests. These include the rights to generate value from Indigenous data in ways that are grounded in Indigenous worldviews and to advance Indigenous innovation and self-determination. The CARE Principles also focus on four areas.
<br /></p>
<h3 id="id7">Collective Benefit</h3>
<p>Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data.</p>
<ul>
<li>For inclusive development and innovation</li>
<li>For improved government and citizen engagement</li>
<li>For equitable outcomes
<br />
</li>
</ul>
<h3 id="id8">Authority to control</h3>
<p>Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered.</p>
<ul>
<li>Recognizing rights and interests</li>
<li>Data for governance</li>
<li>Governance of data.
<br />
</li>
</ul>
<h3 id="id9">Responsibility</h3>
<p>Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit.</p>
<ul>
<li>For positive relationships</li>
<li>For expanding capability and capacity</li>
<li>For Indigenous languages and worldviews.
<br />
</li>
</ul>
<h3 id="id10">Ethics</h3>
<p>Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.</p>
<ul>
<li>For minimizing harm and maximizing benefit</li>
<li>For justice</li>
<li>For future use.</li>
</ul>
<p>Back to <a href="../background/">Background</a></p>
</article></main><div class="bg-secondary"><div class="container flex h-10 items-center justify-center text-white"><small>Ⓒ 2022, <!-- --><a href="/">Language Data Commons of Australia</a></small></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"content":"\u003cp\u003eThe Language Data Commons of Australia aims to ensure the long-term access to language data collections for analysis and reuse. Sustainable management of and access to these significant collections of intangible cultural heritage are underpinned by two sets of complementary guiding principles of data management and stewardship, namely the FAIR and CARE principles.\n\u003cbr /\u003e\u003c/p\u003e\n\u003ch2 id=\"id1\"\u003eFAIR Principles\u003c/h2\u003e\n\u003cbr /\u003e\n\u003cp\u003eThe FAIR principles were \u003ca href=\"https://www.nature.com/articles/sdata201618\"\u003efirst published\u003c/a\u003e by a group of stakeholders representing academia, industry, funding agencies, and scholarly publishers. The principles aim to address issues surrounding data management and stewardship, focusing on four areas (which provide the FAIR acronym).\n\u003cbr /\u003e\u003c/p\u003e\n\u003ch3 id=\"id2\"\u003eFindable\u003c/h3\u003e\n\u003cp\u003eMetadata and data should be easy to find for both humans and computers. Making the data findable includes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(Meta)data are assigned a globally unique and persistent identifier\u003c/li\u003e\n\u003cli\u003eData are described with rich metadata\u003c/li\u003e\n\u003cli\u003eMetadata clearly and explicitly include the identifier of the data they\ndescribe.\u003c/li\u003e\n\u003cli\u003e(Meta)data are registered or indexed in a searchable resource.\n\u003cbr /\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"id3\"\u003eAccessible\u003c/h3\u003e\n\u003cp\u003eOnce the user finds the required data, she/he/they need to know how they can be accessed, possibly including authentication and authorisation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(Meta)data are retrievable by their identifier using a standardised communications protocol\n\u003cul\u003e\n\u003cli\u003eThe protocol is open, free, and universally implementable\u003c/li\u003e\n\u003cli\u003eThe protocol allows for an authentication and authorisation procedure, where necessary\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMetadata are accessible, even when the data are no longer available\n\u003cbr /\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"id4\"\u003eInteroperable\u003c/h3\u003e\n\u003cp\u003eThe data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation\u003c/li\u003e\n\u003cli\u003e(Meta)data use vocabularies that follow FAIR principles\u003c/li\u003e\n\u003cli\u003e(Meta)data include qualified references to other (meta)data\n\u003cbr /\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"id5\"\u003eReusable\u003c/h3\u003e\n\u003cp\u003eThe ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(Meta)data are richly described with a plurality of accurate and relevant attributes\n\u003cul\u003e\n\u003cli\u003e(Meta)data are released with a clear and accessible data usage license\u003c/li\u003e\n\u003cli\u003e(Meta)data are associated with detailed provenance\u003c/li\u003e\n\u003cli\u003e(Meta)data meet domain-relevant community standards\n\u003cbr /\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr /\u003e\n\u003cp\u003eThe Australian Research Data Commons (\u003ca href=\"https://ardc.edu.au/\"\u003eARDC\u003c/a\u003e) supports FAIR data practices and initiatives that make data and related research outputs FAIR. At the same time, the ARDC acknowledges that the implementation of the principles will look different across disciplines and will need discipline-specific approaches and standards.\u003c/p\u003e\n\u003ch2 id=\"id6\"\u003eCARE Principles\u003c/h2\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.gida-global.org/care\"\u003eCARE Principles for Indigenous Data Governance\u003c/a\u003e were developed by the Global Indigenous Alliance (GIDA); they are a response to the FAIR principles and aim to complement them. GIDA highlights how the FAIR principles and the open data movement focus on increasing data sharing among researchers and entities but do not take into account power differentials and historical contexts or fully engage with Indigenous Peoples’ rights and interests. These include the rights to generate value from Indigenous data in ways that are grounded in Indigenous worldviews and to advance Indigenous innovation and self-determination. The CARE Principles also focus on four areas.\n\u003cbr /\u003e\u003c/p\u003e\n\u003ch3 id=\"id7\"\u003eCollective Benefit\u003c/h3\u003e\n\u003cp\u003eData ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor inclusive development and innovation\u003c/li\u003e\n\u003cli\u003eFor improved government and citizen engagement\u003c/li\u003e\n\u003cli\u003eFor equitable outcomes\n\u003cbr /\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"id8\"\u003eAuthority to control\u003c/h3\u003e\n\u003cp\u003eIndigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRecognizing rights and interests\u003c/li\u003e\n\u003cli\u003eData for governance\u003c/li\u003e\n\u003cli\u003eGovernance of data.\n\u003cbr /\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"id9\"\u003eResponsibility\u003c/h3\u003e\n\u003cp\u003eThose working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor positive relationships\u003c/li\u003e\n\u003cli\u003eFor expanding capability and capacity\u003c/li\u003e\n\u003cli\u003eFor Indigenous languages and worldviews.\n\u003cbr /\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"id10\"\u003eEthics\u003c/h3\u003e\n\u003cp\u003eIndigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor minimizing harm and maximizing benefit\u003c/li\u003e\n\u003cli\u003eFor justice\u003c/li\u003e\n\u003cli\u003eFor future use.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBack to \u003ca href=\"../background/\"\u003eBackground\u003c/a\u003e\u003c/p\u003e\n","searchContent":{"posts":[{"title":"What are the FAIR and CARE principles and why should corpus linguists know about them?","slug":"fair-and-care","tags":["FAIR","CARE"],"content":"# FAIR and CARE\nData is becoming increasingly important in today’s world, so corpus linguists might feel that the rest of the world is finally catching up. But the rest of the world are bringing with them new approaches to how data is handled. This means that fields such as corpus linguistics may need to reassess their practices. Such reassessment includes addressing concerns about how data is stored and who can access it (data stewardship) – concerns that are a part of the [Open Science](https://en.wikipedia.org/wiki/Open_science) movement, ultimately grounded on principles of equity and accountability. \n\nThe most influential approach to data stewardship today is the [FAIR](https://www.go-fair.org/) principles.\nAccording to these principles, data should be:\n- *Findable* \n\u003cbr /\u003e\n\u0026emsp; Metadata and data should be easy to find for both humans and computers. \n- *Accessible*\n\u003cbr /\u003e\n\u0026emsp; Once the user finds the required data, she/he/they need to know how can they be accessed, possibly including authentication and authorisation.\n- *Interoperable*\n\u003cbr /\u003e\n\u0026emsp; The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.\n- *Reusable*\n\u003cbr /\u003e\n\u0026emsp; The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings.\n\u003cbr /\u003e\u003cbr /\u003e\n\nIn general, corpus linguists do well on the interoperability criterion. Corpus data is usually stored in non-proprietary formats; even when some structure is imposed on the data, this is almost always in a form which is saved as a simple text file (e.g. csv files or xml annotations). Data stored in such formats is easy to move between applications. But what about the other three criteria? \n\nSome corpus data is easy to discover; it is findable. For example CLARIN, the [portal](https://www.clarin.eu/content/data) to the European Union language resource infrastructure, provides access to many large data collections, as does the [Linguistic Data Consortium](https://www.ldc.upenn.edu/) in the USA. However, some data is never made part of a large collection and often remains under the control of individual researchers or research teams. Such data may be almost impossible to find. Even if we can find such data, it is unlikely to be accompanied by good descriptions of the data and metadata, making reusability problematic. Of course, big corpora such as the [British National Corpus](http://www.natcorp.ox.ac.uk/) will be both findable and accompanied by comprehensive corpus manuals. However, it is worth considering how to make other corpora more findable, including the provision of corpus manuals or corpus descriptions. Corpus resource databases such as [CoRD](https://varieng.helsinki.fi/CoRD/) do aim to work towards this principle.\n\nAccessibility may also be an issue for some data. Copyright law may allow use of material for individual research but prohibit any further distribution of the material. The FAIR approach to such cases is that metadata should be available so that interested parties can know that a data holding exists (F), and the metadata will include information about the conditions under which the data may or may not be shared or reused (A and R). \n\n![FAIR and CARE principles](/fair-care.png)\n\nImage from Global Indigenous Data Alliance (https://www.gida-global.org/)\n\u003cbr /\u003e\u003cbr /\u003e\nFor linguists, there is another very important set of principles concerning data, the CARE principles developed by the Global Indigenous Data Alliance:\n\u003cbr /\u003e\n- *Collective Benefit*\n\u003cbr /\u003e\n\u0026emsp; Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data.\n\u003cbr /\u003e\n- *Authority to control*\n\u003cbr /\u003e\n\u0026emsp;Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered.\n\u003cbr /\u003e\n- *Responsibility*\n\u003cbr /\u003e\n\u0026emsp;Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit.\n\u003cbr /\u003e\n- *Ethics*\n\u003cbr /\u003e\n\u0026emsp;Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.\n\n\u003cbr /\u003e\nThese principles are presented as applying particularly to Indigenous data, but we believe that researchers should adopt this approach in all cases where the people who participate in our research can be seen to have some moral rights in the information they have contributed. Respecting those moral rights should be demonstrated by recognising the participants’ authority to control how data is used, by seeking to ensure that participants derive benefit from use of the data, and by acting ethically and transparently in our relations with the participants. Deborah Cameron and her colleagues (Cameron et al 1993) raised similar issues almost 20 years ago, arguing that the imbalance of power in the relation between researchers and participants needed to be reduced. The CARE principles continue along this path, but go even further in explicitly returning power to the sources of information. \n\nCorpus data is often written language. We have already mentioned that copyright law is relevant to some such material, and that body of law protects at least some rights for the creators of the material. But corpus linguists also work with other kinds of data such as spoken language (spontaneous or produced as a response to some prompt) or written material produced by research participants according to some protocol. In such cases, ethical research practice should include addressing the issues raised by the CARE principles. Some aspects of this practice will fall under institutional ethics requirements (for example, thinking carefully about what permissions we request on consent forms), but other questions must be part of the relationship between the researcher and the research participants. Corpus linguists working with spoken, computer-mediated, or otherwise particularly sensitive data have been aware of at least some of these issues, but the CARE principles offer an opportunity to go further.\n\nAcquiring data for linguistic research takes effort and often that means money. It is therefore a good use of resources if any data we collect can be used by others. The FAIR principles provide a framework to make sharing and reusing data easier, and applying the CARE principles where relevant helps to ensure that our research has a sound ethical basis.\n\n\u003cbr /\u003e\n\u003chr /\u003e\n\u003cbr /\u003e\n\nNote: This post is based on the presentation ‘Advance Australia FAIR’,  given by Simon Musgrave and Michael Haugh to the 4th Forum on Englishes in Australia (LaTrobe University, August 27, 2021). \n\n\u003cbr /\u003e\n\nThanks to Leah Gustafson and Monika Bednarek for helpful comments on drafts.\n\n\u003cbr /\u003e\n\n**Reference:**\nCameron, Deborah, Elizabeth Frazer, Penelope Harvey, Ben Rampton \u0026 Kay Richardson. 1993. Ethics, advocacy and empowerment: Issues of method in researching language. Language \u0026 Communication 13(2). 81–94. [https://doi.org/10.1016/0271-5309(93)90001-4](https://doi.org/10.1016/0271-5309(93)90001-4)\n\n\n\n"},{"title":"A CARE and FAIR-ready distributed access control system for human-created data using the Australian Access Federation and beyond\n","slug":"fair-care-eresearch-2022","content":"\n\nThis work is licensed under a \u003ca rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"\u003eCreative Commons Attribution 4.0 International License\u003c/a\u003e.\n\n\u003ca href=\"/fair-care-eresearch-2022/fair-care-eresearch-2022.pdf\"\u003eDownload as PDF\u003c/a\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide00.png' title='Slide: 0' border='1'  width='85%%'/\u003e\n\nThis is write-up of a talk given at eResearch Australasia 2022, delivered by Peter Sefton, with some additional detail.\n\nBy: Peter Sefton, Jenny Fewster, Moises Sacal Bonequi, Cale Johnstone, Catherine Travis, River Tae Smith, Patrick Carnuccio\n\nEdited by: Simon Musgrave\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide01.png' alt='Project Team(alphabetical order) Michael D’Silva  Marco Fahmi Leah Gustafson  Michael Haugh Cale Johnstone  Kathrin Kaiser  Sara King  Marco La Rosa  Mel Mistica  Simon Musgrave  Joel Nothman  Moises Sacal  Martin Schweinberger  PT Sefton   With thanks for their contribution: Partner Institutions: ' title='Slide: 1' border='1'  width='85%%'/\u003e\n\nThe Language Data Commons of Australia Data Partnerships (LDaCA) and the Australian Text Analytics Platform (ATAP) are building towards a scalable and flexible language data and analytics commons. These projects will be part of the Humanities and Social Sciences Research Data Commons (HASS RDC).\n\nThe Data Commons will focus on preservation and discovery of distributed multi-modal language data collections under a variety of governance frameworks. This will include access control that reflects ethical constraints and intellectual property rights, including those of Aboriginal and Torres Strait Islander, migrant and Pacific communities.\n\nThe platform will provide workbench services to support computational research, starting with code-notebooks with no-code research tools provided in later phases. Research artefacts such as code and derived data will be made available as fully documented research objects that are re-runnable and rigorously described. Metrics to demonstrate the impact of the platform are projected to include usage statistics, data and article citations. These projects are led by Professor Michael Haugh of the School of Languages and Culture at the University of Queensland with several partner institutions.\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide02.png' alt='The Language Data Commons of Australia (LDaCA) and Australian Text Analytics Platform (ATAP) projects received investment (https://doi.org/10.47486/DP768 and https://doi.org/10.47486/PL074) from the Australian Research Data Commons (ARDC). The ARDC is funded by the National Collaborative Research Infrastructure Strategy (NCRIS). ' title='Slide: 2' border='1'  width='85%%'/\u003e\n\nThis work is supported by the Australian Research Data Commons.\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide03.png' alt='  ' title='Slide: 3' border='1'  width='85%%'/\u003e\n\nLast year at eResearch Australasia, the Language Data Commons of Australia (LDaCA) team presented a design for a distributed access control system which could look after the A-is-for-accessible in FAIR data; in this presentation we describe and demonstrate a pilot system based on that design, showing how data licenses that allow access by identified groups of people to language data collections can be used with an AAF pilot system (CILogon) to give the right people access to data resources.\n\nThe ARDC have invested in a pilot of this work as part of the HASS Research Data Commons and Indigenous Research Capability Program integration activities.\n\nThe system has to be able to implement data access policies with real-world complexity and one of our challenges has been developing a data access policy that works across a range of different collections of language data. Here we present a pilot data access policy that we have developed, describing how this policy captures the decisions that must be made by a range of data providers to ensure data accessibility that complies with diverse legal, moral and ethical considerations.\nWe will discuss how the [CARE] and [FAIR] principles underpin this work, and compare this work to other projects such as [CADRE], which promise to deliver more complex solutions in the future. Initial work is with collections curated in a research context but we will also address community access to these resources.\n\nThe idea is to separate safe storage of data from its delivery. Each item in a repository is stored with licensing information in natural language (English at the moment, but could be other languages) and the repository defers access decisions to an Authorization system, where data custodians can design whatever process they like for granting license access. This can range from simple click-through licenses where anyone can agree to license terms, to detailed multi-step workflows where applicants are vetted based on whatever criteria the rights holder wishes; qualifications, membership of a cultural group, have they paid a subscription fee, etc\n\n[fair]: https://www.nature.com/articles/sdata201618\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide04.png' alt='  ' title='Slide: 4' border='1'  width='85%%'/\u003e\n\nRegarding rights, our project is informed by the [CARE] principles for Indigenous data which also describe the level of respect which should be given to any data collected from individuals or communities.\n\n\u003e The current movement toward open data and open science does not fully engage with Indigenous Peoples rights and interests. Existing principles within the open data movement (e.g. FAIR: findable, accessible, interoperable, reusable) primarily focus on characteristics of data that will facilitate increased data sharing among entities while ignoring power differentials and historical contexts. The emphasis on greater data sharing alone creates a tension for Indigenous Peoples who are also asserting greater control over the application and use of Indigenous data and Indigenous Knowledge for collective benefit\n\n[care]: https://www.gida-global.org/care\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide05.png' alt='  ' title='Slide: 5' border='1'  width='85%%'/\u003e\n\nWe are designing the system so that it can work with diverse ways of expressing access rights, for example we are considering how the approach described here could be extended based on the likes of the [Tribal Knowledge labels](https://localcontexts.org/labels/traditional-knowledge-labels/), incorporating them into the data licensing framework we discuss below.\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide06.png' alt='Case Study - Sydney Speaks  ' title='Slide: 6' border='1'  width='85%%'/\u003e\n\nIn this talk we look at a case-study with the [Sydney Speaks project](https://slll.cass.anu.edu.au/sydney-speaks) via LDaCA steering committee member Professor [Catherine Travis](https://orcid.org/0000-0002-1410-3268).\n\n\u003e This project seeks to document and explore Australian English, as spoken in Australia’s largest and most ethnically and linguistically diverse city – Sydney.\n\u003e The title “Sydney Speaks” captures a key defining feature of the project: the data come from recorded conversations between Sydney siders, as they tell stories about their lives and experiences, their opinions and attitudes. This allows us to measure how their lived experiences impact their speech patterns.\n\u003e Working within the framework of variationist sociolinguistics, we examine variation in phonetics, grammar and discourse, in an effort to answer questions of fundamental interest both to Australian English, and language variation and change more broadly, including:\n\u003e\n\u003e - How has Australian English as spoken in Sydney changed over the past 100 years?\n\u003e - Has the change in the ethnic diversity over that time period (and in particular, over the past 40 years) had any impact on the way Australian English is spoken?\n\u003e - What affects the way variation and change spread through society - Who are the initiators and who are the leaders in change? - How do social networks function in a modern metropolis? - What social factors are relevant to Sydney speech today, and over time (gender? class? region? ethnic identity?)\n\u003e   A better understanding of what kind of variation exists in Australian English, and of how and why Australian English has changed over time can help society be more accepting of speech variation and even help address prejudices based on ways of speaking.\n\u003e   Source: \u003chttp://www.dynamicsoflanguage.edu.au/sydney-speaks/\u003e\n\nThe collection contains recordings of people speaking, both contemporary and historic.\n\nBecause this involved human participants there are restrictions on the distribution of data - a situation we see with lots of studies involving people in a huge range of disciplines.\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide07.png' alt='Sydney Speaks Licenses ' title='Slide: 7' border='1'  width='85%%'/\u003e\n\nThere are four tiers of data access we need to enforce and observe for this data based on the participant agreements and ethics arrangements under which the data were collected.\n\nConcerns about rights and interests are important for any data involving people - and a large amount the data both indigenous and non-indigenous we are using will require access control that ensures that data is shared with the right users under the right conditions.\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide08.png' alt='' title='Slide: 8' border='1'  width='85%%'/\u003e\n\n(Image generated by DALLE - prompt: A NSW Driver license for a wolfhound pup named Floki)\n\nLets go over some basics, starting with _licences_.\n\nA licence in this context is _a natural language document_ in which a copyright holder sets out the terms and conditions of use for data. Licences _may_ have metadata that describes them, eg a property to say that this is an open licence (and does not require a check when serving data).\n\nA license is not a computer program, or configuration, or an AI entity that can make decisions, it’s a legal document. You may also know this as a “data sharing agreement” or “terms of use”. Examples of licenses we see all the time are the GNU GPL or the various Creative Commons licenses which grant rights to others to redistribute a creative work, and specifies conditions on what changes are permitted.\n\nThat said, metadata _about_ a license can be used to automate decision making - if it is labelled as being an open license, then a repository can serve data and include that data, if it is labeled as “closed” or more aptly, “authorization-required” then repository software can perform an authorization step, which we cover in detail later.\n\nIn the world of research data generated by or about human participants, licenses can’t always allow unauthenticated access and data redistribution, and they may permit distribution only to certain people, or classes or person. Some data, for example (particularly that which has not been or cannot be de-identified) can only be made available to the original research team.\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide09.png' alt='  ' title='Slide: 9' border='1'  width='85%%'/\u003e\n\n(Dall-e prompt : A sad dog sitting on an iceberg, XKCD)\n\nSo, a license is a document that expresses conditions such as “Data can be used by other researchers”, but unfortunately we don’t have systems in the research-data ecosystem which can automatically identify a user as “a researcher” (this may be surprising to some, but the Australian Access Federation can, at this stage, only say that someone has an account with an institution - it can’t tell a professor from a student administration officer and there are certainly no lists of “certified linguists”).\n\nHere are some cold hard facts:\n\nWe don’t have an authority that can identify someone as a researcher,\n\nOr a “linguist”,\n\nOr an “anthropologist”,\n\nOr a member of an ARC (Australian Research Council) research project,\n\nThe [CADRE] project is working on systems that will eventually support all these things, but they are not available as services yet, and their initial focus is on government data, so we have to work out ways for our data custodians to make decisions on who is considered an “other researcher” in the absence of attribute-based authentication.\n\n[cadre]: https://ardc.edu.au/project/cadre/\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide10.png' alt='' title='Slide: 10' border='1'  width='85%%'/\u003e\n\nThe access control system we have been prototyping is based on licenses.\n\nFor any data object, which could be an entire collection, or one set of recordings of a speaker in a speech study, or a set of hand written linguistic field notes from the 1950s, or a novel etc we store a license with it. This means that future archivists / librarians and researchers can work out how to manage the data if the systems we build today for automated access are no longer operational and we give the license an ID which is a URL we can use to identify it uniquely.\n\nThis diagram shows how a license is explicitly linked to the data using a metadata description standard known as “Research Object Crate” [RO-Crate] . Each object in the repository is a crate, with a metadata file that describes the object and (optionally) its component files, including the data license.\n\n[ro-crate]: http://ptsefton.com/2019/11/05/RO-Crate%20eResearch%20Australasia%202019/index.html\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide11.png' alt='' title='Slide: 11' border='1'  width='85%%'/\u003e\n\n(This diagram has been updated from the one presented at eResearch to show two portals instead of one)\n\nEvery item in a repository has a license, which may be an open one like CC Share Alike or a custom license derived from the ethics and participants agreements for a study in the context of local laws and institutional policy.\n\nUsing this license, distributed access portals in our architecture can check against an authorization system for each request for data. The portals may both host data with the same licensing but do not need to maintain access control lists.\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide12.png' alt='' title='Slide: 12' border='1'  width='85%%'/\u003e\n\n(Images: Various baskets of puppies by DALL-E)\n\nWhen we first developed access controls for LDaCA in 2021 it was a requirement that data licensing and access control decisions be decoupled from each other, and from particular repository software. The usual approach in repositories is to build in a local access-control system, but this is tied to a particular implementation and will not work in a distributed environment where there are multiple different repositories, and services such as computational resources that researchers need to access to process data.\n\nWe could not find an available open source system for managing license-based access to data, so our starting approach used groups as a proxy for granting licences on that basis that all common user-directory services such as LDAP include the concept of user groups.\n\nScope:\n\n- simplest possible license based approach to access control\n\n- NOT attempting to be attribute based as that is not currently feasible within our project scope (see [CADRE] for progress in that direction)\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide13.png' alt='' title='Slide: 13' border='1'  width='85%%'/\u003e\n\nThe first prototype, which we presented at eResearch Australasia last year was a proof-of-concept Github based system. This demonstrated that authorization can be delegated from a repository to an external service. For each of the Sydney Speaks licenses there was a Github group (organization). The repository, when requested to serve data would get the user to login using the Github Authentication services, then check if the user was in the correct license group.\n\nThis worked, but there were issues with this approach:\n\n- There are no workflow options (unless we build a workflow system), just adding people to a Github organisation to pre-authorize them\n\n- The system only supported a single logon service, which is not widely used in academia or by community groups\n\nSo, we talked to the our colleagues at the Australian Access Federation (AAF), about a supported, research-sector-wide service.\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide14.png' alt='' title='Slide: 14' border='1'  width='85%%'/\u003e\n\nThe AAF, as it happened were already working with other research groups on a service called [CILogon] (hosted in the USA initially, but soon to be hosted in Australia), like Github, this service has groups (which was our way of associating users with licenses in the absence of a specific license-granting service), but also allows users to log in with a variety of Authentication providers, including research institutions, via the Australian Access Federation as well as social logins such as Google and Microsoft (and our old friend Github).\n\nAgain this worked, but the current version of CILogon does not have particularly easy-to-use ways for a license-holder to create groups - there are a number of abstract constructs to deal with and there is currently no way to build an approval workflow using the web interface, so as with the Github trial we would have needed to build this part (all of this may change, as the software is under constant development).\n\nThere is a [nine minute silent video](https://youtu.be/xEWXiM-jUfY) of what this looked like on YouTube for those who are really interested.\n\nAAF is engaging with our project on the following:\n\n-  a cloud-based authentication and authorisation infrastructure (AAI) to support the needs of the project\n-  understand and develop business process documentation for authorising access to data and services\n-  configure the AAI to support these business processes and to develop extensions to facilitate new functionality that may be required\n-  create a set of policies, standards and guidelines for managing researchers’ identity and access management\n-  develop support documentation, train community representatives to operate the platform, and provide support to the community managers.\nThe AAF has recommended CILogon \u0026 REMS as potential solutions to investigate \u0026 prototype\n\nCILogon is a federated identity management platform that provides the following features:\nsupport for institutional and community logins\ncross-institutional and community collaboration\nfederated identity and group management\na community management dashboard\nOIDC connectors for downstream services that support authorisation claims for services like\nREMS\nBinderHub\nJupyterHub\nLDaCA Dashboard\n\nREMS (Resource Entitlement Management System) is a tool to help researchers browse resources such as datasets relevant to their research and to manage the application process for access to the resources.\n\n[cilogon]: https://www.cilogon.org/\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide15.png' alt='REMS ' title='Slide: 15' border='1'  width='85%%'/\u003e\n\nRecently (after the abstract for this presentation was submitted) the AAF team made us aware of the Resource Entitlement Management System, [REMS](https://github.com/CSCfi/rems), which is an open source application out of Finland. This software is the missing link for LDaCA in that it allows a data custodian to grant licenses to users. And it works with CILogon as an Authentication layer so we can let users log in using a variety of services.\n\nAt the core of REMS is a set of Licenses which can then be associated with Resources - in our design this is (almost always) a one-to-one correspondence, for example we would have a licence “Sydney Speaks Data Researcher Access License” corresponding to resource that represents ALL data with that licence. These Resources can then be made available through a catalog, and workflows can be set up for pre-authorization processes ranging from single-click authorizations where a user just accepts a licence and a bot approves it, to complex forms where users upload credentials and one or more data custodians approve their request, and grant them the licence.\n\nIt also has features for revoking permissions, and has a full API so admin tasks can be automated (for us that’s in the future).\n\nOnce a user has been granted a license in a pre-authorization process then a repository can authorize access to a resource by checking with REMS to see if a given user is pre-authorized. That is, has been granted a license. Note that users do not have to find REMS on their own - they will be directed to it from data and computing services when they need to apply for pre-authorization.\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide16.png' alt='  ' title='Slide: 16' border='1'  width='85%%'/\u003e\n\nThis interaction diagram shows the flow involved in a user applying for a data license via REMS.\n\nNot shown here are some design and preparation steps:\n\n- The research team read their ethics approval and participant agreements and craft one or more access agreements (AKA licenses) for a data set (NOTE: If the data can be made available automatically with just a license attached, such as when all parties have agreed that data can be Creative commons licensed, or the data is in the public domain then the following steps are not required)\n\n- The research team and support staff add the license to REMS, creating a “resource” a virtual offering that corresponds to any dataset that has the above license\n\n- The research team add a workflow to REMS - this could range from an auto-approved click through where users can agree to license terms, through to detailed (manual) checking of their credentials.\n\nThe next slide shows the interactions involved in accessing data once a user has been granted the license license.\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide17.png' alt='  ' title='Slide: 17' border='1'  width='85%%'/\u003e\n\nThis diagram shows the “access-control dance” for a user who has been granted a license in REMS obtaining access to a dataset at a data portal which gives access to data in a repository or archive.\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide18.png' alt='Demo ' title='Slide: 18' border='1'  width='85%%'/\u003e\n\nIn this video we demonstrate how to use REMS and how does a user request access to an LDaCA resource.\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide19.png' alt='FAQ  ' title='Slide: 19' border='1'  width='85%%'/\u003e\n\n(This section was added after the conference, to try to summarize the discussion and clarify requirements by starting an FAQ on this approach)\n\n## Q Why not \"just\" implement an access control list (ACL) in the repository?\n\nThere are a few reasons for the distributed approach we have taken in LDaCA:\n\n1.  ACLs need maintenance over time - people's identities change, they retire and die, so storing a list of identifiers such as email addresses alongside content is not a viable long-term preservation strategy. Rather, we will encourage data custodians to describe in words what are permitted uses for the data, and by whom, in a license, then allow whomever is the current data custodian to manage that access in a separate administrative system. We expect these administrative systems to be ephemeral, and change over time but also to generate less friction over time as standards are developed. Expected future benefits of concentrating these processes will include that people do not have to prove the same claims they make about themselves multiple times and that it is easier for data custodians to authorize access.\n\n2.  LDaCA data will be stored in a variety of places with separate portal applications serving data for specific purposes; if these systems all have in-built authorization schemes, even if they are the same, then we have the problem of synchronizing access control lists around a network of services.\n\n3.  Accessing data that requires some sort of authorization process is not language or humanities specific, so working with an existing application that can handle pre-authorization workflows and access-control authorization decisions is an attractive choice and should allow LDaCA to take advantage of centrally managed services with functionality that improves over time rather than having to develop and maintain our own systems.\n\n4.  If complex access controls are implemented inside a system then there is a risk that data becomes stranded inside that system and cannot be reused without completely re-implementing the access control. For example, imagine an archive of cultural material with complex access controls encoded into the business logic such as “this item is accessible only to male initiates”. Applications like this need to store user accounts with attributes on both data and user records that can be used to authorize access. There is a high risk of data being stranded in a system such as this if it is no longer supported. This will be mitigated somewhat if the rules are also expressed as licenses, perhaps a composition of Traditional Knowledge (TK) Labels - but the access system is baked-in to the application and not portable.\n\n## Q: Yes but why does data need to have a license if we already have access controls?\n\nThe point of Research Data Commons projects like LDaCA is to create an ecosystem where data can be re-used. For language data, this means that users, including researchers and community members, will be able to download data for certain authorised purposes and activities. The license is the way that data custodians communicate to data users (and future administrators) what those purposes activities are.\n\nA license, which is always packaged with data will allow:\n\n- A user to inspect a five-year-old dataset in their downloads folder and work out what they are allowed to do with it.\n\n- An IT professional to clean up laptop that has been handed in by (or seized from – it happens) a departing faculty member.\n\n- A developer to re-create an access control replacing a decommissioned system.\n\n## Q So many licenses! Sounds like a lot of work!\n\nWe expect that the overhead of writing licenses will diminish greatly over time and standard clauses and complete licenses will be established. A data depositor will be able to choose from a set of standard license terms (such as a standard “restricted to CIs and participants license” for a given repository, using that as a template to mint their own license for a given data set with its own name and ID. The user can choose a standard way of adding pre-authorized licensees (such as email invitations). This ID can then be used by an authorization system.\n\n## Q So you have centralized authorization into a system that grants licenses doesn't that mean you are locked-in to that system?\n\nNo, and Yes\n\n**No**, there is no lock in regarding the list of Licenses and pre-authorized users; licenses and access control lists can be exported via an API so it is possible to import them into another system or save them for audit purposes.\n\n**Yes**, there is lockin, in that at this stage the workflow used to give access to users is specific to the system (such as REMS)\n\n**But**, because our process requires a governance step _first_ in writing a license, then there is a statement of intent for re-building those processes later if needed - a step which is very likely to be missing in a system with built-in access control.\n\nAlso, over time, we expect the administrative burden of constructing workflows will become less as standards are developed for a couple of things:\n\n1. Licenses can be made less complex (particularly in the context of academic studies) if they specify re-use by particular known cohorts in advance - this comes down to improving the design of studies to encourage data reuse. This may also help to simplify academic ethics processes in the medium to long term.\n\n2. The CADRE project is looking to improve pre-authorization workflows that automatically source relevant information about potential users - fetching their publication record, and potentially remembering what certifications they have, so these attributes can be used and reused for decision making. It is conceivable that this approach might be useful in cultural contexts as well to allow data custodians to manage data sharing - this is a discussion we have yet to have in the broader HASS RDC.\n\n## Q What if I have a really simple requirement like giving access to just a couple of people - doesn’t this license approach just add complexity?\n\nIf a data item needs to be locked down to a small group of people, say the chief investigator and the participants in a recorded dialogue then an obvious implementation is to maintain a small access control list (ACL) for the item. But all of the issues identified above with application-specific ACLs are the same, no matter the size of the cohort: the data set can’t be access controlled outside of its home system. If the system is no longer running then the data may be completely inaccessible, and if there is no license document stored with the data setting out terms of re-use in general terms then there is no indication to future administrators about who, if anyone, should have access of the data.\n\n## Q: We don’t need a license, we have a “terms of use”\n\nSame thing. Terms of use for data are what a license does. We are designing our systems so that all the relevant terms and conditions go in one place to minimize confusion.\n\n\u003c/section\u003e\n\nThe final three slides have been contributed by co-author Patrick.\n\nThese slides briefly outline the AAF process for the next phase that will provide the foundations for the development of the service and the creation of those policies to support the community and the service.\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide20.png' alt='Next Steps in the AAF Engagement ….. Revisit and consolidate the project’s vision through interviews and engagement with stakeholders,  collaborators and  community participants This next phase will provide the foundations for the development of policies to support the community. These will assist in creating a trusted community for access to sensitive data that supports the good practice and protocols. These activities are outlined in the following slides … ' title='Slide: 20' border='1'  width='85%%'/\u003e\n\nThis process will support the project to deliver a viable service that meets researchers’ needs and is trusted by the community and the participants to safely distribute data to authorised persons.\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide21.png' alt='The AAF\u0026#x27;s business analyst conducts interviews with stakeholders and community members to discover and formalise the community\u0026#x27;s processes and requirements ' title='Slide: 21' border='1'  width='85%%'/\u003e\n\nThe AAF’s business analyst is conducting interviews with the key stakeholders.\nThis discovery process will collect information on the current and the “to-be” state of the service.\n\nTogether these will establish goals and expectations and provide the basis for further prototyping a service that meets stakeholder needs.\n\nThe process will facilitate the building of a service that empowers the data custodians, the communities and participants to manage access.\n\n\u003c/section\u003e\n\n\u003csection typeof='http://purl.org/ontology/bibo/Slide'\u003e\n\u003cimg src='/fair-care-eresearch-2022/Slide22.png' alt='These feeds into the prototyping \u0026amp; implementing phase   ' title='Slide: 22' border='1'  width='85%%'/\u003e\n\nThe basis for prototyping is iterative:\nIdentify\nPrioritise\nPilot\nReview\nUpdate requirements\n\nThis leads to a production service that meets participant, community and researcher requirements and unifies the services, policies and trust framework for the community.\n\n\u003c/section\u003e\n"},{"title":"HASS RDC Technical Advisory Group Meeting LDaCA \u0026 ATAP Intro","slug":"rdc-tech-meeting","tags":["Repositories"],"content":"\nThis is a presentation Peter Sefton gave to the \n[Humanities, Arts and Social Sciences Research Data Commons and Indigenous Research Capability Program](https://ardc.edu.au/collaborations/strategic-activities/hass-and-indigenous-research-data-commons/)  Technical Advisory Group on Friday 11th February 2022. \n\nThanks to Simon Musgrave for reviewing this and adding a little detail here \nand there.\n\n(This is also available on [Dr Sefton's site](https://ptsefton.com/2022/02/18/hass_rdc_tech_advisory/index.html))\n\n\u003ca href=\"/rdc-tech-meeting/HASS RDC Technical Advisory Group Meeting LDaCA \u0026 ATAP Intro.pdf\"\u003ePDF version\u003c/a\u003e \n\n![Hass RDC Technical Avisory Group Meeting](/rdc-tech-meeting/Slide00.png)\n\nThe Language Data Commons of Australia Data Partnerships (LDaCA) and the Australian Text Analytics Platform (ATAP) are establishing a scalable and flexible language data and analytics commons. These projects will be part of the Humanities and Social Sciences Research Data Commons (HASS RDC).\nThe Data Commons will focus on preservation and discovery of distributed multi-modal language data collections under a variety of governance frameworks. This will include access control that reflects ethical constraints and intellectual property rights, including those of Aboriginal and Torres Strait Islander, migrant and Pacific communities.\n\n![Slide01](/rdc-tech-meeting/Slide01.png)\n\nFor this Research Data Commons work we are using the Arkisto Platform \n(introduced [at eResearch 2020](http://ptsefton.com/2020/11/23/Arkisto/index.html)).\n\nArkisto aims to secure the long term preservation of data independently of \ncode and services - recognizing the ephemeral nature of software and platforms.\nWe know that sustaining software platforms can be hard and aim to make sure \nthat important data assets are not locked up in database or hard-coded logic \nof some hard-to-maintain application.\n\nWe are using three key standards on this project …\n\n\n![Slide02](/rdc-tech-meeting/Slide02.png)\n\nThe first standard is the [Oxford Common File Layout](https://ocfl.io/1.0/spec/) - \nthis is a way of keeping version controlled digital objects on a plain old \nfilesystem or object store. \n\nHere’s the introduction to the spec:\n\n\u003e **Introduction**\n\u003e\n\u003e This section is non-normative.\n\u003e\n\u003e This Oxford Common File Layout (OCFL) specification describes an application-independent approach to the storage of digital objects in a structured, transparent, and predictable manner. It is designed to promote long-term access and management of digital objects within digital repositories.\n\u003e\n\u003e **Need**\n\u003e\n\u003e The OCFL initiative began as a discussion amongst digital repository practitioners to identify well-defined, common, and application-independent file management for a digital repository's persisted objects and represents a specification of the community’s collective recommendations addressing five primary requirements: completeness, parsability, versioning, robustness, and storage diversity.\n\u003e\n\u003e **Completeness**\n\u003e\n\u003e The OCFL recommends storing metadata and the content it describes together so the OCFL object can be fully understood in the absence of original software. The OCFL does not make recommendations about what constitutes an object, nor does it assume what type of metadata is needed to fully understand the object, recognizing those decisions may differ from one repository to another. However, it is recommended that when making this decision, implementers consider what is necessary to rebuild the objects from the files stored.\n\u003e\n\u003e **Parsability**\n\u003e\n\u003e One goal of the OCFL is to ensure objects remain fixed over time. This can be difficult as software and infrastructure change, and content is migrated. To combat this challenge, the OCFL ensures that both humans and machines can understand the layout and corresponding inventory regardless of the software or infrastructure used. This allows for humans to read the layout and corresponding inventory, and understand it without the use of machines. Additionally, if existing software were to become obsolete, the OCFL could easily be understood by a light weight application, even without the full feature repository that might have been used in the past.\n\u003e\n\u003e **Versioning**\n\u003e\n\u003e Another need expressed by the community was the need to update and change objects, either the content itself or the metadata associated with the object. The OCFL relies heavily on the prior art in the [Moab] Design for Digital Object Versioning which utilizes forward deltas to track the history of the object. Utilizing this schema allows implementers of the OCFL to easily recreate past versions of an OCFL object. Like with objects, the OCFL remains silent on when versioning should occur recognizing this may differ from implementation to implementation.\n\u003e\n\u003e **Robustness**\n\u003e\n\u003e The OCFL also fills the need for robustness against errors, corruption, and migration. The versioning schema ensures an OCFL object is robust enough to allow for the discovery of human errors. The fixity checking built into the OCFL via content addressable storage allows implementers to identify file corruption that might happen outside of normal human interactions. The OCFL eases content migrations by providing a technology agnostic method for verifying OCFL objects have remained fixed.\n\u003e\n\u003e **Storage diversity**\n\u003e Finally, the community expressed a need to store content on a wide variety of storage technologies. With that in mind, the OCFL was written with an eye toward various storage infrastructures including cloud object stores.\n\n![Slide03](/rdc-tech-meeting/Slide03.png)\n\nThe second standard is Research Object Crate. (RO-Crate) a method for \ndescribing any dataset of local or remote resources as a digital object using \na **single linked-data metadata document**.\n\nRO-Crate is used in our platform both for describing data objects in the OCFL \nrepository, and for delivering metadata over the API (which we’ll show in \narchitecture diagrams and screenshots below).\n\n![Slide04](/rdc-tech-meeting/Slide04.png)\n\nRO-Crates may contain any kind of data resource about anything, in any format \nas a file or URL - it’s not just for language data; there are also many \nprojects in the sciences starting to \n[use RO-Crate](https://www.researchobject.org/ro-crate/in-use/).\n\n\n![Slide05](/rdc-tech-meeting/Slide05.png)\n\nThis image is taken from a \n[presentation on digital preservation](https://slideplayer.com/slide/3919920/).\n\n[Models](https://pcdm.org/2016/04/18/models)\nThe third key standard for Arkisto is the Portland Common Data Model. Like \nOCFL, this was developed by members of the digital library/repository \ncommunity. It was devised as a way to do interchange between repository \nsystems, most of which, it turned out had evolved very similar ways of having \nnested collections, digital objects that aggregate related files. Using this \nvery simple ontology allows us to store data in the OCFL layer in a very \nflexible way - depending on factors like data size, licensing and whether \ndata is likely to change or need to be withdrawn, we can store entire \ncollections as OCFL objects or across many OCFL objects with PCDM used to \nshow the structure of the data collections regardless of how they happen to \nbe stored.\n\n![Slide06](/rdc-tech-meeting/Slide06.png)\n\nBack to RO-Crates.\n\nRO-Crates are self-documenting and can ship with a HTML file that allows a \nconsumer of the crated data to see whatever documentation the crate authors \nhave added.\n\nThis crate contains an entire collection (RepositoryCollection is the RO-\nCrate term that corresponds to pcdm:Collection). \n\nCrates must have license information that set out how data may be used and if \nit may be redistributed. As we are dealing with language data which is (almost\n) always created by people, it is important that their intellectual property \nrights and privacy are respected. More on this later.\n\n\n![Slide07](/rdc-tech-meeting/Slide07.png)\n\nThis shows a page for what we’re calling an Object (RepositoryObject). A \nRepositoryObject is a single “thing” such as a document, a conversation, a \nsession in a speech study. (this was called an item in Alveo but given that \nboth the Portland Common Data model and Oxford Common File Layout use “Object\n” we are using that term at least for now).\n\nThis shows that the system is capable of dealing with unicode characters - \nwhich is good, as you would expect as it’s 2022 and this is a Language Data \nCommons, but there are still challenges, like dealing with mixtures of left \nto right and right to left text, and we need to find or define metadata terms \nto keep track of “language”, “writing system”, and the difference between \nthings that started as orthographic (written) text, vs spoken or signed etc. \nThere’s a group of us working on that, currently led by Nick Thieberger and \nPeter Sefton.\n\nSimon Musgrave and Peter Sefton \n[presented our progress with multilingual text](https://ptsefton.com/2022/01/27/DAMTA_Slides_v1/) \nat a virtual workshop run by ANU in January.\n\n\n![Slide08](/rdc-tech-meeting/Slide08.png)\nHere’s another screenshot showing one of the government documents in PDF \nformat - with a link back to the abstract RepositoryObject that houses all of \nthe manifestations of the document in various languages.\n\n![Slide09](/rdc-tech-meeting/Slide09.png)\n\nThe above diagram takes a big-picture view of research data management in the \ncontext of *doing* research. It makes a distinction between managed \nrepository storage and the places where work is done - “workspaces”. \nWorkspaces are where researchers collect, analyse and describe data. \nExamples  include the most basic of research IT services, file storage as \nwell as analytical tools such as Jupyter notebooks (the backbone of ATAP - \nthe text analytics platform). Other examples of workspaces include code \nrepositories such as GitHub or GitLab (a slightly different sense of the word \nrepository), survey tools, electronic (lab) notebooks and bespoke code \nwritten for particular research programmes - these workspaces are essential \nresearch systems but usually are not set up for long term management of data.\n\nThe cycle in the centre of  this diagram shows an idealised research practice \nwhere data are collected and described and deposited into a repository \nfrequently. Data are made findable and accessible  as soon as possible and \ncan be “re-collected” for use and re-use.\n\nFor data to be re-usable by humans and machines (such as ATAP notebook code \nthat consumes datasets in a predictable way) it must be well described. The \nATAP and LDaCA approach to this is to use the Research Object Crate (RO-Crate\n) specification. RO-Crate is essentially a guide to using a number of \nstandards and standard approaches to describe both data and re-runnable \nsoftware such as workflows or notebooks.\n\n\n![Slide10](/rdc-tech-meeting/Slide10.png)\n\nThis rather messy slide captures the overall high-level architecture for the \nLDaCA Research Data Commons  - there will be an analytical workbench (left of \nthe diagram) which is the basis of the Australian Text Analytics (ATAP) \nproject - this will focus on notebook-style programming using one of the \nemerging Jupyter notebook platforms in that space. (This is not 100% decided \nyet, but that has not stopped the team from starting to collect and develop \nnotebooks that open up text analytics to new coders from the linguistics \ncommunity.) Our engagement lead, Dr Simon Musgrave sees the ATAP work as \nprimarily an educational enterprise encouraging researchers to adopt new \nresearch practices - which will be underpinned by services built on the \nArkisto standards that allow for rigorous, re-runnable research.\n\n![Slide11](/rdc-tech-meeting/Slide11.png)\n\nIn this presentation we are going to focus on the portal/repository \narchitecture more than on the ATAP notebook side of things. We know that we \nwill be using (at least) the SWAN Jupyter notebook service perceived by \nAARNet but we are still scoping how notebooks will be made portable between \nsystems and where they will be stored at various stages of their development. \nWe will be supporting and encouraging researchers to archive notebooks \nwrapped in RO-Crates with re-use information OUTSIDE of the SWAN platform \nthough - it’s a workspace, not a repository; it does not have governance in \nplace for long term preservation.\n\n\n![Slide12](/rdc-tech-meeting/Slide12.png)\n\nThis is a much simpler view zooming in on the core infrastructure components \nthat we have built so far. We are starting with bulk ingest of existing \ncollections and will add one-by-one deposit of individual items after that.\n\nThis show the OCFL repository at the bottom - with a Data \u0026 Access API that \nmediates access. This API understands the RO-Crate format and in particular \nits use of the Portland Common Data Model to structure data. The API also \nenforces access control to objects; every repository object has a license \nsetting out the terms of use and re-use for its data, which will reflect the \nway the data were collected - whether participants signed agreements, ethics \napprovals and privacy law are all relevant here. Each license will correspond \nto a group of people who have agreed to and/or been selected by a data \ncustodian. We are in negotiations with the \n[Australian Access Federation (AAF)](https://aaf.edu.au/) to use their \n[CILogon](https://www.cilogon.org/) service for this authorization step and \nfor authentication of users across a wide variety of services including the \nAAF itself and Google, Microsoft, GitHub etc.\n\nThere’s also an access portal which will be based on a full-text index (at \nthis stage we’re using ElasticSearch) which is designed to help people find \ndata they might be interested in using. This follows current conventions for \nbrowse/search interfaces which we’re familiar with from shopping sites - you \ncan search for text and/or drill down using *facets* (which are called \naggregations in Elastic-land). eg which language am in interested in or do I \nwant [ ] Spoken or [ ] Written material? \n\n\n![Slide13](/rdc-tech-meeting/Slide13.png)\n\nThis architecture is very modular and designed to operate in a distributed \nfashion, potentially with distributed file and/or object based repositories \nall being indexed by a centralised service. There may also be other ‘flavours’\nof index such as triple or graph stores, relational databases that ingest \ntabular data or domain specific discovery tools such as corpus analysis \nsoftware. And, there may be collection specific portals that show a slice of \na bigger repository with features or branding specific to a subset of data.\n\n\n![Slide14](/rdc-tech-meeting/Slide14.png)\n\nThis implementation of the Arkisto standards-stack is known as Oni.  That’s \nnot really an acronym any more though it once stood for OCFL, Ngnix (a web \nserver) or Node (a Javascript framework)  and an Index. An Oni is a kind of \nJapanese demon. 👹\n\n\n![Slide15](/rdc-tech-meeting/Slide15.png)\n\nBut how will data get into the OCFL repository? At the moment we’re loading \ndata using a series of scripts which are being developed at our github \norganization.\n\nThis diagram and the next come from the \n[Arkisto Use cases page](https://arkisto-platform.github.io/use-cases/) it \nshow how we will be converting data from existing collections into a form \nwhere they can be preserved in an OCFL repository and be part of a bigger \ncollection, ALWAYS with access control based on licenses.\n\n\n![Slide16](/rdc-tech-meeting/Slide16.png)\n\nThis is a screenshot our github repository showing the corpus migration tools \nwe’ve started developing (there are six, and one general purpose text-\ncleaning tool). These repositories have not all been made public yet, but \nthey will be - they contain tools to build Arkisto-ready file repositories \nthat can be made available in one or more portals \n\n\n![Slide17](/rdc-tech-meeting/Slide17.png)\n\nHere’s our portal which give a browse interface to allow drill-down data discovery.\n\nBut wait! That’s not the LDaCA portal - that’s Alveo!\n\nOh yes, so it is.\n\nAlveo was built ten years ago - and has not seen a much uptake.\n\n![Slide18](/rdc-tech-meeting/Slide18.png)\n\nThis screenshot shows some of the browse facets for the COOEE corpus, which \ncontains early Australian **written** English materials. But facets like `\nWritten Mode` and `Communication Medium` both of which are known for COOEE \nare not populated.\n\nThere are a quite few things that were wrong with Alveo - like we obviously \ndidn’t get all the metadata populated to the level that it would make these \nbrowse facets actually useful for filtering. But more importantly, there was \nnot enough work done to check which browse facets *are* useful and not enough \nof the budget was able to be spent on user engagement and training rather \nthan software development.\n\nOne of my current LDaCA senior colleagues said to me a couple of years ago \nthat Alveo was useless:  “I just wanted to get all the data” they siad. Me, I \nwas thinking “but it has an API so you CAN get all the data - what’s the \nproblem?”. We have tried not to repeat this mistake by making sure that the \nAPI delivers entire collections and we have some demonstrations of doing this \nfor real work.\n\nAnother colleague who was actually on the Alveo team said that this interface \nwas \"equally useless for everyone\", and they later built a custom interface \nfor one of the collections.\n\nWe’re taking these lessons to heart in designing the LDaCA infrastructure - \nmaking sure that as we go we have people using the software - it helps that \nwe have an in house (though distributed) development team rather than an \nexternal contractor so feedback is very fast - we can jump onto a call and \ndemo stuff at any time.\n\n![Slide19](/rdc-tech-meeting/Slide19.png)\n\nWe decided to build from the data API first.\n\nIn this demo developer Moises Sacal Bonequi is looking at the API via the \nPostman tool. This demonstration shows how the API can be used to find \ncollections (that conform to our metadata profile)  \n\n1. First he lists the collections, then chooses one\n2. He then gets a collection with the `\u0026resolve` parameter, meaning that the \n   API will internally traverse the PCDM collection hierarchy and return ALL \n   the metadata for the collection - down to the file level\n3. He then downloads a file (for which he has a license that most of you \n   reading this don’t have - hence the obfuscation of the \n   dialogue)\n\nThis API has been used and road tested at ANU to develop techniques for topic \nmodelling on the Sydney Speaks corpus (more about which corpus below) - by a \nstudent Marcel Reverter-Rambaldi under the supervision of  Prof Catherine \nTravis at ANU - we are hoping to publish this work as a re-usable notebook \nthat can be adapted for other projects, and to allow the techniques the ANU \nteam have been developing to be applied to other similar data in LDaCA.\n\n![Slide20](/rdc-tech-meeting/Slide20.png)\n\nAnd one of the data scientists who was working with us at UQ, Mel Mistica, \ndeveloped a [demonstration notebook](https://github.com/Australian-Text-Analytics-Platform/ro-crate-metadata/blob/main/ro-crate-metadata.ipynb) \nwith our tech team that used the API to access another full collection (which \nis also suitable for the ANU topic modelling approach) - this notebook gets \nall the metadata for a small social history collection which contains \ntranscribed interviews with women in Western Sydney and shows how a data \nscientist might explore what’s in it and start asking questions about the data,\nlike the age distribution of the participants and start digging in to what \nthey were talking about.\n\n\n![Slide21](/rdc-tech-meeting/Slide21.png)\n\nThis screencast shows a work-in-progress snapshot of the Oni portal we talked \nabout above in action, showing how search and browse might be used to find \nrepository objects from the index - in this case searching for Arabic words \nin a small set of Australian Government documents.\n\n![Slide22](/rdc-tech-meeting/Slide22.png)\n\nHang on!\n\nYou keep talking about\n[“repositories”](http://ptsefton.com/2012/02/14/an-australian-research-data-repository/),\ndon’t you always say stuff like, \"A repository is not just a software \napplication. It’s a lifestyle. It’s not just for Christmas?\"\n\nThat’s right - we’ve been talking about repository software architectures \nhere but it is important to remember that a repository needs to be considered \nan institution rather than a software stack or collection of files, more\n\"University Library\" than \"My Database\".\n\n![Slide23](/rdc-tech-meeting/Slide23.png)\n\nThe next half a dozen slides are based on \n[a presentation](https://ptsefton.com/2021/10/12/ldaca2021/index.html)\nI gave at eResearch Australasia 2021 with Moises Sacal Bonequi\n\nToday we will look in detail at one important part of this architecture - \naccess control. How can we make sure that in a distributed system, with \nmultiple data repositories and registries residing with different data \ncustodians, the right people have access to the right data?\n\nI didn’t spell this out in the recorded conference presentation, but for data \nthat resides in the repositories at the right of the diagram we want to \nencourage research processes that clearly separate data from code. Notebooks \nand other code workflows that use data will fetch a version-controlled \nreference copy from a repository - using an access key if needed, process the \ndata and produce results that are then deposited into an appropriate \nrepository alongside the code itself.  Given that a lot of the data in the \nlanguage world is NOT available under open licenses such as Creative Commons \nit is important to establish this practice - each user of the data must \nnegotiate or be granted access individually. Research can still be \nreproducible using this model, but without a culture of sharing datasets \nwithout regard for the rights of those who were involved in the creation of \nthe data.\n\n![Slide24](/rdc-tech-meeting/Slide24.png)\nRegarding rights, our project is informed by the \n[CARE principles](https://www.gida-global.org/care) for Indigenous data.\n\n\u003e The current movement toward open data and open science does not fully \n\u003e engage with Indigenous Peoples rights and interests. Existing principles \n\u003e within the open data movement (e.g. FAIR: findable, accessible, interoperable\n\u003e , reusable) primarily focus on characteristics of data that will facilitate \n\u003e increased data sharing among entities while ignoring power differentials and \n\u003e historical contexts. The emphasis on greater data sharing alone creates a \n\u003e tension for Indigenous Peoples who are also asserting greater control over \n\u003e the application and use of Indigenous data and Indigenous Knowledge for \n\u003e collective benefit\n\nBut we do not see the CARE principles as only applying to Indigenous data and \nknowledge. Most language data is a record of the behaviour of people who have \nmoral rights in the material (even if they do not have legal rights) and \ntaking the CARE principles as relevant in such cases ensures serious thinking \nabout the protection of tose moral rights.\n\n![Slide25](/rdc-tech-meeting/Slide25.png)\n[Traditional Knowledge Labels](https://localcontexts.org/labels/traditional-knowledge-labels/)\n\nWe are designing the system so that it can work with diverse ways of \nexpressing access rights, for example licensing like the Tribal Knowledge \nlabels.The idea is to separate safe storage of data with a license on each \nitem, which may reference the TK labels from a system that is administered by \nthe data custodians who can make decisions about who is allowed to access data.\n\n![Slide26](/rdc-tech-meeting/Slide26.png)\nWe are working on a case-study with the \n[Sydney Speaks project](http://www.dynamicsoflanguage.edu.au/sydney-speaks/) \nvia steering committee member Catherine Travis.\n\n\u003e This project seeks to document and explore Australian English, as spoken in \n  Australia’s largest and most ethnically and linguistically diverse city – Sydney. \n\u003e The title “Sydney Speaks” captures a key defining feature of the project: \n  the data come from recorded conversations between Sydney siders, as they tell \n  stories about their lives and experiences, their opinions and attitudes. This \n  allows us to measure how their lived experiences impact their speech \n  patterns.\n\u003e Working within the framework of variationist sociolinguistics, we examine \n  variation in phonetics, grammar and discourse, in an effort to answer \n  questions of fundamental interest both to Australian English, and language \n  variation and change more broadly, including:\n\u003e - How has Australian English as spoken in Sydney changed over the past 100 years?\n\u003e - Has the change in the ethnic diversity over that time period (and in particular, over the past 40 years) had any impact on the way Australian English is spoken?\n\u003e - What affects the way variation and change spread through society\n\u003e     - Who are the initiators and who are the leaders in change?\n\u003e     - How do social networks function in a modern metropolis?\n\u003e     - What social factors are relevant to Sydney speech today, and over time (gender? class? region? ethnic identity?)\n\u003e A better understanding of what kind of variation exists in Australian \n  English, and of how and why Australian English has changed over time can \n  help society be more accepting of speech variation and even help address \n  prejudices based on ways of speaking.\n\u003e Source: \u003chttp://www.dynamicsoflanguage.edu.au/sydney-speaks/\u003e\n\nThe collection contains recordings of people speaking both contemporary and \nhistoric.\n\nBecause this involved human participants there are restrictions on the \ndistribution of data - a situation we see with lots of studies involving \npeople in a huge range of disciplines.\n\n\n![Slide27](/rdc-tech-meeting/Slide27.png)\nThere are four tiers of data access we need to enforce and observe for this \ndata based on the participant agreements and ethics arrangements under which \nthe data were collected.\n\nConcerns about rights and interests are important for any data involving \npeople - and a large amount the data both indigenous and non-indigenous we \nare using will require access control that ensures that data sharing is \nappropriate. \n\n\n![Slide28](/rdc-tech-meeting/Slide28.png)\n\nIn this example demo we uploaded various collections and are authorising with \nGithub organisations \n\nIn a our production release we will use AAF to authorise different groups.\n\nLet's find a dataset: The Sydney Speaks Corpus.\n\nAs you can see we cannot see any data\n\nLets login… We authorise Github…\n\nNow you can see we have access sub corpus data and I am just opening a couple of items\n\n—\n\nNow in Github we can see the group management example. \n\nI have given access to all the licences to myself, as you can see here and \ngiven access to licence A to others.\n\n\n![Slide29](/rdc-tech-meeting/Slide29.png)\n\nThis diagram is a sketch of the interaction that took place in the demo - it \nshows how a repository can delegate authorization to an external system - in \nthis case Github rather than CILogon. But we are working with the ARDC to set \nup a trial with the Australian Access Federation to allow CILogon access for \nthe HASS Research Data Commons so we can pilot group-based access control.\n\n\n![Slide30](/rdc-tech-meeting/Slide30.png)\n\nThere’s a lot still to do.\n"}],"pages":[{"title":"Background","slug":"background","content":"\n# Language Data - Background Information\n### [Principles](/principles)\nInformation about the principles on which the work of LDaCA is based.\n\n### [Technologies](/technologies)\nInformation about the technologies being used in LDaCA.\n\n### [Metadata](/metadata)\nInformation about the approach to \n[metadata](https://en.wikipedia.org/wiki/Metadata) being taken by LDaCA.\n\n### [Sample Collections](/collections)\nInformation about the first datasets which have been added to LDaCA.\n\n### [Case Studies](/case-studies)\nAccounts by collectors of various kinds of language data illustrating the \ndifferent solutions they have adopted for the problems encountered in the \nprocess.\n\n\u003chr class=\"dots\" /\u003e\n"},{"title":"Case-studies","slug":"case-studies","content":"## Fieldwork in Papua New Guinea\n\u003cbr /\u003e\n\nHarriet Sheppard's [account](../fieldwork-png) of collecting data on a remote island in Papua New Guinea.\n\n\u003cbr /\u003e\n\n## Fieldwork in Sydney\n\u003cbr /\u003e\n\u003cbr /\u003e\n\n## Data in a Language Centre\n\u003cbr /\u003e\n\u003cbr /\u003e\n\n## Archival data\n\u003cbr /\u003e\n\u003cbr /\u003e\n\nBack to [Background](../background/)\n"},{"title":"Collections","slug":"collections","content":"LDaCA has begun ingesting data sets, including:\n\n- Corpus of Oz Early English [CoOEE](https://varieng.helsinki.fi/CoRD/corpora/COOEE/basic.html): a collection of texts written in Australia between 1788 and 1900. The corpus is divided into four time periods (1788–1825, 1826–1850, 1851–1875 and 1876–1900) each holding about 500,000 words. Four registers were defined for CoOEE: the Speech-based Register (SB), the Private Written Register (PrW), the Public Written Register (PcW) and the register of Government English (GE). In every Period 1-4 there is a similar number of words in the different registers.\n- [Sydney Speaks](https://www.dynamicsoflanguage.edu.au/sydney-speaks/): This project seeks to document and explore Australian English, as spoken in Australia’s largest and most ethnically and linguistically diverse city – Sydney. \n- [From Farms to Freeways](http://omeka.uws.edu.au/farmstofreeways/): This research project sought to analyse the experiences of women who had lived in the Blacktown and Penrith areas since the early 1950s, including their responses to social changes brought about by rapid suburbanisation in the Western Sydney region in the post-war period. Two-hour taped discussions were held with 34 women, aged sixty and over, who were in their early twenties during the Western Sydney region's population growth.\n\nWe have also ingested a collection of government documents in various languages. This a very small dataset assembled to check that our technology can handle different languages and different scripts; more information about this work is available in [this presentation](https://ptsefton.com/2022/01/27/DAMTA_Slides_v1/index.html).\n\nBack to [Background](../background/)\n"},{"title":"Contact us","slug":"contact","content":"\n### Contact us\n\nYou can contact the Language Data Commons of Australia by [email](mailto:info@ldaca.edu.au)\n\nOur logo was designed by Otis Carmichael. [Read more](/designer) about Otis and the ideas behind his design.\n\nWe share a [Twitter account](https://twitter.com/LDaCA_Program) with the [Australian Text Analytics Platform](https://www.atap.edu.au):\u003cbr\u003e\n"},{"title":"Language Data","slug":"data","content":"\n# Language Data\n### [Principles](/principles)\nInformation about the principles on which the work of LDaCA is based.\n\n### [Technologies](/technologies)\nInformation about the technologies being used in LDaCA.\n\n### [Metadata](/metadata)\nInformation about the approach to \n[metadata](https://en.wikipedia.org/wiki/Metadata) being taken by LDaCA.\n\n### [Sample Collections](/collections)\nInformation about the first datasets which have been added to LDaCA.\n\n### [Case Studies](/case-studies)\nAccounts by collectors of various kinds of language data illustrating the \ndifferent solutions they have adopted for the problems encountered in the \nprocess.\n\n\u003chr class=\"dots\" /\u003e\n"},{"title":"Otis Carmichael","slug":"designer","content":"\n![LDaCA logo](/logo-dark.png)\n\n### Otis Carmichael\n\nThe design of the LDaCA logo came from my own experience researching the Waanyi language, the language of [my mob](https://en.wikipedia.org/wiki/Waanyi). The design flows from the beginning to the end, where it flows back into itself at the beginning, representing the cyclical nature of life and the Land. The strands of language interweave with themselves, influenced by everything that touches them until they become something entirely their own. The complexity of these languages is a thing of beauty and their preservation, study and proliferation lays claim to this unceded land. The inspiration of the design is as follows:\n\n**L** - Language is shown as a flow of words that can be merged, split and rearranged to create different meanings, just as these lines can be.\n\n**D** - Data is depicted by these specks of raw information, undecipherable before processing but heavy with power.\n\n**a** - This data is not just numbers and words but signals from the Universe, such as animal tracks.\n\n**C** - Whilst research often appears niche in these commons, there is no limit to the range of it’s influence, the ripples of which disrupt everything it touches.\n\n**A** - The language data of this common has been collected from many individual nations that make up ‘Australia’, often with a foundation on extraction based research techniques. This project aims to make up for the mistakes of the past by celebrating and giving back to these nations, which are so incredibly distinct whilst sharing beautiful connections with each other.\n\nThe colours utilised in this design draw from those that are seen in my peoples land of Boodjamulla, in the Gulf of Carpentaria. They reflect the connections between the Sky, the People and the Land in the hope that these connections continue to strengthen indefinitely.\n\n**My Bio**\nMy name is Otis Carmichael and I am a proud Waanyi man living in Meanjin. My designs reflect my appreciation of the past and my hope for the future.\n"},{"title":"Fieldwork PNG","slug":"fieldwork-png","content":"\n# Fieldwork in Papua New Guinea\n\nby Harriet Sheppard\n![Harriet on an outrigger boat off Vanatɨna](/HS-sailing-outrigger-small.jpg)\nHarriet on an outrigger boat off Vanatɨna\n\nMy fieldwork experience, in many ways, conforms to stereotypes of language \nfieldwork – a community outsider working with a smaller language community \nlocated far from an urban centre. For my postgraduate research, I documented \nand described grammatical topics of \n[Sudest](https://glottolog.org/resource/languoid/id/sude1239) (known as Vanga \nVanatɨna by its speakers), an Oceanic language spoken in Papua New Guinea (PNG\n). The language is spoken on the islands of Vanatinai (also known as Sudest \nand Tagula) and Yeina in the Louisiade Archipelago, approximately 350 \nkilometres southeast of the PNG mainland. In the following, I discuss \ndecisions I made and processes I followed related to the collection of \nlanguage data and archiving the resulting corpus of texts, including ethical \nconsiderations, ownership of the data, data formats, metadata, and data \nsecurity and reuse.  This is by no means meant as a guide (of best practice) \nfor projects where language data are being collected with speakers of un(der)\ndescribed languages. Rather, it is an account of some of the types of \nconsiderations and procedures that need to be considered when collecting \nlanguage data and how I handled them in this instance.\n\n\n![Map of Louisiade Archipelago](/map-louisiade.jpg)\n\nBefore recording with a speaker for the first time, I would explain the \nproject to the speaker and their rights as contributors to the project. I \nwould explain how I would use the language recordings, how they would be \nstored and accessed and how speakers of the language or other researchers \nmight access and use the recordings in the future. This included discussing \nhow they, the person(s) making the recording, would retain ownership over the \nrecording (i.e., intellectual and cultural property rights). They would also \nbe the ones to choose access rights over the text and have the option to \nwithdraw any or all recordings they made from the corpus at any point (more \non these below). After these discussions, if the speaker was still interested \nin the project, I would then record an oral consent form with them. The \nconsent form was in English, the language of wider communication in the \nprovince. In cases where the speaker didn’t speak or spoke limited English, \nwe would have a translator present for this process. I chose to use an oral \nconsent form as it is accessible to all participants no matter their literacy \nlevels.  It also has the advantage of being a more durable format compared \nwith a paper consent form, particularly when working in a hot and humid \nclimate and it can also be easily backed up.\n\n![Transcription session with Abel Sam](/transcription-session-small.jpg)\nEach time I made a recording with a speaker or speakers, I played the \nrecording back to them to check that they were happy with the recording and \nso that they could decide on what sort of access conditions they wanted the \narchived recording to have. If the speaker was not satisfied with the \nrecording, sometimes they would choose to make another recording and delete \nthe original or simply delete the recording and maybe try again later. If \nthey were happy with the recording, they would decide on the access \nconditions they wanted for the archived recording. The possibilities ranged \nfrom the most restricted, being that only I, as the original researcher, \ncould access the recordings and use them for my research, to the opposite end \nof the access continuum where anyone interested could download and listen to \nthe recordings, read any transcriptions, and potentially use them for \neducational or research purposes. In between these two ends of the continuum, \nthey could opt for other access conditions such as only allowing access for \nregistered users of the archive, or only for researchers and speakers, or \njust for speakers. They could also choose to have a text embargoed for a \ncertain period of time.\n\n\n![Transcription session with Abel Sam](/elicitation-session-notes-small.jpg)\nWhen I was preparing my ethics application for the university, I applied and \nreceived ethics approval to work with minors. I didn’t plan to collect data \nfrom children (although I did want the option of recording older teens if any \nshowed an interest in recording), but included minors in my ethics \napplication because of the potential for children to be inadvertently filmed, \nfor example if they walk into frame or come up to a parent while the parent \nis being recorded. The ethics approval meant that if such instances arose \nduring a recording, the recording could still be kept and archived (\nassuming approval from the speaker making the recording). While such \nsituations didn’t end up arising during my own data collection, this would \nnot have been an ideal fix for this issue. More widely, as a discipline, we \ndon’t have good model(s) regarding how we deal with such scenarios in regards \nto access and consent. One suggestion I heard was to only make such \nrecordings openly available after obtaining informed consent from any \nchildren involved after they turned 18 but, for most circumstances, such an \napproach is unlikely to be realistic.\n\nAs the researcher who collected the data and deposited it in the archive, I \nam the person the archive contacts when someone puts in a request to access \nrestricted data. But granting any access to data, whether restricted or not, \nis based on the wishes of the owner(s) of a specific text, that is, the \nspeaker(s) who recorded it. As mentioned, the speaker(s) retains ownership of \nthe text and intellectual and cultural property rights over the recording. \nThey also retain the right to change access restrictions on a recording or \nremove it completely from the archived corpus at any point and request that \nit not be used in any future research from that point forwards (given it \nwould be difficult or impossible to remove excerpts from recordings in \npreviously submitted or published work on the language, this is not something \nthat is generally done). Away from the field, this can get a bit complicated \ndue to limited telephone and internet coverage in the islands although it is \nincreasingly more feasible as more and more people have access to mobile \ntelephones and apps like WhatsApp and Facebook. Any future users of the \ncorpus who download data from the archive must agree to only use the data for \neducational or research purposes and not for (direct) financial gain. Having \nsaid that, it needs to be acknowledged that researchers including myself can \nand do benefit indirectly from such data collections in the form of \nscholarships, grants, qualifications, and other potential employment \nopportunities related to our work with the language.  Although I, like many (\nhopefully most) fellow linguists, subscribe to the idea that the source of \nthe data, that is the speaker(s), should have control over their own \nrecordings, the current systems we have set up make that hard or impossible \nto implement. This is something that is increasingly being acknowledged and \nwe as a community of researchers don’t currently have answers for. Each \nspeaker community is different and so one-size-fits-all models for \ncommunities are unlikely to work. \n\nWhen collecting spoken or signed language data, the best practice is to make \naudio and video recordings in formats that are ‘lossless’ rather than ‘lossy\n’. Lossless compression allows for perfect reconstruction of the original data\n. Lossy formats reduce file size but also discard some information that \ncannot be reconstructed which is obviously not an ideal outcome for precious \nlanguage data! For audio recordings, I use a recorder that records Waveform \nAudio File Format or WAV files (.wav) and for video recordings I record \nAdvanced Video Coding High Definition or AVCHD files (.mts) which are both \nlossless formats and accepted by many archives. WAV files are also the audio \nformat needed for many linguistics software programs. AVCHD files are, however\n, not a file type accepted by the archive where the Sudest corpus is housed \nwhich only accepts MPEG video files (.mpg) which are lossy. This situation is \nlikely due to the fact that the archive was set up some two decades ago when \nstorage capacity was a big issue and the archive has not yet caught up in \nthis regard. This is not an ideal situation and something that should be \nconsidered if you are able to choose the archive where your data will be \ndeposited. For derived data including text-audio aligned transcriptions and \ntranslations as well as any other annotations, I use plain text (.txt) and \nELAN Annotation Format files (.eaf). The plain text files are the file type \ncreated when working in the Field Linguist’s Toolbox software program and can \nbe opened, read, and edited in most text editors.  It is also a good format \nfor storing information if you want it to persist. The EAF files are more \nspecialist although they are a transferable format. They are only created \nand used with ELAN which is an annotation tool for audio and video widely \nused by linguists. Both text and EAF files are some of the standard file \nformats for transcriptions and annotations that are generally accepted by \nlanguage archives today. \n\n![Setting up for a recording session](/recording-setup-small.jpg)\nWhen collecting metadata, I adopted the Component MetaData Infrastructure (\nCMDI) schema used by the archive where I was planning to deposit my data. The \nlogistics of fieldwork can be overwhelming at times and following the archive’\ns metadata schema meant I would minimally have the metadata required to \ndeposit the corpus. The schema was also practical in that there were some \nfields that were obligatory (e.g., text title, creation data, country of \nrecording) but many fields are optional (e.g., speaker date of birth/age, \neducation level, address where recording took place). Having optional fields \nthat could be filled in or not worked well for me because collecting more \ndetailed information, particularly biographical information about speaker(s), \nwasn’t always possible or appropriate. Not all speakers, particularly older \nspeakers, necessarily knew their date or year of birth and they may only know \ntheir approximate age. Some speakers I only met when they came to make a \nrecording and getting the detailed biographic information one might like for \nmetadata records is not necessarily culturally appropriate, particularly \ngiven the power imbalance between myself, an educated, white researcher from \nAustralia, and speakers I don’t have an established relationship with. Aside \nfrom asking about where the speaker grew up, currently lived, and their \napproximate age, if I didn’t already have an established relationship with \nthe individual, I tended to let information emerge incidentally as \nvolunteered rather than questioning them in depth. One topic I did discuss \nwith speakers regarding metadata was the question of authorship and \nattribution of the recording and whether they wanted their name to be listed \nor would like to be given a pseudonym. With small communities, especially \nwhen you are recording audio and video of the speaker, complete anonymisation \nis not really possible if the speaker also wants the recording to be \naccessible to other speakers and this could be a potential issue. However, in \nmy experiences, (all) speakers chose to have their names listed as the author(\ns) of the text without anonymisation and were generally excited or proud to \nbe making a record of their language and knowledge.    \n\nWhile the amount of personal metadata collected for individual speakers varied\n, there was also situational data that I wish I had collected and didn’t. I \ncollected information including the date and location of the recording (e.g., \nthe village and house where the recording took place), who was present at the \nrecording, the genre of recording (e.g., a historical narrative, conversation\n, instructions on how to complete a specific task, etc.). One piece of \ninformation that I didn’t include, for example, is the exact location of the \nspeaker and their orientation in space. Were they facing towards the north? \nNorth-west? Since my original fieldwork, I have begun to research the use of \nco-speech gestures in the language, that is, gestures that occur at the same \ntime as someone is speaking. Frequently, speakers point to real world \nlocations while they are talking but this can be hard to identify if you don’\nt know exactly how the speaker was positioned in relation to where they are \ngesturing. Luckily, by watching the videos I can still identify the direction \na speaker was facing and add this to the recording’s metadata but this would \nbe a very difficult if not impossible information for a future user of the \ndata to ascertain if they wanted to study gesture. \n\nData security is something I had to consider both in the field and at home. \nIn order to keep data secure on computers and storage devices, I made sure \nthat they were all password protected. In the field, I also had to consider \nspecific factors in the environment that could endanger the data - the major \nissue is the humidity. The average humidity level on the island hovers around \n80 percent year-round and I had paper notebooks, a computer, hard drives and \nSD cards that I need to keep in working order in a house that is relatively \nopen to the outside environment with no glass windows and no electricity (I \ndo remember a field manual suggesting a fridge makes a good home for \nequipment in humid environments if that is an option). To protect equipment,\nI stored everything in waterproof pouches with silica sachets. I would \ndistribute backup USBs and SD cards across different pouches so if one failed\n, I would hopefully have backups. I also tried to double bag equipment and \nnotes when travelling, particularly by boat. Boat transport in the islands is \neither by traditional wooden sailing outrigger canoes or fibreglass dinghy ‘\nbanana’ boat, both of which are open to the elements. \n\nTo secure data, ideally, I would also make backup copies of all new \nrecordings and transcription files each day in the field. However, during my \nfirst fieldtrip, I didn’t have access to a power source and therefore couldn’\nt charge my laptop meaning I couldn’t backup recordings. As this also meant I \nwas transcribing all texts using pencil and paper, I did photograph all new \ntranscriptions each night to preserve a backup copy. Towards the end of my \ntrip, I became quite worried about having no backups for recordings, \nparticularly when thinking about the boat trip back to the main island which \ncan take up to eight hours (or more in bad conditions). To avoid making this \ntrip with only the original copies, I tagged along on a walk to the local \nprimary school two and a half hours away because it was rumoured that one of \nthe teachers had a solar panel connected to a car battery and I would be able \nto charge my laptop and therefore backup the recordings. All the community’s \nprimary school students make this walk twice a week, living at school during \nthe week. At one point in the journey, you have to cross a river in a small \noutrigger canoe. It was quite alarming when the teenager paddling me across \nnonchalantly pointed out a nearby crocodile while we were mid-river but at \nleast I managed to back up all the files! For subsequent fieldtrips, I \ninvested in a battery and solar panel for charging equipment and backing up \ndata although I still had to cross rivers with crocodiles from time to time. \n\n\n![Crossing the Veora river](/crossing-river-small.jpg) \nIn order to secure the corpus of texts into the future and make sure that it \ndoesn’t get lost on an old hard drive, I have uploaded it to a language \narchive. The archive is a digital repository tasked with safeguarding \nlanguage corpora of smaller languages for which there is limited documented \ndata. As well as keeping copies of the uploaded data, the archive will also \nensure that the data continues to be accessible, for example, by converting \ndata files to newer formats if the format a file was uploaded in becomes \nobsolete. Having the corpus saved in an online archive also means that it is \ndiscoverable for other researchers, speakers, and other interested parties. \nIt can be found through the search portal of the archive by searching by \nlanguage name(s), the contributor’s name, or by browsing by country. The \ncorpus can also be found by searching through the Open Language Archive \nCommunity (OLAC) portal which is an online virtual library of information and \nlinks to language resources available in a specific language (e.g., grammars, \ndictionaries, archived texts). The information on OLAC is automatically \ncollected or ‘harvested’ daily from participating archives meaning that when \nyou upload a file to an archive, it will automatically be listed in OLAC the \nnext time it is updated. Because there is limited information online about \nSudest, keyword searches on most online search engines such as Google also \ndisplays links to the corpus quite high up in the results.  \n\nIt's often suggested that the researcher should make a will and assign a \nliterary executor for the data they collect. This raises the question about \nwho to nominate? As a graduate student, I nominated one of my supervisors as \nthe person to have direct access and editing powers over the archived corpus. \nThis makes sense in that, after myself, my supervisor is one of the people \nthat knows the most about the project and data collection. Supervisors are \nalso often the chief investigator for ethics approval for graduate data \ncollection as well. But supervisors tend to be older than graduate students \nand don’t necessarily have any personal connection to the speaker community. \nIdeally, any succession plan would bring control of the data back to the \ncommunity but what would that mean in practice? There may be issues both \npolitical and practical if just one individual from the community becomes the \nexecutor. In some cases, there may be an obvious option if the community has \na language and culture centre or registered Indigenous corporation. In such \ncases, access and stewardship plans may have been discussed and incorporated \ninto the project since the beginning. For many communities, especially in the \nPacific, this is not the case and therefore there is no clear governing group \nwhich might take such a task on. \n\nIn future research projects I might work in, I can build on my past \nexperiences and hopefully improve on them. This would likely involve \nexpanding the list of situational metadata I automatically collect for each \nrecording as well as considering the file types accepted by an archive when \ndeciding where to deposit any recordings. Although I’ve only touched briefly \non issues relating to speaker-community control and intellectual and cultural \nproperty rights, I would also aim to build in more funding and space into the \ntimeline of a project for more equitable community consultation and \ncollaboration. Such discussion would then likely have a flow-on effect for \nhow to secure the data into the future. Ethical and practical questions \nregarding data collection, access, and stewardship are complex. It is \nimpossible to take all contingencies into account. Best practice and \nstandards change with much of the change led by Indigenous and First Nations \nresearchers. Best practices from a decade ago are not those of today and this \nis a good thing!\n\n\n![Vuwo village](/Vuwo.JPG)\n\nBack to [Language Data](../data/)\n"},{"slug":"home","content":"## The Language Data Commons of Australia (LDaCA) will make nationally significant language data available for academic and non-academic use and provide a model for ensuring continued access with appropriate community control.\nAustralia is a massively multilingual country in one of the world’s most \nlinguistically diverse regions. Significant collections of this intangible \ncultural heritage have been amassed, including collections of Australian \nIndigenous languages, regional languages of the Pacific, and of Australian \nEnglish, as well as collections important for cyber-security and for \nemergency communication. LDaCA will integrate this existing work into a \nnational research infrastructure while also securing collections which remain \nunder-utilised or at risk. LDaCA will thus ensure long-lasting access for \nanalysis and reuse of these invaluable data, and will manage the data in a \nculturally, ethically and legally appropriate manner guided by FAIR and CARE \nprinciples.\n\nTo accomplish these goals, LDaCA will:\n- Develop a comprehensive language data access policy framework,\n- Develop shared technical infrastructure and standards across institutions,\n- Build a sustainable long-term repository for ingesting and curating existing\n  language data collections of national significance,\n- and build a portal for discovery and access of language data.\n\nThe result will be an integrated national technical infrastructure to analyse \nlanguage collections at scale which will open up the social and economic \npossibilities of Australia's rich linguistic heritage. The project will build \nconnections to other HASS RDC projects and assist in laying the foundation \nfor the establishment of a broader HASS Research Data Commons as well as \npositioning Australia internationally as a leading contributor of language \ncollections and digital infrastructure.\n\n![Acknowledgement](/AcknowledgeARDC.png)\n\nThe Language Data Commons of Australia (LDaCA) project received investment \n([1](https://doi.org/10.47486/DP768) and [2](https://doi.org/10.47486/HIR001))\nfrom the Australian Research Data Commons (ARDC). The ARDC is funded by the \nNational Collaborative Research Infrastructure Strategy (NCRIS).\n\n*LDaCA acknowledges and pays respects to the Elders and Traditional Owners of \nthe lands on which we live and work.*\n\n\n"},{"title":"Metadata","slug":"metadata","content":"\nMetadata is often defined as 'data about data'. High quality metadata is important in making data FAIR:\n- Findable: metadata is the starting point for searching data collections. For example, if we want to find data in a particular language, this will only be possible for data which has a language recorded in its metadata. (Tracking languages is in itself problematic, see [below](#identifying-codes-for-languages).)\n- Accessible: access conditions which apply to data should be part of the associated metadata.\n- Interoperable: information about the format of data and whether it requires specific software to be usable should be part of the associated metadata.\n- Reusable: all of aspects of metadata mentioned above contribute to making data reusable. The more we know about some data, the easier it is to know whether it will be useful to us or not.\n\nRO-Crates in general have basic metadata requirements, but it is possible to specify a **profile** for crates for specific purposes. LDaCA is developing such a profile for our data; we are basing this largely on previous work in the area. An important aspect of the RO-Crate approach is that it uses the principles of [Linked Open Data](https://en.wikipedia.org/wiki/Linked_data#Linked_open_data). This means that terms used in our metadata will (whenever possible) link to an openly available definition. In developing the profile, we are drawing on two existing attempts to provide vocabularies for describing language data.\n\n## [OLAC](http://www.language-archives.org/)\n\nThe Open Language Archives Community is an international partnership of institutions and individuals; one of their activities is developing consensus on best current practice for the digital archiving of language resources and this includes making recommendations for metadata. The OLAC metadata scheme is based on [Dublin Core](https://www.dublincore.org/) (DC), a widely used general metadata schema. OLAC have suggested refinements and extensions of the DC base which make it more useful for describing language resources.\n\n\n## [CMDI](https://www.clarin.eu/content/component-metadata)\n\nThe Component Metadata Infrastructure was developed within the [CLARIN](https://www.clarin.eu) project. It draws on the earlier ISLE Metadata Initiative ([IMDI](https://en.wikipedia.org/wiki/IMDI)), but where IMDI attempted to specify a comprehensive scheme for (multimodal) language data, CMDI adopts a more flexible approach where components are assembled into reusable profiles. This is very similar to the RO-Crate approach described above but with an important difference: the components of a CMDI profile are all drawn from a central registry, whereas components of an RO-Crate profile come from any linkable location.\n\n\n## Identifying Codes for Languages\n\nOne very important piece of metadata for language data is a description of the language or languages which the data represent. This is not a simple problem because the relationship between languages and names for them is not one to one. Some languages have more than one name: for example *Farsi* and *Persian* can both be used to refer to the same language. Some names refer to more than one language: for example there are languages called *Buru* used in Nigeria and in Indonesia. To avoid the confusion which can arise from such situations, various systems have been developed to assign unique identifiers to languages. None of these systems gives a comprehensive list of languages and all such systems struggle with another problem, the distinction between separate languages and dialects of one language, as can be seen in the case study below. LDaCA includes identifiers from each of the three systems below where they are available and relevant.\n\n##### [ISO-639](https://iso639-3.sil.org/)\nThis system is recognised as a standard by the International Standards Organisation. An earlier version of this system used two-letter codes to identify languages; more recent versions use three-letter codes (referred to as ISO 639-3). These codes are used by *[Ethnologue](https://www.ethnologue.com/)*, which is a catalogue of the languages of the world, and in many other contexts. The ISO 639-3 code for French is **fra**, and that for Warlpiri is **wbp**\n\n##### [Glottolog](https://glottolog.org/)\nGlottolog is an alternative catalogue of the world's languages, language families and dialects - Glottolog uses the term *languoid* to cover all of these. Each languoid is assigned a unique identifier consisting of four alphanumeric characters and four digits. For example, (standard) French has the code **stan1290**, amd Warlpiri is **warl1254**.\n\n##### [Austlang](https://collection.aiatsis.gov.au/austlang/about)\n\nAustLang provides a controlled vocabulary of persistent identifiers, a thesaurus of languages and peoples, and information about Aboriginal and Torres Strait Islander languages which has been assembled from referenced sources. Alphanumeric codes are used as persistent identifiers, while associated text strings are changeable and can reflect community preferences (including alternative names and spellings). In Austlang, Warlpiri has two codes: **C15** for the language in general, and **C15.1** for the variety named as Wakirti Warlpiri. (French is not covered by Austlang.)\n\n#### Case study - Kala Lagaw Ya\n\nKala Lagaw Ya is a language spoken in the Torres Strait Islands. The language has several dialects or varieties and the table below shows how the different code schemes deal with this.\n\n\u003ctable\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eName\u003c/b\u003e\u003c/td\u003e\u003ctd\u003e\u003cb\u003eISO 639\u003c/b\u003e\u003c/td\u003e\u003ctd\u003e\u003cb\u003eGlottolog\u003c/b\u003e\u003c/td\u003e\u003ctd\u003e\u003cb\u003eAustlang\u003c/b\u003e\u003c/td\u003e\u003ctd\u003e\u003cb\u003eNotes\u003c/b\u003e\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003eKala Lagaw Ya\u003c/td\u003e\u003ctd\u003emwp\u003c/td\u003e\u003ctd\u003ekala1377\u003c/td\u003e\u003ctd\u003eY1\u003c/td\u003e\u003ctd\u003eAustlang: Marked with symbol ^ which indicates that the name is used to refer to a language and a dialect of the language.\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003eKalaw Kawaw Ya\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003ctd\u003ekala1378\u003c/td\u003e\u003ctd\u003eY2\u003c/td\u003e\u003ctd\u003eEthnologue: Kalaw Kawaw is a dialect\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003eKawrareg\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003ctd\u003ekawr1234\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003eKulkalgau Ya\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003ctd\u003ekulk1234\u003c/td\u003e\u003ctd\u003eY4\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003eMabuyag\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003ctd\u003emabu1234\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003ctd\u003eEthnologue: Mabuiag is an alternate name\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003eKawalgaw Ya\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003ctd\u003eY5\u003c/td\u003e\u003ctd\u003eAustlang: Kaurareg is an alternative name (probably the same as Glottolog kawr1234)\u003c/td\u003e\u003c/tr\u003e\n\u003c/table\u003e\n\n\u003cbr /\u003e\n\nBack to [Background](../background/)\n"},{"title":"Organisation","slug":"organisation","content":"\n# Organisation\n\nLDaCA is one strand of the partnership between the Australian Research Data Commons ([ARDC](https://ardc.edu.au/)) and the [School of Languages and Cultures](https://languages-cultures.uq.edu.au/) at the [University of Queensland](https://www.uq.edu.au/). This partnership includes a number of projects that explore language-related technologies, data collection infrastructure and Indigenous capability programs. These projects are being led out of the Language Technology and Data Analytics Lab ([LADAL](https://slcladal.github.io/index.html)), which is overseen by [Professor Michael Haugh](https://languages-cultures.uq.edu.au/profile/1498/michael-haugh) and [Dr Martin Schweinberger](https://languages-cultures.uq.edu.au/profile/4295/martin-schweinberger).\n\nThe LDaCA project received investment from ARDC through two of its programs:\n\n1. Australian Data Partnerships Program: Developing policy and technology foundations of a nationally integrated research infrastructure for language data collections of high strategic importance for the Australian research community ([DP](https://ardc.edu.au/project/language-data-commons-of-australia-ldaca/)).\n2. HASS Research Data Commons and Indigenous Research Capability Program: Capitalising on existing infrastructure, securing vulnerable and dispersed collections and linking with improved analysis environments for new research outcomes ([HASS-RDC](https://ardc.edu.au/news/announcing-3-successful-projects-ardc-hass-rdc/)).\n\n\u003cbr /\u003e\n\u003chr class=\"dots\" /\u003e\n\u003cbr /\u003e\n\n## Partner Institutions:\n\n- **University of Queensland** (DP, HASS-RDC): Professor Michael Haugh, Professor Clint Bracknell\n- **Australian National University** (DP, HASS-RDC): Professor Catherine Travis\n- **Monash University** (DP, HASS-RDC): Associate Professor Louisa Willoughby\n- **University of Melbourne** (DP, HASS-RDC): Associate Professor Nick Thieberger\n- **University of Sydney** (HASS-RDC): Professor Monika Bednarek (Sydney Corpus Lab)\n- **AARNet** (HASS-RDC)\n- **First Languages Australia** (HASS-RDC): Beau Williams\n\n### Advisory / Consultative Partners\n\n- PARADISEC\n- ARC Centre of Excellence for the Dynamics of Language\n- Australian Digital Observatory\n- CLARIN\n"},{"title":"Pre-ALS Conference Activity Day","slug":"pre-als-activities","content":"\n## Language Data Commons of Australia\n\n## Australian Text Analytics Platform\n\n### Day of Activities before ALS2022 (Tuesday 29/11)\n\n\u003ctable\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eTime\u003c/b\u003e\u003c/td\u003e\u003ctd\u003e\u003cb\u003eActivity\u003c/b\u003e\u003c/td\u003e\u003ctd\u003e\u003cb\u003ePeople\u003c/b\u003e\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e09:00 - 10:00\u003c/td\u003e\u003ctd\u003eIntroduction to notebooks\u003cbr\u003e\u003ci\u003eWhat are Jupyter notebooks and why are they useful?\u003c/i\u003e\n\u003c/td\u003e\u003ctd\u003eSara King\u003cbr\u003eSimon Musgrave\u003c/td\u003e\u003c/tr\u003e\n\u003ctr bgcolor = 'LightGray'\u003e\u003ctd\u003e10:00 - 10:30\u003c/td\u003e\u003ctd\u003eBreak\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e10:30 - 11:00\u003c/td\u003e\u003ctd\u003eGeneral progress report\u003cbr\u003e\u003ci\u003eAn overview of the projects and of progress to date\u003c/i\u003e\n\u003c/td\u003e\u003ctd\u003eMichael Haugh\u003cbr\u003eRobert McLellan\u003cbr\u003eSimon Musgrave\u003cbr\u003eJenny Fewster (ARDC)\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e11:00 - 12:00\u003c/td\u003e\u003ctd\u003eIntroducing the LDaCA Portal\u003cbr\u003e\u003ci\u003eHow will I find data in LDaCA? Your chance to tell us whether we are meeting your needs!\u003c/i\u003e\u003c/td\u003e\u003ctd\u003ePeter Sefton\u003cbr\u003eKathrin Kaiser\u003c/td\u003e\u003c/tr\u003e\n\u003ctr bgcolor = 'LightGray'\u003e\u003ctd\u003e12:00 - 01:00\u003c/td\u003e\u003ctd\u003eLunch\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e01:00 - 02:00\u003c/td\u003e\u003ctd\u003eData and access\u003cbr\u003e\u003ci\u003eHow is access to data managed in LDaCA and how can I access data?\u003c/i\u003e\n\u003c/td\u003e\u003ctd\u003ePeter Sefton\u003cbr\u003eKathrin Kaiser\u003cbr\u003eCatherine Travis\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e02:00 - 02:30\u003c/td\u003e\u003ctd\u003eAuslan project update\u003cbr\u003e\u003ci\u003eReport on work by the team at Monash University\u003c/i\u003e\n\u003c/td\u003e\u003ctd\u003eLouisa Willoughby\u003cbr\u003eTrevor Johnston\u003c/td\u003e\u003c/tr\u003e\n\u003ctr bgcolor = 'LightGray'\u003e\u003ctd\u003e02:30 - 03:00\u003c/td\u003e\u003ctd\u003eBreak\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e03:00 - 03:30\u003c/td\u003e\u003ctd\u003eText analytics for corpus linguistics\u003cbr\u003e\u003ci\u003eSelected ATAP tools showcase\u003c/i\u003e\n\u003c/td\u003e\u003ctd\u003eSydney Informatics Hub with Sydney Corpus Lab (Monika Bednarek)\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e03:30 - 05:00\u003c/td\u003e\u003ctd\u003eDiscursis workshop\u003cbr\u003e\u003ci\u003eHow to get started with this tool for tracking topics in conversations \u003c/td\u003e\u003ctd\u003eBen Foley\u003cbr\u003eSam Hames\u003c/td\u003e\u003c/tr\u003e\u003c/i\u003e\n\u003c/table\u003e\n\nThe [Australian Text Analytics Platform](https://www.atap.edu.au/) and the [Language Data Commons of Australia](https://www.ldaca.edu.au/) are projects supported by the [Australian Research Data Commons](https://www.ardc.edu.au) to develop infrastructure for Australian researchers who work with language data. This day of activities organised by the projects will give ALS conference delegates (and anyone else who is interested) the opportunity to learn more about this work. The day will include:\n\n- an overview of the projects and the work to date\n- reports on specific sub-projects\n- introductory workshops on the tools and resources being developed\n- a workshop on using Discursis, a tool for tracking topics in interactive use of language\n- the opportunity to influence future work by exploring and providing feedback on the data interface which we are building.\n\nThe activities will be of interest to anyone who conducts research which includes language data, especially those who use or would like to use computational tools in their research. Participants should bring their own computer; no software is required beyond a web browser.\n\nCatering is provided and therefore a registration fee is being charged for this event to partially cover the cost ($25 full rate and $15 discount).\n\n[Registration](https://als.asn.au/Conference/Conference2022/Workshops)\n\nThe Australian Text Analytics Platform and the Language Data Commons of Australia received investment from the Australian Research Data Commons (ARDC). The ARDC is funded by the National Collaborative Research Infrastructure Strategy (NCRIS).\n"},{"title":"Principles","slug":"principles","content":"The Language Data Commons of Australia aims to ensure the long-term access to language data collections for analysis and reuse. Sustainable management of and access to these significant collections of intangible cultural heritage are underpinned by two sets of complementary guiding principles of data management and stewardship, namely the FAIR and CARE principles. \n\u003cbr /\u003e\n## FAIR Principles\n\u003cbr /\u003e\n\nThe FAIR principles were [first published](https://www.nature.com/articles/sdata201618) by a group of stakeholders representing academia, industry, funding agencies, and scholarly publishers. The principles aim to address issues surrounding data management and stewardship, focusing on four areas (which provide the FAIR acronym).\n\u003cbr /\u003e\n### Findable\nMetadata and data should be easy to find for both humans and computers. Making the data findable includes:\n- (Meta)data are assigned a globally unique and persistent identifier\n- Data are described with rich metadata \n- Metadata clearly and explicitly include the identifier of the data they\ndescribe.\n- (Meta)data are registered or indexed in a searchable resource.\n\u003cbr /\u003e\n\n### Accessible\nOnce the user finds the required data, she/he/they need to know how they can be accessed, possibly including authentication and authorisation.\n- (Meta)data are retrievable by their identifier using a standardised communications protocol\n    + The protocol is open, free, and universally implementable\n    + The protocol allows for an authentication and authorisation procedure, where necessary\n- Metadata are accessible, even when the data are no longer available\n\u003cbr /\u003e\n\n### Interoperable\nThe data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.\n- (Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation\n- (Meta)data use vocabularies that follow FAIR principles\n- (Meta)data include qualified references to other (meta)data\n\u003cbr /\u003e\n\n### Reusable\nThe ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings.\n- (Meta)data are richly described with a plurality of accurate and relevant attributes\n    + (Meta)data are released with a clear and accessible data usage license\n    + (Meta)data are associated with detailed provenance\n    + (Meta)data meet domain-relevant community standards\n\u003cbr /\u003e\n\u003cbr /\u003e\n\nThe Australian Research Data Commons ([ARDC](https://ardc.edu.au/)) supports FAIR data practices and initiatives that make data and related research outputs FAIR. At the same time, the ARDC acknowledges that the implementation of the principles will look different across disciplines and will need discipline-specific approaches and standards. \n\n## CARE Principles\nThe [CARE Principles for Indigenous Data Governance](https://www.gida-global.org/care) were developed by the Global Indigenous Alliance (GIDA); they are a response to the FAIR principles and aim to complement them. GIDA highlights how the FAIR principles and the open data movement focus on increasing data sharing among researchers and entities but do not take into account power differentials and historical contexts or fully engage with Indigenous Peoples’ rights and interests. These include the rights to generate value from Indigenous data in ways that are grounded in Indigenous worldviews and to advance Indigenous innovation and self-determination. The CARE Principles also focus on four areas.\n\u003cbr /\u003e\n\n### Collective Benefit\nData ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data.\n- For inclusive development and innovation\n- For improved government and citizen engagement\n- For equitable outcomes \n\u003cbr /\u003e\n\n### Authority to control\nIndigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered.\n- Recognizing rights and interests\n- Data for governance\n- Governance of data.\n\u003cbr /\u003e\n\n### Responsibility\nThose working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit.\n- For positive relationships\n- For expanding capability and capacity\n- For Indigenous languages and worldviews.\n\u003cbr /\u003e\n\n### Ethics\nIndigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.\n- For minimizing harm and maximizing benefit\n- For justice\n- For future use.\n\n\nBack to [Background](../background/)\n"},{"title":"Resources","slug":"resources","content":"# Resources\n\n- [Indigenous Knowledge](#indigenous-knowledge)\n- [Ethics](#ethics)\n- [Languages of the World](#languages-of-the-world)\n- [Language Archives](#language-archives)\n- [Australian Organisations](#australian-organisations)\n- [International Organisations](#international-organisations)\n- [Publications](#publications)\n- [Tools](#tools)\n\n\u003chr class=\"dots\" /\u003e\n\n### Indigenous knowledge {#indigenous-knowledge}\n\n- [Protocols](https://australiacouncil.gov.au/wp-content/uploads/2021/07/protocols-for-using-first-nati-5f72716d09f01.pdf)\n  for using First Nations Cultural and Intellectual Property in the Arts (Australia Council)\n- Citing Indigenous elders and knowledge keepers\n    - [Templates For Citing Indigenous Elders and Knowledge Keepers](https://kula.uvic.ca/index.php/kula/article/view/135)\n    (Lorisia Macleod)\n    - University of Alberta Library's [Guide](https://guides.library.ualberta.ca/c.php?g=715568\u0026p=5112574)\n    - University of Alberta Library's [blog](https://news.library.ualberta.ca/blog/2022/01/27/citing-indigenous-elders-and-knowledge-keepers/)\n- [TK Labels](https://localcontexts.org/labels/traditional-knowledge-labels/): Traditional Knowledge labels are an initiative for Indigenous communities and local organizations. Developed through sustained partnership and testing within Indigenous communities across multiple countries, the Labels allow communities to express local and specific conditions for sharing and engaging in future research and relationships in ways that are consistent with already existing community rules, governance and protocols for using, sharing and circulating knowledge and data.\n- Decolonising linguistics: [Spinning a better yarn](https://www.dynamicsoflanguage.edu.au/ialr/decolonising-linguistics-spinning-a-better-yarn/) \n- Rawlings, V, Flexner, J L, Riley, L (eds.) (2021) *[Community-Led Research: Walking New Pathways Together](https://open.sydneyuniversitypress.com.au/files/9781743327630.pdf)*. Sydney University Press\n\n#### Indigenous Languages of Australia\n\n- [First Languages Australia](https://www.firstlanguages.org.au/): First \n  Languages Australia encourages communication between communities, the \n  government and key partners whose work can impact Aboriginal and Torres \n  Strait Islander languages.\n- [Living Languages](https://www.livinglanguages.org.au/): Living Languages \n  provides grassroots training to people, communities and Language Centres in \n  Australia doing language work on the ground in remote, regional and urban \n  areas.\n- Language Centres: The Indigenous Languages and Arts (ILA) program  \n  currently supports a number of organisations, including a network of 20 \n  language centres. A list of these centres is available from \n  [this page](https://www.arts.gov.au/documents/indigenous-languages-and-arts-program-language-centres) \n  (pdf or docx format). \n- [Austlang](https://collection.aiatsis.gov.au/austlang/about): AustLang \n  provides a controlled vocabulary of persistent identifiers, a thesaurus of \n  languages and peoples, and information about Aboriginal and Torres Strait \n  Islander languages which has been assembled from referenced sources.\n- [Nyingarn](https://nyingarn.net/): a platform for primary sources in \n  Australian Indigenous languages \n- [Digital Daisy Bates](https://bates.org.au/): a collection of over 23,000 \n  pages of wordlists of Australian languages, originally recorded by Daisy \n  Bates in the early 1900s, made up of the original questionnaires and around \n  4,000 pages of typescripts.\n- [50 Words Project](https://50words.online/): The 50 Words Project showcases \n  words from 64 languages across Australia, with further languages being \n  added regularly as more communities around Australia become involved. For \n  each language you can hear the words spoken via a map that shows the \n  general location of the language.\n- [Gambay](https://www.abc.net.au/indigenous/gambay-languages-map): a map of \n  Australia's first languages.\n- [Australian Indigenous language collections](https://www.nla.gov.au/research-guides/indigenous-language-resources#): \n  a guide to materials held in the National Library of Australia, with links \n  to similar resources at State Libraries. \n- Living Archive of Australian Languages: see [below](#language-archives)\n- Contemporary and Historical Reconstruction in the Indigenous Languages of \n  Australia ([CHIRILA](http://www.pamanyungan.net/chirila/)): CHIRILA is a \n  lexical database (a database with words from different languages). \n  Currently there are about 780,000 words, from all over Australia, of which \n  about 20% is publicly available.\n\n### Ethics {#ethics}\n\n- Australian Institute of Aboriginal and Torres Strait Islander Studies [Code of ethics](https://aiatsis.gov.au/research/ethical-research/code-ethics)\n- Australian Linguistic Society [policies](https://als.asn.au/AboutALS/Policies): includes Policy on Linguistic rights of Aboriginal and Islander communities and Statement of Ethics.\n- [Australian Code for the Responsible Conduct of Research, 2018](https://www.nhmrc.gov.au/about-us/publications/australian-code-responsible-conduct-research-2018)\n- Linguistic Society of America: \n    - [Ethics Statement 2019](https://www.linguisticsociety.org/content/lsa-revised-ethics-statement-approved-july-2019)\n    - [Further resources](https://www.linguisticsociety.org/resource/ethics-further-resources)\n- [Tromsø recommendations for citation of research data in linguistics](https://www.rd-alliance.org/group/linguistics-data-ig/outcomes/troms%C3%B8-recommendations-citation-research-data-linguistics)\n\nLanguage and linguistics datasets are often not cited, or cited imprecisely, \nbecause of confusion surrounding the proper methods for citing them. For the \nuse of researchers and scholars in the field working with datasets, the \nTromsø recommendations propose components of data citation for referencing \nlanguage data, both in the bibliography and in the text of linguistics \npublications. \n\n### Languages of the world {#languages-of-the-world}\n\n- [Glottolog](https://glottolog.org/): Comprehensive reference information for the world's languages, especially the lesser known languages. LDaCA uses Glottolog [language codes](../metadata/#glottologhttpsglottologorg) in our metadata. \n\n- Open Language Archives Community ([OLAC](http://www.language-archives.org/)) is an international partnership of institutions and individuals who are creating a worldwide virtual library of language resources by: (i) developing consensus on best current practice for the digital archiving of language resources, and (ii) developing a network of interoperating repositories and services for housing and accessing such resources. OLAC harvests metadata and their web site has a search facility to find resources for languages. OLAC [metadata recommendations](../metadata/#olachttpwwwlanguage-archivesorg) are the basis for some of LDaCA's metadata.\n\n- [Ethnologue](https://www.ethnologue.com/): *Ethnologue* provides information about the languages of the world, but operates on a subscription model. The information which is available without a subscription is very limited: the first three lines of individual language entries which include the ISO 639-3 code, the classification of the language into a language family, and a link to the language’s OLAC page.\n\n- The World Atlas of Linguistic Structures Online ([WALS](https://wals.info/)) is a large database of structural (phonological, grammatical, lexical) properties of languages gathered from descriptive materials (such as reference grammars) by a team of 55 authors.\n\n### Language Archives {#language-archives}\n\n- [LAAL](https://livingarchive.cdu.edu.au/): \nThe Living Archive of Aboriginal Languages is a digital archive of endangered literature in Australian Indigenous languages of the Northern Territory. It contains nearly 4000 books in 50 languages from 40 communities available to read online or download freely. This is a living archive, with connections to the people and communities where the books were created. This will allow for collaborative research work with the Indigenous authorities and communities.\n- [PARADISEC](https://www.paradisec.org.au/): The Pacific and Regional Archive for Digital Sources in Endangered Cultures holds 14,500 hours of audio recordings and 2,000 hours of video recordings that might otherwise have been lost. These recordings are of performance, narrative, singing, and other oral tradition. This amounts to 150 terabytes, and represents 1,315 languages, mainly from the Pacific region.\n- [ELAR](https://www.elararchive.org/): The Endangered Languages Archive is a digital repository for preserving multimedia collections of endangered languages from all over the world, making them available for future generations.\n- [The Language Archive](https://archive.mpi.nl/tla/): The Language Archive (TLA) is an integral part of the Max Planck Institute for Psycholinguistics in Nijmegen. It contains various types of materials, including: audio and video language corpus data from languages around the world; photographs, notes, experimental data, and other relevant information required to document and describe languages and how people use them; records of speech in everyday interactions in families and communities; naturalistic data from adult conversations from endangered and under-studied languages, and linguistic phenomena.\n- [Kaipuleohone Language Archive](http://ling.hawaii.edu/kaipuleohone-language-archive/): Kaipuleohone is the digital language archive of the University of Hawaiʻi. Founded in 2008, the archive houses texts, images, audio, and video collected from around the world by linguists, anthropologists, ethnomusicologists, and more. Our collection includes a wealth of photographs, notes, dictionaries, transcriptions, and other materials related to small and endangered languages.\n- [DELAMAN](https://www.delaman.org/): The Digital Endangered Languages and Musics Archive is an international network of archives of data on linguistic and cultural diversity, in particular on small languages and cultures under pressure.\n\n### Australian Organisations {#australian-organisations}\n\n- The Australian Linguistic Society ([ALS](https://als.asn.au/Home)) is the national organisation for linguists and linguistics in Australia. Its primary goal is to further interest in and support for linguistics research and teaching in Australia.\n\n- The Applied Linguistics Association of Australia ([ALAA](https://alaa.net.au/)) is the national professional organisation for applied linguistics in Australia. We welcome academics, teachers, researchers, students and members of the wider community to join us and become part of an active community interested in questions, issues and problems that can be understood and addressed through a focus on language in our world.\n\n- The Australasian Langauge Technology Association ([ALTA](https://www.alta.asn.au/index.html)) has the purpose of promoting language technology research and development in Australia and New Zealand.\n\n- The Australasian Speech Science and Technology Association ([ASSTA](https://assta.org/)) is a scientific association which aims to advance the understanding of speech science and its application to speech technology in a way that is appropriate for Australia and New Zealand.\n\n- The Australian Institute of Aboriginal and Torres Strait Islander Studies ([AIATSIS](https://aiatsis.gov.au/)) is an Indigenous-led, national institute that celebrates, educates and inspires people from all walks of life to connect with the knowledge, heritage and cultures of Australia’s First Peoples.\n\n### International Organisations {#international-organisations}\n\n- Common Language Resources and Technology Infrastructure ([CLARIN](https://www.clarin.eu/)) is a research infrastructure that was initiated from the vision that all digital language resources and tools from all over Europe and beyond are accessible through an online environment for the support of researchers in the humanities and social sciences.\n\n- Endangered Languages Documentation Program ([ELDP](https://www.eldp.net/)) supports the documentation and preservation of endangered languages through granting, training and outreach activities. The collections compiled through our funding are freely accessible at the [Endangered Languages Archive](#language-archives).\n\n- The Linguistic Data Consortium ([LDC](https://www.ldc.upenn.edu/)) is an open consortium of universities, libraries, corporations and government research laboratories. LDC was formed in 1992 to address the critical data shortage then facing language technology research and development. Initially, LDC's primary role was as a repository and distribution point for language resources. but with the help of its members, LDC has grown into an organization that creates and distributes a wide array of language resources. \n\n### Publications {#publications}\n\n- [The Open Handbook of Linguistic Data Management](https://direct.mit.edu/books/book/5244/The-Open-Handbook-of-Linguistic-Data-Management) \nEdited by Andrea L. Berez-Kroeker, Bradley McDonnell, Eve Koller, Lauren B. Collister. MIT Press 2022.\n\n    \"A guide to principles and methods for the management, archiving, sharing, and citing of linguistic research data, especially digital data. “Doing language science” depends on collecting, transcribing, annotating, analyzing, storing, and sharing linguistic research data. This volume offers a guide to linguistic data management, engaging with current trends toward the transformation of linguistics into a more data-driven and reproducible scientific endeavor. It offers both principles and methods, presenting the conceptual foundations of linguistic data management and a series of case studies, each of which demonstrates a concrete application of abstract principles in a current practice.\"\n\n    This material is Open Access.\n\n- [Language Documentation and Conservation](https://nflrc.hawaii.edu/ldc/)\n\n    “LD\u0026C publishes papers on all topics related to language documentation and conservation, including, but not limited to, the goals of language documentation, data management, fieldwork methods, ethical issues, orthography design, reference grammar design, lexicography, methods of assessing ethnolinguistic vitality, archiving matters, language planning, areal survey reports, short field reports on endangered or underdocumented languages, reports on language maintenance, preservation, and revitalization efforts, plus software, hardware, and book reviews.”\n\n    LD\u0026C is an Open Access journal.\n\n- [Living languages/Lenguas vivas/Línguas vivas journal](https://scholarworks.umass.edu/livinglanguages/)\n\n    “Living Languages is an international, multilingual journal dedicated to topics in language revitalization and sustainability. The goal of the journal is to promote scholarly work and experience-sharing in the field. The primary focus is on bringing together language revitalization practitioners from a diversity of backgrounds, whether academic or not, within a peer-reviewed publication venue that is not limited to academic contributions and is inclusive of a diversity of perspectives and forms of expression.”\n\n    *Living Languages* is an Open Access journal.\n\n- [Pacific Linguistics](http://sealang.net/archives/pl/): a digital archive of many Pacific Linguistics publications up to 2012.\n\n### Tools {#tools}\n\n- [Audacity](https://www.audacityteam.org/): Audacity is a free, easy-to-use, multi-track audio editor and recorder for Windows, macOS, GNU/Linux and other operating systems.\n- [ELAN](https://archive.mpi.nl/tla/elan): a tool for making time-aligned annotations on audio and video recordings.\n- [Praat](https://www.fon.hum.uva.nl/praat/): Free software for doing phonetics by computer.\n- [FieldWorks](https://software.sil.org/fieldworks/): Software tools for managing linguistic and cultural data. FieldWorks supports tasks ranging from the initial entry of collected data through to the preparation of data for publication, including dictionary development, interlinearization of texts, morphological analysis, and other publications.\n- [Toolbox](https://software.sil.org/toolbox/): Toolbox is a data management and analysis tool for field linguists. It is especially useful for maintaining lexical data, and for parsing and interlinearizing text, but it can be used to manage virtually any kind of data.\n-  [AntConc](https://www.laurenceanthony.net/software/antconc/): A freeware corpus analysis toolkit for concordancing and text analysis.\n\n"},{"title":"Technologies","slug":"technologies","content":"LDaCA is basing its data storage on the two technologies described below. The overall approach is informed by the [Arkisto](https://arkisto-platform.github.io/) platform, taking the view that research data has interest and value that extends beyond funding cycles and its long term preservation and accessibility must continue to be managed. [This presentation](https://ptsefton.com/2022/02/18/hass_rdc_tech_advisory/index.html) gives further details of the technical architecture.\n\n## Research Object Crates ([RO-Crate](https://www.researchobject.org/ro-crate/))\n\nA Research Object ([RO](https://www.researchobject.org/)) is a structured archive of all the items that contributed to the research outcome, including their identifiers, provenance, relations and annotations. [RO-Crate](https://www.researchobject.org/ro-crate/) is a lightweight approach to packaging research data with their metadata. It is based on [schema.org](https://schema.org/) annotations in [JSON-LD](https://json-ld.org/), and aims to make best-practice in formal metadata description accessible and practical for use in a wide variety of situations. While RO-Crates can be considered general-purpose containers of arbitrary data and open-ended metadata, in practical use within a particular domain, application or framework, it is beneficial to further constrain RO-Crate to a specific profile: a set of conventions, types and properties that one minimally can require and expect to be present in that subset of RO-Crates. LDaCA is developing such a profile to be used for language data.\n\n\n## Oxford Common File Layout ([OCFL](https://ocfl.io/))\n\nOCFL is a specification for laying out digital collections on file or object storage. It is designed with long-term preservation principles in mind and does not rely on specialised software. Amongst the benefits of using OCFL with RO-Crate objects are:\n- completeness: a repository can be re-indexed from the files it stores\n- versioning: repositories can make changes to objects and still allow their history to persist\n\u003cbr /\u003e\n\u003cbr /\u003e\n\n\u003cbr /\u003e\n\u003cbr /\u003e\n\nBack to [Background](../background/)\n"}]}},"__N_SSG":true},"page":"/[page]","query":{"page":"principles"},"buildId":"AWBu_R8MsSXbxYahFIoQG","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>