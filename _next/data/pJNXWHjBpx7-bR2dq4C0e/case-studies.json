{"pageProps":{"content":"<h2 id=\"id1\">Fieldwork in Papua New Guinea</h2>\n<br />\n<p>Harriet Sheppard's <a href=\"../fieldwork-png\">account</a> of collecting data on a remote island in Papua New Guinea.</p>\n<br />\n<h2 id=\"id2\">Fieldwork in Sydney</h2>\n<br />\n<br />\n<h2 id=\"id3\">Data in a Language Centre</h2>\n<br />\n<br />\n<h2 id=\"id4\">Archival data</h2>\n<br />\n<br />\n<p>Back to <a href=\"../background/\">Background</a></p>\n","searchContent":{"posts":[{"title":"What are the FAIR and CARE principles and why should corpus linguists know about them?","slug":"fair-and-care","tags":["FAIR","CARE"],"content":"# FAIR and CARE\nData is becoming increasingly important in today’s world, so corpus linguists might feel that the rest of the world is finally catching up. But the rest of the world are bringing with them new approaches to how data is handled. This means that fields such as corpus linguistics may need to reassess their practices. Such reassessment includes addressing concerns about how data is stored and who can access it (data stewardship) – concerns that are a part of the [Open Science](https://en.wikipedia.org/wiki/Open_science) movement, ultimately grounded on principles of equity and accountability. \n\nThe most influential approach to data stewardship today is the [FAIR](https://www.go-fair.org/) principles.\nAccording to these principles, data should be:\n- *Findable* \n<br />\n&emsp; Metadata and data should be easy to find for both humans and computers. \n- *Accessible*\n<br />\n&emsp; Once the user finds the required data, she/he/they need to know how can they be accessed, possibly including authentication and authorisation.\n- *Interoperable*\n<br />\n&emsp; The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.\n- *Reusable*\n<br />\n&emsp; The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings.\n<br /><br />\n\nIn general, corpus linguists do well on the interoperability criterion. Corpus data is usually stored in non-proprietary formats; even when some structure is imposed on the data, this is almost always in a form which is saved as a simple text file (e.g. csv files or xml annotations). Data stored in such formats is easy to move between applications. But what about the other three criteria? \n\nSome corpus data is easy to discover; it is findable. For example CLARIN, the [portal](https://www.clarin.eu/content/data) to the European Union language resource infrastructure, provides access to many large data collections, as does the [Linguistic Data Consortium](https://www.ldc.upenn.edu/) in the USA. However, some data is never made part of a large collection and often remains under the control of individual researchers or research teams. Such data may be almost impossible to find. Even if we can find such data, it is unlikely to be accompanied by good descriptions of the data and metadata, making reusability problematic. Of course, big corpora such as the [British National Corpus](http://www.natcorp.ox.ac.uk/) will be both findable and accompanied by comprehensive corpus manuals. However, it is worth considering how to make other corpora more findable, including the provision of corpus manuals or corpus descriptions. Corpus resource databases such as [CoRD](https://varieng.helsinki.fi/CoRD/) do aim to work towards this principle.\n\nAccessibility may also be an issue for some data. Copyright law may allow use of material for individual research but prohibit any further distribution of the material. The FAIR approach to such cases is that metadata should be available so that interested parties can know that a data holding exists (F), and the metadata will include information about the conditions under which the data may or may not be shared or reused (A and R). \n\n![FAIR and CARE principles](/fair-care.png)\n\nImage from Global Indigenous Data Alliance (https://www.gida-global.org/)\n<br /><br />\nFor linguists, there is another very important set of principles concerning data, the CARE principles developed by the Global Indigenous Data Alliance:\n<br />\n- *Collective Benefit*\n<br />\n&emsp; Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data.\n<br />\n- *Authority to control*\n<br />\n&emsp;Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered.\n<br />\n- *Responsibility*\n<br />\n&emsp;Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit.\n<br />\n- *Ethics*\n<br />\n&emsp;Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.\n\n<br />\nThese principles are presented as applying particularly to Indigenous data, but we believe that researchers should adopt this approach in all cases where the people who participate in our research can be seen to have some moral rights in the information they have contributed. Respecting those moral rights should be demonstrated by recognising the participants’ authority to control how data is used, by seeking to ensure that participants derive benefit from use of the data, and by acting ethically and transparently in our relations with the participants. Deborah Cameron and her colleagues (Cameron et al 1993) raised similar issues almost 20 years ago, arguing that the imbalance of power in the relation between researchers and participants needed to be reduced. The CARE principles continue along this path, but go even further in explicitly returning power to the sources of information. \n\nCorpus data is often written language. We have already mentioned that copyright law is relevant to some such material, and that body of law protects at least some rights for the creators of the material. But corpus linguists also work with other kinds of data such as spoken language (spontaneous or produced as a response to some prompt) or written material produced by research participants according to some protocol. In such cases, ethical research practice should include addressing the issues raised by the CARE principles. Some aspects of this practice will fall under institutional ethics requirements (for example, thinking carefully about what permissions we request on consent forms), but other questions must be part of the relationship between the researcher and the research participants. Corpus linguists working with spoken, computer-mediated, or otherwise particularly sensitive data have been aware of at least some of these issues, but the CARE principles offer an opportunity to go further.\n\nAcquiring data for linguistic research takes effort and often that means money. It is therefore a good use of resources if any data we collect can be used by others. The FAIR principles provide a framework to make sharing and reusing data easier, and applying the CARE principles where relevant helps to ensure that our research has a sound ethical basis.\n\n<br />\n<hr />\n<br />\n\nNote: This post is based on the presentation ‘Advance Australia FAIR’,  given by Simon Musgrave and Michael Haugh to the 4th Forum on Englishes in Australia (LaTrobe University, August 27, 2021). \n\n<br />\n\nThanks to Leah Gustafson and Monika Bednarek for helpful comments on drafts.\n\n<br />\n\n**Reference:**\nCameron, Deborah, Elizabeth Frazer, Penelope Harvey, Ben Rampton & Kay Richardson. 1993. Ethics, advocacy and empowerment: Issues of method in researching language. Language & Communication 13(2). 81–94. [https://doi.org/10.1016/0271-5309(93)90001-4](https://doi.org/10.1016/0271-5309(93)90001-4)\n\n\n\n"},{"title":"HASS RDC Technical Advisory Group Meeting LDaCA & ATAP Intro","slug":"rdc-tech-meeting","tags":["Repositories"],"content":"\nThis is a presentation Peter Sefton gave to the \n[Humanities, Arts and Social Sciences Research Data Commons and Indigenous Research Capability Program](https://ardc.edu.au/collaborations/strategic-activities/hass-and-indigenous-research-data-commons/)  Technical Advisory Group on Friday 11th February 2022. \n\nThanks to Simon Musgrave for reviewing this and adding a little detail here \nand there.\n\n(This is also available on [Dr Sefton's site](https://ptsefton.com/2022/02/18/hass_rdc_tech_advisory/index.html))\n\n<a href=\"/rdc-tech-meeting/HASS RDC Technical Advisory Group Meeting LDaCA & ATAP Intro.pdf\">PDF version</a> \n\n![Hass RDC Technical Avisory Group Meeting](/rdc-tech-meeting/Slide00.png)\n\nThe Language Data Commons of Australia Data Partnerships (LDaCA) and the Australian Text Analytics Platform (ATAP) are establishing a scalable and flexible language data and analytics commons. These projects will be part of the Humanities and Social Sciences Research Data Commons (HASS RDC).\nThe Data Commons will focus on preservation and discovery of distributed multi-modal language data collections under a variety of governance frameworks. This will include access control that reflects ethical constraints and intellectual property rights, including those of Aboriginal and Torres Strait Islander, migrant and Pacific communities.\n\n![Slide01](/rdc-tech-meeting/Slide01.png)\n\nFor this Research Data Commons work we are using the Arkisto Platform \n(introduced [at eResearch 2020](http://ptsefton.com/2020/11/23/Arkisto/index.html)).\n\nArkisto aims to secure the long term preservation of data independently of \ncode and services - recognizing the ephemeral nature of software and platforms.\nWe know that sustaining software platforms can be hard and aim to make sure \nthat important data assets are not locked up in database or hard-coded logic \nof some hard-to-maintain application.\n\nWe are using three key standards on this project …\n\n\n![Slide02](/rdc-tech-meeting/Slide02.png)\n\nThe first standard is the [Oxford Common File Layout](https://ocfl.io/1.0/spec/) - \nthis is a way of keeping version controlled digital objects on a plain old \nfilesystem or object store. \n\nHere’s the introduction to the spec:\n\n> **Introduction**\n>\n> This section is non-normative.\n>\n> This Oxford Common File Layout (OCFL) specification describes an application-independent approach to the storage of digital objects in a structured, transparent, and predictable manner. It is designed to promote long-term access and management of digital objects within digital repositories.\n>\n> **Need**\n>\n> The OCFL initiative began as a discussion amongst digital repository practitioners to identify well-defined, common, and application-independent file management for a digital repository's persisted objects and represents a specification of the community’s collective recommendations addressing five primary requirements: completeness, parsability, versioning, robustness, and storage diversity.\n>\n> **Completeness**\n>\n> The OCFL recommends storing metadata and the content it describes together so the OCFL object can be fully understood in the absence of original software. The OCFL does not make recommendations about what constitutes an object, nor does it assume what type of metadata is needed to fully understand the object, recognizing those decisions may differ from one repository to another. However, it is recommended that when making this decision, implementers consider what is necessary to rebuild the objects from the files stored.\n>\n> **Parsability**\n>\n> One goal of the OCFL is to ensure objects remain fixed over time. This can be difficult as software and infrastructure change, and content is migrated. To combat this challenge, the OCFL ensures that both humans and machines can understand the layout and corresponding inventory regardless of the software or infrastructure used. This allows for humans to read the layout and corresponding inventory, and understand it without the use of machines. Additionally, if existing software were to become obsolete, the OCFL could easily be understood by a light weight application, even without the full feature repository that might have been used in the past.\n>\n> **Versioning**\n>\n> Another need expressed by the community was the need to update and change objects, either the content itself or the metadata associated with the object. The OCFL relies heavily on the prior art in the [Moab] Design for Digital Object Versioning which utilizes forward deltas to track the history of the object. Utilizing this schema allows implementers of the OCFL to easily recreate past versions of an OCFL object. Like with objects, the OCFL remains silent on when versioning should occur recognizing this may differ from implementation to implementation.\n>\n> **Robustness**\n>\n> The OCFL also fills the need for robustness against errors, corruption, and migration. The versioning schema ensures an OCFL object is robust enough to allow for the discovery of human errors. The fixity checking built into the OCFL via content addressable storage allows implementers to identify file corruption that might happen outside of normal human interactions. The OCFL eases content migrations by providing a technology agnostic method for verifying OCFL objects have remained fixed.\n>\n> **Storage diversity**\n> Finally, the community expressed a need to store content on a wide variety of storage technologies. With that in mind, the OCFL was written with an eye toward various storage infrastructures including cloud object stores.\n\n![Slide03](/rdc-tech-meeting/Slide03.png)\n\nThe second standard is Research Object Crate. (RO-Crate) a method for \ndescribing any dataset of local or remote resources as a digital object using \na **single linked-data metadata document**.\n\nRO-Crate is used in our platform both for describing data objects in the OCFL \nrepository, and for delivering metadata over the API (which we’ll show in \narchitecture diagrams and screenshots below).\n\n![Slide04](/rdc-tech-meeting/Slide04.png)\n\nRO-Crates may contain any kind of data resource about anything, in any format \nas a file or URL - it’s not just for language data; there are also many \nprojects in the sciences starting to \n[use RO-Crate](https://www.researchobject.org/ro-crate/in-use/).\n\n\n![Slide05](/rdc-tech-meeting/Slide05.png)\n\nThis image is taken from a \n[presentation on digital preservation](https://slideplayer.com/slide/3919920/).\n\n[Models](https://pcdm.org/2016/04/18/models)\nThe third key standard for Arkisto is the Portland Common Data Model. Like \nOCFL, this was developed by members of the digital library/repository \ncommunity. It was devised as a way to do interchange between repository \nsystems, most of which, it turned out had evolved very similar ways of having \nnested collections, digital objects that aggregate related files. Using this \nvery simple ontology allows us to store data in the OCFL layer in a very \nflexible way - depending on factors like data size, licensing and whether \ndata is likely to change or need to be withdrawn, we can store entire \ncollections as OCFL objects or across many OCFL objects with PCDM used to \nshow the structure of the data collections regardless of how they happen to \nbe stored.\n\n![Slide06](/rdc-tech-meeting/Slide06.png)\n\nBack to RO-Crates.\n\nRO-Crates are self-documenting and can ship with a HTML file that allows a \nconsumer of the crated data to see whatever documentation the crate authors \nhave added.\n\nThis crate contains an entire collection (RepositoryCollection is the RO-\nCrate term that corresponds to pcdm:Collection). \n\nCrates must have license information that set out how data may be used and if \nit may be redistributed. As we are dealing with language data which is (almost\n) always created by people, it is important that their intellectual property \nrights and privacy are respected. More on this later.\n\n\n![Slide07](/rdc-tech-meeting/Slide07.png)\n\nThis shows a page for what we’re calling an Object (RepositoryObject). A \nRepositoryObject is a single “thing” such as a document, a conversation, a \nsession in a speech study. (this was called an item in Alveo but given that \nboth the Portland Common Data model and Oxford Common File Layout use “Object\n” we are using that term at least for now).\n\nThis shows that the system is capable of dealing with unicode characters - \nwhich is good, as you would expect as it’s 2022 and this is a Language Data \nCommons, but there are still challenges, like dealing with mixtures of left \nto right and right to left text, and we need to find or define metadata terms \nto keep track of “language”, “writing system”, and the difference between \nthings that started as orthographic (written) text, vs spoken or signed etc. \nThere’s a group of us working on that, currently led by Nick Thieberger and \nPeter Sefton.\n\nSimon Musgrave and Peter Sefton \n[presented our progress with multilingual text](https://ptsefton.com/2022/01/27/DAMTA_Slides_v1/) \nat a virtual workshop run by ANU in January.\n\n\n![Slide08](/rdc-tech-meeting/Slide08.png)\nHere’s another screenshot showing one of the government documents in PDF \nformat - with a link back to the abstract RepositoryObject that houses all of \nthe manifestations of the document in various languages.\n\n![Slide09](/rdc-tech-meeting/Slide09.png)\n\nThe above diagram takes a big-picture view of research data management in the \ncontext of *doing* research. It makes a distinction between managed \nrepository storage and the places where work is done - “workspaces”. \nWorkspaces are where researchers collect, analyse and describe data. \nExamples  include the most basic of research IT services, file storage as \nwell as analytical tools such as Jupyter notebooks (the backbone of ATAP - \nthe text analytics platform). Other examples of workspaces include code \nrepositories such as GitHub or GitLab (a slightly different sense of the word \nrepository), survey tools, electronic (lab) notebooks and bespoke code \nwritten for particular research programmes - these workspaces are essential \nresearch systems but usually are not set up for long term management of data.\n\nThe cycle in the centre of  this diagram shows an idealised research practice \nwhere data are collected and described and deposited into a repository \nfrequently. Data are made findable and accessible  as soon as possible and \ncan be “re-collected” for use and re-use.\n\nFor data to be re-usable by humans and machines (such as ATAP notebook code \nthat consumes datasets in a predictable way) it must be well described. The \nATAP and LDaCA approach to this is to use the Research Object Crate (RO-Crate\n) specification. RO-Crate is essentially a guide to using a number of \nstandards and standard approaches to describe both data and re-runnable \nsoftware such as workflows or notebooks.\n\n\n![Slide10](/rdc-tech-meeting/Slide10.png)\n\nThis rather messy slide captures the overall high-level architecture for the \nLDaCA Research Data Commons  - there will be an analytical workbench (left of \nthe diagram) which is the basis of the Australian Text Analytics (ATAP) \nproject - this will focus on notebook-style programming using one of the \nemerging Jupyter notebook platforms in that space. (This is not 100% decided \nyet, but that has not stopped the team from starting to collect and develop \nnotebooks that open up text analytics to new coders from the linguistics \ncommunity.) Our engagement lead, Dr Simon Musgrave sees the ATAP work as \nprimarily an educational enterprise encouraging researchers to adopt new \nresearch practices - which will be underpinned by services built on the \nArkisto standards that allow for rigorous, re-runnable research.\n\n![Slide11](/rdc-tech-meeting/Slide11.png)\n\nIn this presentation we are going to focus on the portal/repository \narchitecture more than on the ATAP notebook side of things. We know that we \nwill be using (at least) the SWAN Jupyter notebook service perceived by \nAARNet but we are still scoping how notebooks will be made portable between \nsystems and where they will be stored at various stages of their development. \nWe will be supporting and encouraging researchers to archive notebooks \nwrapped in RO-Crates with re-use information OUTSIDE of the SWAN platform \nthough - it’s a workspace, not a repository; it does not have governance in \nplace for long term preservation.\n\n\n![Slide12](/rdc-tech-meeting/Slide12.png)\n\nThis is a much simpler view zooming in on the core infrastructure components \nthat we have built so far. We are starting with bulk ingest of existing \ncollections and will add one-by-one deposit of individual items after that.\n\nThis show the OCFL repository at the bottom - with a Data & Access API that \nmediates access. This API understands the RO-Crate format and in particular \nits use of the Portland Common Data Model to structure data. The API also \nenforces access control to objects; every repository object has a license \nsetting out the terms of use and re-use for its data, which will reflect the \nway the data were collected - whether participants signed agreements, ethics \napprovals and privacy law are all relevant here. Each license will correspond \nto a group of people who have agreed to and/or been selected by a data \ncustodian. We are in negotiations with the \n[Australian Access Federation (AAF)](https://aaf.edu.au/) to use their \n[CILogon](https://www.cilogon.org/) service for this authorization step and \nfor authentication of users across a wide variety of services including the \nAAF itself and Google, Microsoft, GitHub etc.\n\nThere’s also an access portal which will be based on a full-text index (at \nthis stage we’re using ElasticSearch) which is designed to help people find \ndata they might be interested in using. This follows current conventions for \nbrowse/search interfaces which we’re familiar with from shopping sites - you \ncan search for text and/or drill down using *facets* (which are called \naggregations in Elastic-land). eg which language am in interested in or do I \nwant [ ] Spoken or [ ] Written material? \n\n\n![Slide13](/rdc-tech-meeting/Slide13.png)\n\nThis architecture is very modular and designed to operate in a distributed \nfashion, potentially with distributed file and/or object based repositories \nall being indexed by a centralised service. There may also be other ‘flavours’\nof index such as triple or graph stores, relational databases that ingest \ntabular data or domain specific discovery tools such as corpus analysis \nsoftware. And, there may be collection specific portals that show a slice of \na bigger repository with features or branding specific to a subset of data.\n\n\n![Slide14](/rdc-tech-meeting/Slide14.png)\n\nThis implementation of the Arkisto standards-stack is known as Oni.  That’s \nnot really an acronym any more though it once stood for OCFL, Ngnix (a web \nserver) or Node (a Javascript framework)  and an Index. An Oni is a kind of \nJapanese demon. 👹\n\n\n![Slide15](/rdc-tech-meeting/Slide15.png)\n\nBut how will data get into the OCFL repository? At the moment we’re loading \ndata using a series of scripts which are being developed at our github \norganization.\n\nThis diagram and the next come from the \n[Arkisto Use cases page](https://arkisto-platform.github.io/use-cases/) it \nshow how we will be converting data from existing collections into a form \nwhere they can be preserved in an OCFL repository and be part of a bigger \ncollection, ALWAYS with access control based on licenses.\n\n\n![Slide16](/rdc-tech-meeting/Slide16.png)\n\nThis is a screenshot our github repository showing the corpus migration tools \nwe’ve started developing (there are six, and one general purpose text-\ncleaning tool). These repositories have not all been made public yet, but \nthey will be - they contain tools to build Arkisto-ready file repositories \nthat can be made available in one or more portals \n\n\n![Slide17](/rdc-tech-meeting/Slide17.png)\n\nHere’s our portal which give a browse interface to allow drill-down data discovery.\n\nBut wait! That’s not the LDaCA portal - that’s Alveo!\n\nOh yes, so it is.\n\nAlveo was built ten years ago - and has not seen a much uptake.\n\n![Slide18](/rdc-tech-meeting/Slide18.png)\n\nThis screenshot shows some of the browse facets for the COOEE corpus, which \ncontains early Australian **written** English materials. But facets like `\nWritten Mode` and `Communication Medium` both of which are known for COOEE \nare not populated.\n\nThere are a quite few things that were wrong with Alveo - like we obviously \ndidn’t get all the metadata populated to the level that it would make these \nbrowse facets actually useful for filtering. But more importantly, there was \nnot enough work done to check which browse facets *are* useful and not enough \nof the budget was able to be spent on user engagement and training rather \nthan software development.\n\nOne of my current LDaCA senior colleagues said to me a couple of years ago \nthat Alveo was useless:  “I just wanted to get all the data” they siad. Me, I \nwas thinking “but it has an API so you CAN get all the data - what’s the \nproblem?”. We have tried not to repeat this mistake by making sure that the \nAPI delivers entire collections and we have some demonstrations of doing this \nfor real work.\n\nAnother colleague who was actually on the Alveo team said that this interface \nwas \"equally useless for everyone\", and they later built a custom interface \nfor one of the collections.\n\nWe’re taking these lessons to heart in designing the LDaCA infrastructure - \nmaking sure that as we go we have people using the software - it helps that \nwe have an in house (though distributed) development team rather than an \nexternal contractor so feedback is very fast - we can jump onto a call and \ndemo stuff at any time.\n\n![Slide19](/rdc-tech-meeting/Slide19.png)\n\nWe decided to build from the data API first.\n\nIn this demo developer Moises Sacal Bonequi is looking at the API via the \nPostman tool. This demonstration shows how the API can be used to find \ncollections (that conform to our metadata profile)  \n\n1. First he lists the collections, then chooses one\n2. He then gets a collection with the `&resolve` parameter, meaning that the \n   API will internally traverse the PCDM collection hierarchy and return ALL \n   the metadata for the collection - down to the file level\n3. He then downloads a file (for which he has a license that most of you \n   reading this don’t have - hence the obfuscation of the \n   dialogue)\n\nThis API has been used and road tested at ANU to develop techniques for topic \nmodelling on the Sydney Speaks corpus (more about which corpus below) - by a \nstudent Marcel Reverter-Rambaldi under the supervision of  Prof Catherine \nTravis at ANU - we are hoping to publish this work as a re-usable notebook \nthat can be adapted for other projects, and to allow the techniques the ANU \nteam have been developing to be applied to other similar data in LDaCA.\n\n![Slide20](/rdc-tech-meeting/Slide20.png)\n\nAnd one of the data scientists who was working with us at UQ, Mel Mistica, \ndeveloped a [demonstration notebook](https://github.com/Australian-Text-Analytics-Platform/ro-crate-metadata/blob/main/ro-crate-metadata.ipynb) \nwith our tech team that used the API to access another full collection (which \nis also suitable for the ANU topic modelling approach) - this notebook gets \nall the metadata for a small social history collection which contains \ntranscribed interviews with women in Western Sydney and shows how a data \nscientist might explore what’s in it and start asking questions about the data,\nlike the age distribution of the participants and start digging in to what \nthey were talking about.\n\n\n![Slide21](/rdc-tech-meeting/Slide21.png)\n\nThis screencast shows a work-in-progress snapshot of the Oni portal we talked \nabout above in action, showing how search and browse might be used to find \nrepository objects from the index - in this case searching for Arabic words \nin a small set of Australian Government documents.\n\n![Slide22](/rdc-tech-meeting/Slide22.png)\n\nHang on!\n\nYou keep talking about\n[“repositories”](http://ptsefton.com/2012/02/14/an-australian-research-data-repository/),\ndon’t you always say stuff like, \"A repository is not just a software \napplication. It’s a lifestyle. It’s not just for Christmas?\"\n\nThat’s right - we’ve been talking about repository software architectures \nhere but it is important to remember that a repository needs to be considered \nan institution rather than a software stack or collection of files, more\n\"University Library\" than \"My Database\".\n\n![Slide23](/rdc-tech-meeting/Slide23.png)\n\nThe next half a dozen slides are based on \n[a presentation](https://ptsefton.com/2021/10/12/ldaca2021/index.html)\nI gave at eResearch Australasia 2021 with Moises Sacal Bonequi\n\nToday we will look in detail at one important part of this architecture - \naccess control. How can we make sure that in a distributed system, with \nmultiple data repositories and registries residing with different data \ncustodians, the right people have access to the right data?\n\nI didn’t spell this out in the recorded conference presentation, but for data \nthat resides in the repositories at the right of the diagram we want to \nencourage research processes that clearly separate data from code. Notebooks \nand other code workflows that use data will fetch a version-controlled \nreference copy from a repository - using an access key if needed, process the \ndata and produce results that are then deposited into an appropriate \nrepository alongside the code itself.  Given that a lot of the data in the \nlanguage world is NOT available under open licenses such as Creative Commons \nit is important to establish this practice - each user of the data must \nnegotiate or be granted access individually. Research can still be \nreproducible using this model, but without a culture of sharing datasets \nwithout regard for the rights of those who were involved in the creation of \nthe data.\n\n![Slide24](/rdc-tech-meeting/Slide24.png)\nRegarding rights, our project is informed by the \n[CARE principles](https://www.gida-global.org/care) for Indigenous data.\n\n> The current movement toward open data and open science does not fully \n> engage with Indigenous Peoples rights and interests. Existing principles \n> within the open data movement (e.g. FAIR: findable, accessible, interoperable\n> , reusable) primarily focus on characteristics of data that will facilitate \n> increased data sharing among entities while ignoring power differentials and \n> historical contexts. The emphasis on greater data sharing alone creates a \n> tension for Indigenous Peoples who are also asserting greater control over \n> the application and use of Indigenous data and Indigenous Knowledge for \n> collective benefit\n\nBut we do not see the CARE principles as only applying to Indigenous data and \nknowledge. Most language data is a record of the behaviour of people who have \nmoral rights in the material (even if they do not have legal rights) and \ntaking the CARE principles as relevant in such cases ensures serious thinking \nabout the protection of tose moral rights.\n\n![Slide25](/rdc-tech-meeting/Slide25.png)\n[Traditional Knowledge Labels](https://localcontexts.org/labels/traditional-knowledge-labels/)\n\nWe are designing the system so that it can work with diverse ways of \nexpressing access rights, for example licensing like the Tribal Knowledge \nlabels.The idea is to separate safe storage of data with a license on each \nitem, which may reference the TK labels from a system that is administered by \nthe data custodians who can make decisions about who is allowed to access data.\n\n![Slide26](/rdc-tech-meeting/Slide26.png)\nWe are working on a case-study with the \n[Sydney Speaks project](http://www.dynamicsoflanguage.edu.au/sydney-speaks/) \nvia steering committee member Catherine Travis.\n\n> This project seeks to document and explore Australian English, as spoken in \n  Australia’s largest and most ethnically and linguistically diverse city – Sydney. \n> The title “Sydney Speaks” captures a key defining feature of the project: \n  the data come from recorded conversations between Sydney siders, as they tell \n  stories about their lives and experiences, their opinions and attitudes. This \n  allows us to measure how their lived experiences impact their speech \n  patterns.\n> Working within the framework of variationist sociolinguistics, we examine \n  variation in phonetics, grammar and discourse, in an effort to answer \n  questions of fundamental interest both to Australian English, and language \n  variation and change more broadly, including:\n> - How has Australian English as spoken in Sydney changed over the past 100 years?\n> - Has the change in the ethnic diversity over that time period (and in particular, over the past 40 years) had any impact on the way Australian English is spoken?\n> - What affects the way variation and change spread through society\n>     - Who are the initiators and who are the leaders in change?\n>     - How do social networks function in a modern metropolis?\n>     - What social factors are relevant to Sydney speech today, and over time (gender? class? region? ethnic identity?)\n> A better understanding of what kind of variation exists in Australian \n  English, and of how and why Australian English has changed over time can \n  help society be more accepting of speech variation and even help address \n  prejudices based on ways of speaking.\n> Source: <http://www.dynamicsoflanguage.edu.au/sydney-speaks/>\n\nThe collection contains recordings of people speaking both contemporary and \nhistoric.\n\nBecause this involved human participants there are restrictions on the \ndistribution of data - a situation we see with lots of studies involving \npeople in a huge range of disciplines.\n\n\n![Slide27](/rdc-tech-meeting/Slide27.png)\nThere are four tiers of data access we need to enforce and observe for this \ndata based on the participant agreements and ethics arrangements under which \nthe data were collected.\n\nConcerns about rights and interests are important for any data involving \npeople - and a large amount the data both indigenous and non-indigenous we \nare using will require access control that ensures that data sharing is \nappropriate. \n\n\n![Slide28](/rdc-tech-meeting/Slide28.png)\n\nIn this example demo we uploaded various collections and are authorising with \nGithub organisations \n\nIn a our production release we will use AAF to authorise different groups.\n\nLet's find a dataset: The Sydney Speaks Corpus.\n\nAs you can see we cannot see any data\n\nLets login… We authorise Github…\n\nNow you can see we have access sub corpus data and I am just opening a couple of items\n\n—\n\nNow in Github we can see the group management example. \n\nI have given access to all the licences to myself, as you can see here and \ngiven access to licence A to others.\n\n\n![Slide29](/rdc-tech-meeting/Slide29.png)\n\nThis diagram is a sketch of the interaction that took place in the demo - it \nshows how a repository can delegate authorization to an external system - in \nthis case Github rather than CILogon. But we are working with the ARDC to set \nup a trial with the Australian Access Federation to allow CILogon access for \nthe HASS Research Data Commons so we can pilot group-based access control.\n\n\n![Slide30](/rdc-tech-meeting/Slide30.png)\n\nThere’s a lot still to do.\n"}],"pages":[{"title":"Background","slug":"background","content":"\n# Language Data - Background Information\n### [Principles](/principles)\nInformation about the principles on which the work of LDaCA is based.\n\n### [Technologies](/technologies)\nInformation about the technologies being used in LDaCA.\n\n### [Metadata](/metadata)\nInformation about the approach to \n[metadata](https://en.wikipedia.org/wiki/Metadata) being taken by LDaCA.\n\n### [Sample Collections](/collections)\nInformation about the first datasets which have been added to LDaCA.\n\n### [Case Studies](/case-studies)\nAccounts by collectors of various kinds of language data illustrating the \ndifferent solutions they have adopted for the problems encountered in the \nprocess.\n\n<hr class=\"dots\" />\n"},{"title":"Case-studies","slug":"case-studies","content":"## Fieldwork in Papua New Guinea\n<br />\n\nHarriet Sheppard's [account](../fieldwork-png) of collecting data on a remote island in Papua New Guinea.\n\n<br />\n\n## Fieldwork in Sydney\n<br />\n<br />\n\n## Data in a Language Centre\n<br />\n<br />\n\n## Archival data\n<br />\n<br />\n\nBack to [Background](../background/)\n"},{"title":"Collections","slug":"collections","content":"LDaCA has begun ingesting data sets, including:\n\n- Corpus of Oz Early English [CoOEE](https://varieng.helsinki.fi/CoRD/corpora/COOEE/basic.html): a collection of texts written in Australia between 1788 and 1900. The corpus is divided into four time periods (1788–1825, 1826–1850, 1851–1875 and 1876–1900) each holding about 500,000 words. Four registers were defined for CoOEE: the Speech-based Register (SB), the Private Written Register (PrW), the Public Written Register (PcW) and the register of Government English (GE). In every Period 1-4 there is a similar number of words in the different registers.\n- [Sydney Speaks](https://www.dynamicsoflanguage.edu.au/sydney-speaks/): This project seeks to document and explore Australian English, as spoken in Australia’s largest and most ethnically and linguistically diverse city – Sydney. \n- [From Farms to Freeways](http://omeka.uws.edu.au/farmstofreeways/): This research project sought to analyse the experiences of women who had lived in the Blacktown and Penrith areas since the early 1950s, including their responses to social changes brought about by rapid suburbanisation in the Western Sydney region in the post-war period. Two-hour taped discussions were held with 34 women, aged sixty and over, who were in their early twenties during the Western Sydney region's population growth.\n\nWe have also ingested a collection of government documents in various languages. This a very small dataset assembled to check that our technology can handle different languages and different scripts; more information about this work is available in [this presentation](https://ptsefton.com/2022/01/27/DAMTA_Slides_v1/index.html).\n\nBack to [Background](../background/)\n"},{"title":"Contact us","slug":"contact","content":"\n### Contact us\n\nYou can contact the Language Data Commons of Australia by [email](mailto:info@ldaca.edu.au)\n\nOur logo was designed by Otis Carmichael. [Read more](/designer) about Otis and the ideas behind his design.\n\nWe share a [Twitter account](https://twitter.com/LDaCA_Program) with the [Australian Text Analytics Platform](https://www.atap.edu.au):<br>\n(if the tweets don't load, please try refreshing the page)\n\n<a class=\"twitter-timeline\" href=\"https://twitter.com/LDaCA_Program?ref_src=twsrc%5Etfw\" data-width=\"500\"\n  data-height=\"1000\" data-tweet-limit=\"5\">Tweets by LDaCA_Program</a> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"},{"title":"Language Data","slug":"data","content":"\n# Language Data\n### [Principles](/principles)\nInformation about the principles on which the work of LDaCA is based.\n\n### [Technologies](/technologies)\nInformation about the technologies being used in LDaCA.\n\n### [Metadata](/metadata)\nInformation about the approach to \n[metadata](https://en.wikipedia.org/wiki/Metadata) being taken by LDaCA.\n\n### [Sample Collections](/collections)\nInformation about the first datasets which have been added to LDaCA.\n\n### [Case Studies](/case-studies)\nAccounts by collectors of various kinds of language data illustrating the \ndifferent solutions they have adopted for the problems encountered in the \nprocess.\n\n<hr class=\"dots\" />\n"},{"title":"Otis Carmichael","slug":"designer","content":"\n### Otis Carmichael\n\nThe design of the LDaCA logo came from my own experience researching the Waanyi language, the language of [my mob](https://en.wikipedia.org/wiki/Waanyi). The design flows from the beginning to the end, where it flows back into itself at the beginning, representing the cyclical nature of life and the Land. The strands of language interweave with themselves, influenced by everything that touches them until they become something entirely their own. The complexity of these languages is a thing of beauty and their preservation, study and proliferation lays claim to this unceded land. The inspiration of the design is as follows:\n\n**L** - Language is shown as a flow of words that can be merged, split and rearranged to create different meanings, just as these lines can be.\n\n**D** - Data is depicted by these specks of raw information, undecipherable before processing but heavy with power.\n\n**a** - This data is not just numbers and words but signals from the Universe, such as animal tracks.\n\n**C** - Whilst research often appears niche in these commons, there is no limit to the range of it’s influence, the ripples of which disrupt everything it touches.\n\n**A** - The language data of this common has been collected from many individual nations that make up ‘Australia’, often with a foundation on extraction based research techniques. This project aims to make up for the mistakes of the past by celebrating and giving back to these nations, which are so incredibly distinct whilst sharing beautiful connections with each other.\n\nThe colours utilised in this design draw from those that are seen in my peoples land of Boodjamulla, in the Gulf of Carpentaria. They reflect the connections between the Sky, the People and the Land in the hope that these connections continue to strengthen indefinitely.\n\n**My Bio**\nMy name is Otis Carmichael and I am a proud Waanyi man living in Meanjin. My designs reflect my appreciation of the past and my hope for the future.\n"},{"title":"Fieldwork PNG","slug":"fieldwork-png","content":"\n# Fieldwork in Papua New Guinea\n\nby Harriet Sheppard\n![Harriet on an outrigger boat off Vanatɨna](/HS-sailing-outrigger-small.jpg)\nHarriet on an outrigger boat off Vanatɨna\n\nMy fieldwork experience, in many ways, conforms to stereotypes of language \nfieldwork – a community outsider working with a smaller language community \nlocated far from an urban centre. For my postgraduate research, I documented \nand described grammatical topics of \n[Sudest](https://glottolog.org/resource/languoid/id/sude1239) (known as Vanga \nVanatɨna by its speakers), an Oceanic language spoken in Papua New Guinea (PNG\n). The language is spoken on the islands of Vanatinai (also known as Sudest \nand Tagula) and Yeina in the Louisiade Archipelago, approximately 350 \nkilometres southeast of the PNG mainland. In the following, I discuss \ndecisions I made and processes I followed related to the collection of \nlanguage data and archiving the resulting corpus of texts, including ethical \nconsiderations, ownership of the data, data formats, metadata, and data \nsecurity and reuse.  This is by no means meant as a guide (of best practice) \nfor projects where language data are being collected with speakers of un(der)\ndescribed languages. Rather, it is an account of some of the types of \nconsiderations and procedures that need to be considered when collecting \nlanguage data and how I handled them in this instance.\n\n\n![Map of Louisiade Archipelago](/map-louisiade.jpg)\n\nBefore recording with a speaker for the first time, I would explain the \nproject to the speaker and their rights as contributors to the project. I \nwould explain how I would use the language recordings, how they would be \nstored and accessed and how speakers of the language or other researchers \nmight access and use the recordings in the future. This included discussing \nhow they, the person(s) making the recording, would retain ownership over the \nrecording (i.e., intellectual and cultural property rights). They would also \nbe the ones to choose access rights over the text and have the option to \nwithdraw any or all recordings they made from the corpus at any point (more \non these below). After these discussions, if the speaker was still interested \nin the project, I would then record an oral consent form with them. The \nconsent form was in English, the language of wider communication in the \nprovince. In cases where the speaker didn’t speak or spoke limited English, \nwe would have a translator present for this process. I chose to use an oral \nconsent form as it is accessible to all participants no matter their literacy \nlevels.  It also has the advantage of being a more durable format compared \nwith a paper consent form, particularly when working in a hot and humid \nclimate and it can also be easily backed up.\n\n![Transcription session with Abel Sam](/transcription-session-small.jpg)\nEach time I made a recording with a speaker or speakers, I played the \nrecording back to them to check that they were happy with the recording and \nso that they could decide on what sort of access conditions they wanted the \narchived recording to have. If the speaker was not satisfied with the \nrecording, sometimes they would choose to make another recording and delete \nthe original or simply delete the recording and maybe try again later. If \nthey were happy with the recording, they would decide on the access \nconditions they wanted for the archived recording. The possibilities ranged \nfrom the most restricted, being that only I, as the original researcher, \ncould access the recordings and use them for my research, to the opposite end \nof the access continuum where anyone interested could download and listen to \nthe recordings, read any transcriptions, and potentially use them for \neducational or research purposes. In between these two ends of the continuum, \nthey could opt for other access conditions such as only allowing access for \nregistered users of the archive, or only for researchers and speakers, or \njust for speakers. They could also choose to have a text embargoed for a \ncertain period of time.\n\n\n![Transcription session with Abel Sam](/elicitation-session-notes-small.jpg)\nWhen I was preparing my ethics application for the university, I applied and \nreceived ethics approval to work with minors. I didn’t plan to collect data \nfrom children (although I did want the option of recording older teens if any \nshowed an interest in recording), but included minors in my ethics \napplication because of the potential for children to be inadvertently filmed, \nfor example if they walk into frame or come up to a parent while the parent \nis being recorded. The ethics approval meant that if such instances arose \nduring a recording, the recording could still be kept and archived (\nassuming approval from the speaker making the recording). While such \nsituations didn’t end up arising during my own data collection, this would \nnot have been an ideal fix for this issue. More widely, as a discipline, we \ndon’t have good model(s) regarding how we deal with such scenarios in regards \nto access and consent. One suggestion I heard was to only make such \nrecordings openly available after obtaining informed consent from any \nchildren involved after they turned 18 but, for most circumstances, such an \napproach is unlikely to be realistic.\n\nAs the researcher who collected the data and deposited it in the archive, I \nam the person the archive contacts when someone puts in a request to access \nrestricted data. But granting any access to data, whether restricted or not, \nis based on the wishes of the owner(s) of a specific text, that is, the \nspeaker(s) who recorded it. As mentioned, the speaker(s) retains ownership of \nthe text and intellectual and cultural property rights over the recording. \nThey also retain the right to change access restrictions on a recording or \nremove it completely from the archived corpus at any point and request that \nit not be used in any future research from that point forwards (given it \nwould be difficult or impossible to remove excerpts from recordings in \npreviously submitted or published work on the language, this is not something \nthat is generally done). Away from the field, this can get a bit complicated \ndue to limited telephone and internet coverage in the islands although it is \nincreasingly more feasible as more and more people have access to mobile \ntelephones and apps like WhatsApp and Facebook. Any future users of the \ncorpus who download data from the archive must agree to only use the data for \neducational or research purposes and not for (direct) financial gain. Having \nsaid that, it needs to be acknowledged that researchers including myself can \nand do benefit indirectly from such data collections in the form of \nscholarships, grants, qualifications, and other potential employment \nopportunities related to our work with the language.  Although I, like many (\nhopefully most) fellow linguists, subscribe to the idea that the source of \nthe data, that is the speaker(s), should have control over their own \nrecordings, the current systems we have set up make that hard or impossible \nto implement. This is something that is increasingly being acknowledged and \nwe as a community of researchers don’t currently have answers for. Each \nspeaker community is different and so one-size-fits-all models for \ncommunities are unlikely to work. \n\nWhen collecting spoken or signed language data, the best practice is to make \naudio and video recordings in formats that are ‘lossless’ rather than ‘lossy\n’. Lossless compression allows for perfect reconstruction of the original data\n. Lossy formats reduce file size but also discard some information that \ncannot be reconstructed which is obviously not an ideal outcome for precious \nlanguage data! For audio recordings, I use a recorder that records Waveform \nAudio File Format or WAV files (.wav) and for video recordings I record \nAdvanced Video Coding High Definition or AVCHD files (.mts) which are both \nlossless formats and accepted by many archives. WAV files are also the audio \nformat needed for many linguistics software programs. AVCHD files are, however\n, not a file type accepted by the archive where the Sudest corpus is housed \nwhich only accepts MPEG video files (.mpg) which are lossy. This situation is \nlikely due to the fact that the archive was set up some two decades ago when \nstorage capacity was a big issue and the archive has not yet caught up in \nthis regard. This is not an ideal situation and something that should be \nconsidered if you are able to choose the archive where your data will be \ndeposited. For derived data including text-audio aligned transcriptions and \ntranslations as well as any other annotations, I use plain text (.txt) and \nELAN Annotation Format files (.eaf). The plain text files are the file type \ncreated when working in the Field Linguist’s Toolbox software program and can \nbe opened, read, and edited in most text editors.  It is also a good format \nfor storing information if you want it to persist. The EAF files are more \nspecialist although they are a transferable format. They are only created \nand used with ELAN which is an annotation tool for audio and video widely \nused by linguists. Both text and EAF files are some of the standard file \nformats for transcriptions and annotations that are generally accepted by \nlanguage archives today. \n\n![Setting up for a recording session](/recording-setup-small.jpg)\nWhen collecting metadata, I adopted the Component MetaData Infrastructure (\nCMDI) schema used by the archive where I was planning to deposit my data. The \nlogistics of fieldwork can be overwhelming at times and following the archive’\ns metadata schema meant I would minimally have the metadata required to \ndeposit the corpus. The schema was also practical in that there were some \nfields that were obligatory (e.g., text title, creation data, country of \nrecording) but many fields are optional (e.g., speaker date of birth/age, \neducation level, address where recording took place). Having optional fields \nthat could be filled in or not worked well for me because collecting more \ndetailed information, particularly biographical information about speaker(s), \nwasn’t always possible or appropriate. Not all speakers, particularly older \nspeakers, necessarily knew their date or year of birth and they may only know \ntheir approximate age. Some speakers I only met when they came to make a \nrecording and getting the detailed biographic information one might like for \nmetadata records is not necessarily culturally appropriate, particularly \ngiven the power imbalance between myself, an educated, white researcher from \nAustralia, and speakers I don’t have an established relationship with. Aside \nfrom asking about where the speaker grew up, currently lived, and their \napproximate age, if I didn’t already have an established relationship with \nthe individual, I tended to let information emerge incidentally as \nvolunteered rather than questioning them in depth. One topic I did discuss \nwith speakers regarding metadata was the question of authorship and \nattribution of the recording and whether they wanted their name to be listed \nor would like to be given a pseudonym. With small communities, especially \nwhen you are recording audio and video of the speaker, complete anonymisation \nis not really possible if the speaker also wants the recording to be \naccessible to other speakers and this could be a potential issue. However, in \nmy experiences, (all) speakers chose to have their names listed as the author(\ns) of the text without anonymisation and were generally excited or proud to \nbe making a record of their language and knowledge.    \n\nWhile the amount of personal metadata collected for individual speakers varied\n, there was also situational data that I wish I had collected and didn’t. I \ncollected information including the date and location of the recording (e.g., \nthe village and house where the recording took place), who was present at the \nrecording, the genre of recording (e.g., a historical narrative, conversation\n, instructions on how to complete a specific task, etc.). One piece of \ninformation that I didn’t include, for example, is the exact location of the \nspeaker and their orientation in space. Were they facing towards the north? \nNorth-west? Since my original fieldwork, I have begun to research the use of \nco-speech gestures in the language, that is, gestures that occur at the same \ntime as someone is speaking. Frequently, speakers point to real world \nlocations while they are talking but this can be hard to identify if you don’\nt know exactly how the speaker was positioned in relation to where they are \ngesturing. Luckily, by watching the videos I can still identify the direction \na speaker was facing and add this to the recording’s metadata but this would \nbe a very difficult if not impossible information for a future user of the \ndata to ascertain if they wanted to study gesture. \n\nData security is something I had to consider both in the field and at home. \nIn order to keep data secure on computers and storage devices, I made sure \nthat they were all password protected. In the field, I also had to consider \nspecific factors in the environment that could endanger the data - the major \nissue is the humidity. The average humidity level on the island hovers around \n80 percent year-round and I had paper notebooks, a computer, hard drives and \nSD cards that I need to keep in working order in a house that is relatively \nopen to the outside environment with no glass windows and no electricity (I \ndo remember a field manual suggesting a fridge makes a good home for \nequipment in humid environments if that is an option). To protect equipment,\nI stored everything in waterproof pouches with silica sachets. I would \ndistribute backup USBs and SD cards across different pouches so if one failed\n, I would hopefully have backups. I also tried to double bag equipment and \nnotes when travelling, particularly by boat. Boat transport in the islands is \neither by traditional wooden sailing outrigger canoes or fibreglass dinghy ‘\nbanana’ boat, both of which are open to the elements. \n\nTo secure data, ideally, I would also make backup copies of all new \nrecordings and transcription files each day in the field. However, during my \nfirst fieldtrip, I didn’t have access to a power source and therefore couldn’\nt charge my laptop meaning I couldn’t backup recordings. As this also meant I \nwas transcribing all texts using pencil and paper, I did photograph all new \ntranscriptions each night to preserve a backup copy. Towards the end of my \ntrip, I became quite worried about having no backups for recordings, \nparticularly when thinking about the boat trip back to the main island which \ncan take up to eight hours (or more in bad conditions). To avoid making this \ntrip with only the original copies, I tagged along on a walk to the local \nprimary school two and a half hours away because it was rumoured that one of \nthe teachers had a solar panel connected to a car battery and I would be able \nto charge my laptop and therefore backup the recordings. All the community’s \nprimary school students make this walk twice a week, living at school during \nthe week. At one point in the journey, you have to cross a river in a small \noutrigger canoe. It was quite alarming when the teenager paddling me across \nnonchalantly pointed out a nearby crocodile while we were mid-river but at \nleast I managed to back up all the files! For subsequent fieldtrips, I \ninvested in a battery and solar panel for charging equipment and backing up \ndata although I still had to cross rivers with crocodiles from time to time. \n\n\n![Crossing the Veora river](/crossing-river-small.jpg) \nIn order to secure the corpus of texts into the future and make sure that it \ndoesn’t get lost on an old hard drive, I have uploaded it to a language \narchive. The archive is a digital repository tasked with safeguarding \nlanguage corpora of smaller languages for which there is limited documented \ndata. As well as keeping copies of the uploaded data, the archive will also \nensure that the data continues to be accessible, for example, by converting \ndata files to newer formats if the format a file was uploaded in becomes \nobsolete. Having the corpus saved in an online archive also means that it is \ndiscoverable for other researchers, speakers, and other interested parties. \nIt can be found through the search portal of the archive by searching by \nlanguage name(s), the contributor’s name, or by browsing by country. The \ncorpus can also be found by searching through the Open Language Archive \nCommunity (OLAC) portal which is an online virtual library of information and \nlinks to language resources available in a specific language (e.g., grammars, \ndictionaries, archived texts). The information on OLAC is automatically \ncollected or ‘harvested’ daily from participating archives meaning that when \nyou upload a file to an archive, it will automatically be listed in OLAC the \nnext time it is updated. Because there is limited information online about \nSudest, keyword searches on most online search engines such as Google also \ndisplays links to the corpus quite high up in the results.  \n\nIt's often suggested that the researcher should make a will and assign a \nliterary executor for the data they collect. This raises the question about \nwho to nominate? As a graduate student, I nominated one of my supervisors as \nthe person to have direct access and editing powers over the archived corpus. \nThis makes sense in that, after myself, my supervisor is one of the people \nthat knows the most about the project and data collection. Supervisors are \nalso often the chief investigator for ethics approval for graduate data \ncollection as well. But supervisors tend to be older than graduate students \nand don’t necessarily have any personal connection to the speaker community. \nIdeally, any succession plan would bring control of the data back to the \ncommunity but what would that mean in practice? There may be issues both \npolitical and practical if just one individual from the community becomes the \nexecutor. In some cases, there may be an obvious option if the community has \na language and culture centre or registered Indigenous corporation. In such \ncases, access and stewardship plans may have been discussed and incorporated \ninto the project since the beginning. For many communities, especially in the \nPacific, this is not the case and therefore there is no clear governing group \nwhich might take such a task on. \n\nIn future research projects I might work in, I can build on my past \nexperiences and hopefully improve on them. This would likely involve \nexpanding the list of situational metadata I automatically collect for each \nrecording as well as considering the file types accepted by an archive when \ndeciding where to deposit any recordings. Although I’ve only touched briefly \non issues relating to speaker-community control and intellectual and cultural \nproperty rights, I would also aim to build in more funding and space into the \ntimeline of a project for more equitable community consultation and \ncollaboration. Such discussion would then likely have a flow-on effect for \nhow to secure the data into the future. Ethical and practical questions \nregarding data collection, access, and stewardship are complex. It is \nimpossible to take all contingencies into account. Best practice and \nstandards change with much of the change led by Indigenous and First Nations \nresearchers. Best practices from a decade ago are not those of today and this \nis a good thing!\n\n\n![Vuwo village](/Vuwo.JPG)\n\nBack to [Language Data](../data/)\n"},{"slug":"home","content":"## The Language Data Commons of Australia (LDaCA) will make nationally significant language data available for academic and non-academic use and provide a model for ensuring continued access with appropriate community control.\nAustralia is a massively multilingual country in one of the world’s most \nlinguistically diverse regions. Significant collections of this intangible \ncultural heritage have been amassed, including collections of Australian \nIndigenous languages, regional languages of the Pacific, and of Australian \nEnglish, as well as collections important for cyber-security and for \nemergency communication. LDaCA will integrate this existing work into a \nnational research infrastructure while also securing collections which remain \nunder-utilised or at risk. LDaCA will thus ensure long-lasting access for \nanalysis and reuse of these invaluable data, and will manage the data in a \nculturally, ethically and legally appropriate manner guided by FAIR and CARE \nprinciples.\n\nTo accomplish these goals, LDaCA will:\n- Develop a comprehensive language data access policy framework,\n- Develop shared technical infrastructure and standards across institutions,\n- Build a sustainable long-term repository for ingesting and curating existing\n  language data collections of national significance,\n- and build a portal for discovery and access of language data.\n\nThe result will be an integrated national technical infrastructure to analyse \nlanguage collections at scale which will open up the social and economic \npossibilities of Australia's rich linguistic heritage. The project will build \nconnections to other HASS RDC projects and assist in laying the foundation \nfor the establishment of a broader HASS Research Data Commons as well as \npositioning Australia internationally as a leading contributor of language \ncollections and digital infrastructure.\n\n![Acknowledgement](/AcknowledgeARDC.png)\n\nThe Language Data Commons of Australia (LDaCA) project received investment \n([1](https://doi.org/10.47486/DP768) and [2](https://doi.org/10.47486/HIR001))\nfrom the Australian Research Data Commons (ARDC). The ARDC is funded by the \nNational Collaborative Research Infrastructure Strategy (NCRIS).\n\n*LDaCA acknowledges and pays respects to the Elders and Traditional Owners of \nthe lands on which we live and work.*\n\n\n"},{"title":"Metadata","slug":"metadata","content":"\nMetadata is often defined as 'data about data'. High quality metadata is important in making data FAIR:\n- Findable: metadata is the starting point for searching data collections. For example, if we want to find data in a particular language, this will only be possible for data which has a language recorded in its metadata. (Tracking languages is in itself problematic, see [below](#identifying-codes-for-languages).)\n- Accessible: access conditions which apply to data should be part of the associated metadata.\n- Interoperable: information about the format of data and whether it requires specific software to be usable should be part of the associated metadata.\n- Reusable: all of aspects of metadata mentioned above contribute to making data reusable. The more we know about some data, the easier it is to know whether it will be useful to us or not.\n\nRO-Crates in general have basic metadata requirements, but it is possible to specify a **profile** for crates for specific purposes. LDaCA is developing such a profile for our data; we are basing this largely on previous work in the area. An important aspect of the RO-Crate approach is that it uses the principles of [Linked Open Data](https://en.wikipedia.org/wiki/Linked_data#Linked_open_data). This means that terms used in our metadata will (whenever possible) link to an openly available definition. In developing the profile, we are drawing on two existing attempts to provide vocabularies for describing language data.\n\n## [OLAC](http://www.language-archives.org/)\n\nThe Open Language Archives Community is an international partnership of institutions and individuals; one of their activities is developing consensus on best current practice for the digital archiving of language resources and this includes making recommendations for metadata. The OLAC metadata scheme is based on [Dublin Core](https://www.dublincore.org/) (DC), a widely used general metadata schema. OLAC have suggested refinements and extensions of the DC base which make it more useful for describing language resources.\n\n\n## [CMDI](https://www.clarin.eu/content/component-metadata)\n\nThe Component Metadata Infrastructure was developed within the [CLARIN](https://www.clarin.eu) project. It draws on the earlier ISLE Metadata Initiative ([IMDI](https://en.wikipedia.org/wiki/IMDI)), but where IMDI attempted to specify a comprehensive scheme for (multimodal) language data, CMDI adopts a more flexible approach where components are assembled into reusable profiles. This is very similar to the RO-Crate approach described above but with an important difference: the components of a CMDI profile are all drawn from a central registry, whereas components of an RO-Crate profile come from any linkable location.\n\n\n## Identifying Codes for Languages\n\nOne very important piece of metadata for language data is a description of the language or languages which the data represent. This is not a simple problem because the relationship between languages and names for them is not one to one. Some languages have more than one name: for example *Farsi* and *Persian* can both be used to refer to the same language. Some names refer to more than one language: for example there are languages called *Buru* used in Nigeria and in Indonesia. To avoid the confusion which can arise from such situations, various systems have been developed to assign unique identifiers to languages. None of these systems gives a comprehensive list of languages and all such systems struggle with another problem, the distinction between separate languages and dialects of one language, as can be seen in the case study below. LDaCA includes identifiers from each of the three systems below where they are available and relevant.\n\n##### [ISO-639](https://iso639-3.sil.org/)\nThis system is recognised as a standard by the International Standards Organisation. An earlier version of this system used two-letter codes to identify languages; more recent versions use three-letter codes (referred to as ISO 639-3). These codes are used by *[Ethnologue](https://www.ethnologue.com/)*, which is a catalogue of the languages of the world, and in many other contexts. The ISO 639-3 code for French is **fra**, and that for Warlpiri is **wbp**\n\n##### [Glottolog](https://glottolog.org/)\nGlottolog is an alternative catalogue of the world's languages, language families and dialects - Glottolog uses the term *languoid* to cover all of these. Each languoid is assigned a unique identifier consisting of four alphanumeric characters and four digits. For example, (standard) French has the code **stan1290**, amd Warlpiri is **warl1254**.\n\n##### [Austlang](https://collection.aiatsis.gov.au/austlang/about)\n\nAustLang provides a controlled vocabulary of persistent identifiers, a thesaurus of languages and peoples, and information about Aboriginal and Torres Strait Islander languages which has been assembled from referenced sources. Alphanumeric codes are used as persistent identifiers, while associated text strings are changeable and can reflect community preferences (including alternative names and spellings). In Austlang, Warlpiri has two codes: **C15** for the language in general, and **C15.1** for the variety named as Wakirti Warlpiri. (French is not covered by Austlang.)\n\n#### Case study - Kala Lagaw Ya\n\nKala Lagaw Ya is a language spoken in the Torres Strait Islands. The language has several dialects or varieties and the table below shows how the different code schemes deal with this.\n\n<table>\n<tr><td><b>Name</b></td><td><b>ISO 639</b></td><td><b>Glottolog</b></td><td><b>Austlang</b></td><td><b>Notes</b></td></tr>\n<tr><td>Kala Lagaw Ya</td><td>mwp</td><td>kala1377</td><td>Y1</td><td>Austlang: Marked with symbol ^ which indicates that the name is used to refer to a language and a dialect of the language.</td></tr>\n<tr><td>Kalaw Kawaw Ya</td><td></td><td>kala1378</td><td>Y2</td><td>Ethnologue: Kalaw Kawaw is a dialect</td></tr>\n<tr><td>Kawrareg</td><td></td><td>kawr1234</td><td></td><td></td></tr>\n<tr><td>Kulkalgau Ya</td><td></td><td>kulk1234</td><td>Y4</td><td></td></tr>\n<tr><td>Mabuyag</td><td></td><td>mabu1234</td><td></td><td>Ethnologue: Mabuiag is an alternate name</td></tr>\n<tr><td>Kawalgaw Ya</td><td></td><td></td><td>Y5</td><td>Austlang: Kaurareg is an alternative name (probably the same as Glottolog kawr1234)</td></tr>\n</table>\n\n<br />\n\nBack to [Background](../background/)\n"},{"title":"Organisation","slug":"organisation","content":"\n# Organisation\n\nLDaCA is one strand of the partnership between the Australian Research Data Commons ([ARDC](https://ardc.edu.au/)) and the [School of Languages and Cultures](https://languages-cultures.uq.edu.au/) at the [University of Queensland](https://www.uq.edu.au/). This partnership includes a number of projects that explore language-related technologies, data collection infrastructure and Indigenous capability programs. These projects are being led out of the Language Technology and Data Analytics Lab ([LADAL](https://slcladal.github.io/index.html)), which is overseen by [Professor Michael Haugh](https://languages-cultures.uq.edu.au/profile/1498/michael-haugh) and [Dr Martin Schweinberger](https://languages-cultures.uq.edu.au/profile/4295/martin-schweinberger).\n\nThe LDaCA project received investment from ARDC through two of its programs:\n\n1. Australian Data Partnerships Program: Developing policy and technology foundations of a nationally integrated research infrastructure for language data collections of high strategic importance for the Australian research community ([DP](https://ardc.edu.au/project/language-data-commons-of-australia-ldaca/)).\n2. HASS Research Data Commons and Indigenous Research Capability Program: Capitalising on existing infrastructure, securing vulnerable and dispersed collections and linking with improved analysis environments for new research outcomes ([HASS-RDC](https://ardc.edu.au/news/announcing-3-successful-projects-ardc-hass-rdc/)).\n\n<br />\n<hr class=\"dots\" />\n<br />\n\n## Partner Institutions:\n\n- **University of Queensland** (DP, HASS-RDC): Professor Michael Haugh, Professor Clint Bracknell\n- **Australian National University** (DP, HASS-RDC): Professor Catherine Travis\n- **Monash University** (DP, HASS-RDC): Associate Professor Louisa Willoughby\n- **University of Melbourne** (DP, HASS-RDC): Associate Professor Nick Thieberger\n- **University of Sydney** (HASS-RDC): Professor Monika Bednarek (Sydney Corpus Lab)\n- **AARNet** (HASS-RDC)\n- **First Languages Australia** (HASS-RDC): Beau Williams\n\n### Advisory / Consultative Partners\n\n- PARADISEC\n- ARC Centre of Excellence for the Dynamics of Language\n- Australian Digital Observatory\n- CLARIN\n"},{"title":"Pre-ALS Conference Activity Day","slug":"pre-als-activities","content":"\n## Language Data Commons of Australia\n\n## Australian Text Analytics Platform\n\n### Day of Activities before ALS2022 (Tuesday 29/11)\n\n<table>\n<tr><td><b>Time</b></td><td><b>Activity</b></td><td><b>People</b></td></tr>\n<tr><td>09:00 - 10:00</td><td>Introduction to notebooks<br><i>What are Jupyter notebooks and why are they useful?</i>\n</td><td>Sara King<br>Simon Musgrave</td></tr>\n<tr bgcolor = 'LightGray'><td>10:00 - 10:30</td><td>Break</td><td></td></tr>\n<tr><td>10:30 - 11:00</td><td>General progress report<br><i>An overview of the projects and of progress to date</i>\n</td><td>Michael Haugh<br>Robert McLellan<br>Simon Musgrave<br>Jenny Fewster (ARDC)</td></tr>\n<tr><td>11:00 - 12:00</td><td>Introducing the LDaCA Portal<br><i>How will I find data in LDaCA? Your chance to tell us whether we are meeting your needs!</i></td><td>Peter Sefton<br>Kathrin Kaiser</td></tr>\n<tr bgcolor = 'LightGray'><td>12:00 - 01:00</td><td>Lunch</td><td></td></tr>\n<tr><td>01:00 - 02:00</td><td>Data and access<br><i>How is access to data managed in LDaCA and how can I access data?</i>\n</td><td>Peter Sefton<br>Kathrin Kaiser<br>Catherine Travis</td></tr>\n<tr><td>02:00 - 02:30</td><td>Auslan project update<br><i>Report on work by the team at Monash University</i>\n</td><td>Louisa Willoughby<br>Trevor Johnston</td></tr>\n<tr bgcolor = 'LightGray'><td>02:30 - 03:00</td><td>Break</td><td></td></tr>\n<tr><td>03:00 - 03:30</td><td>Text analytics for corpus linguistics<br><i>Selected ATAP tools showcase</i>\n</td><td>Sydney Informatics Hub with Sydney Corpus Lab (Monika Bednarek)</td></tr>\n<tr><td>03:30 - 05:00</td><td>Discursis workshop<br><i>How to get started with this tool for tracking topics in conversations </td><td>Ben Foley<br>Sam Hames</td></tr></i>\n</table>\n\nThe [Australian Text Analytics Platform](https://www.atap.edu.au/) and the [Language Data Commons of Australia](https://www.ldaca.edu.au/) are projects supported by the [Australian Research Data Commons](https://www.ardc.edu.au) to develop infrastructure for Australian researchers who work with language data. This day of activities organised by the projects will give ALS conference delegates (and anyone else who is interested) the opportunity to learn more about this work. The day will include:\n\n- an overview of the projects and the work to date\n- reports on specific sub-projects\n- introductory workshops on the tools and resources being developed\n- a workshop on using Discursis, a tool for tracking topics in interactive use of language\n- the opportunity to influence future work by exploring and providing feedback on the data interface which we are building.\n\nThe activities will be of interest to anyone who conducts research which includes language data, especially those who use or would like to use computational tools in their research. Participants should bring their own computer; no software is required beyond a web browser.\n\nCatering is provided and therefore a registration fee is being charged for this event to partially cover the cost ($25 full rate and $15 discount).\n\n[Registration](https://als.asn.au/Conference/Conference2022/Workshops)\n\nThe Australian Text Analytics Platform and the Language Data Commons of Australia received investment from the Australian Research Data Commons (ARDC). The ARDC is funded by the National Collaborative Research Infrastructure Strategy (NCRIS).\n"},{"title":"Principles","slug":"principles","content":"The Language Data Commons of Australia aims to ensure the long-term access to language data collections for analysis and reuse. Sustainable management of and access to these significant collections of intangible cultural heritage are underpinned by two sets of complementary guiding principles of data management and stewardship, namely the FAIR and CARE principles. \n<br />\n## FAIR Principles\n<br />\n\nThe FAIR principles were [first published](https://www.nature.com/articles/sdata201618) by a group of stakeholders representing academia, industry, funding agencies, and scholarly publishers. The principles aim to address issues surrounding data management and stewardship, focusing on four areas (which provide the FAIR acronym).\n<br />\n### Findable\nMetadata and data should be easy to find for both humans and computers. Making the data findable includes:\n- (Meta)data are assigned a globally unique and persistent identifier\n- Data are described with rich metadata \n- Metadata clearly and explicitly include the identifier of the data they\ndescribe.\n- (Meta)data are registered or indexed in a searchable resource.\n<br />\n\n### Accessible\nOnce the user finds the required data, she/he/they need to know how they can be accessed, possibly including authentication and authorisation.\n- (Meta)data are retrievable by their identifier using a standardised communications protocol\n    + The protocol is open, free, and universally implementable\n    + The protocol allows for an authentication and authorisation procedure, where necessary\n- Metadata are accessible, even when the data are no longer available\n<br />\n\n### Interoperable\nThe data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.\n- (Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation\n- (Meta)data use vocabularies that follow FAIR principles\n- (Meta)data include qualified references to other (meta)data\n<br />\n\n### Reusable\nThe ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings.\n- (Meta)data are richly described with a plurality of accurate and relevant attributes\n    + (Meta)data are released with a clear and accessible data usage license\n    + (Meta)data are associated with detailed provenance\n    + (Meta)data meet domain-relevant community standards\n<br />\n<br />\n\nThe Australian Research Data Commons ([ARDC](https://ardc.edu.au/)) supports FAIR data practices and initiatives that make data and related research outputs FAIR. At the same time, the ARDC acknowledges that the implementation of the principles will look different across disciplines and will need discipline-specific approaches and standards. \n\n## CARE Principles\nThe [CARE Principles for Indigenous Data Governance](https://www.gida-global.org/care) were developed by the Global Indigenous Alliance (GIDA); they are a response to the FAIR principles and aim to complement them. GIDA highlights how the FAIR principles and the open data movement focus on increasing data sharing among researchers and entities but do not take into account power differentials and historical contexts or fully engage with Indigenous Peoples’ rights and interests. These include the rights to generate value from Indigenous data in ways that are grounded in Indigenous worldviews and to advance Indigenous innovation and self-determination. The CARE Principles also focus on four areas.\n<br />\n\n### Collective Benefit\nData ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data.\n- For inclusive development and innovation\n- For improved government and citizen engagement\n- For equitable outcomes \n<br />\n\n### Authority to control\nIndigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered.\n- Recognizing rights and interests\n- Data for governance\n- Governance of data.\n<br />\n\n### Responsibility\nThose working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit.\n- For positive relationships\n- For expanding capability and capacity\n- For Indigenous languages and worldviews.\n<br />\n\n### Ethics\nIndigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.\n- For minimizing harm and maximizing benefit\n- For justice\n- For future use.\n\n\nBack to [Background](../background/)\n"},{"title":"Resources","slug":"resources","content":"# Resources\n\n- [Indigenous Knowledge](#indigenous-knowledge)\n- [Ethics](#ethics)\n- [Languages of the World](#languages-of-the-world)\n- [Language Archives](#language-archives)\n- [Australian Organisations](#australian-organisations)\n- [International Organisations](#international-organisations)\n- [Publications](#publications)\n- [Tools](#tools)\n\n<hr class=\"dots\" />\n\n### Indigenous knowledge {#indigenous-knowledge}\n\n- [Protocols](https://australiacouncil.gov.au/wp-content/uploads/2021/07/protocols-for-using-first-nati-5f72716d09f01.pdf)\n  for using First Nations Cultural and Intellectual Property in the Arts (Australia Council)\n- Citing Indigenous elders and knowledge keepers\n    - [Templates For Citing Indigenous Elders and Knowledge Keepers](https://kula.uvic.ca/index.php/kula/article/view/135)\n    (Lorisia Macleod)\n    - University of Alberta Library's [Guide](https://guides.library.ualberta.ca/c.php?g=715568&p=5112574)\n    - University of Alberta Library's [blog](https://news.library.ualberta.ca/blog/2022/01/27/citing-indigenous-elders-and-knowledge-keepers/)\n- [TK Labels](https://localcontexts.org/labels/traditional-knowledge-labels/): Traditional Knowledge labels are an initiative for Indigenous communities and local organizations. Developed through sustained partnership and testing within Indigenous communities across multiple countries, the Labels allow communities to express local and specific conditions for sharing and engaging in future research and relationships in ways that are consistent with already existing community rules, governance and protocols for using, sharing and circulating knowledge and data.\n- Decolonising linguistics: [Spinning a better yarn](https://www.dynamicsoflanguage.edu.au/ialr/decolonising-linguistics-spinning-a-better-yarn/) \n- Rawlings, V, Flexner, J L, Riley, L (eds.) (2021) *[Community-Led Research: Walking New Pathways Together](https://open.sydneyuniversitypress.com.au/files/9781743327630.pdf)*. Sydney University Press\n\n#### Indigenous Languages of Australia\n\n- [First Languages Australia](https://www.firstlanguages.org.au/): First \n  Languages Australia encourages communication between communities, the \n  government and key partners whose work can impact Aboriginal and Torres \n  Strait Islander languages.\n- [Living Languages](https://www.livinglanguages.org.au/): Living Languages \n  provides grassroots training to people, communities and Language Centres in \n  Australia doing language work on the ground in remote, regional and urban \n  areas.\n- Language Centres: The Indigenous Languages and Arts (ILA) program  \n  currently supports a number of organisations, including a network of 20 \n  language centres. A list of these centres is available from \n  [this page](https://www.arts.gov.au/documents/indigenous-languages-and-arts-program-language-centres) \n  (pdf or docx format). \n- [Austlang](https://collection.aiatsis.gov.au/austlang/about): AustLang \n  provides a controlled vocabulary of persistent identifiers, a thesaurus of \n  languages and peoples, and information about Aboriginal and Torres Strait \n  Islander languages which has been assembled from referenced sources.\n- [Nyingarn](https://nyingarn.net/): a platform for primary sources in \n  Australian Indigenous languages \n- [Digital Daisy Bates](https://bates.org.au/): a collection of over 23,000 \n  pages of wordlists of Australian languages, originally recorded by Daisy \n  Bates in the early 1900s, made up of the original questionnaires and around \n  4,000 pages of typescripts.\n- [50 Words Project](https://50words.online/): The 50 Words Project showcases \n  words from 64 languages across Australia, with further languages being \n  added regularly as more communities around Australia become involved. For \n  each language you can hear the words spoken via a map that shows the \n  general location of the language.\n- [Gambay](https://www.abc.net.au/indigenous/gambay-languages-map): a map of \n  Australia's first languages.\n- [Australian Indigenous language collections](https://www.nla.gov.au/research-guides/indigenous-language-resources#): \n  a guide to materials held in the National Library of Australia, with links \n  to similar resources at State Libraries. \n- Living Archive of Australian Languages: see [below](#language-archives)\n- Contemporary and Historical Reconstruction in the Indigenous Languages of \n  Australia ([CHIRILA](http://www.pamanyungan.net/chirila/)): CHIRILA is a \n  lexical database (a database with words from different languages). \n  Currently there are about 780,000 words, from all over Australia, of which \n  about 20% is publicly available.\n\n### Ethics {#ethics}\n\n- Australian Institute of Aboriginal and Torres Strait Islander Studies [Code of ethics](https://aiatsis.gov.au/research/ethical-research/code-ethics)\n- Australian Linguistic Society [policies](https://als.asn.au/AboutALS/Policies): includes Policy on Linguistic rights of Aboriginal and Islander communities and Statement of Ethics.\n- [Australian Code for the Responsible Conduct of Research, 2018](https://www.nhmrc.gov.au/about-us/publications/australian-code-responsible-conduct-research-2018)\n- Linguistic Society of America: \n    - [Ethics Statement 2019](https://www.linguisticsociety.org/content/lsa-revised-ethics-statement-approved-july-2019)\n    - [Further resources](https://www.linguisticsociety.org/resource/ethics-further-resources)\n- [Tromsø recommendations for citation of research data in linguistics](https://www.rd-alliance.org/group/linguistics-data-ig/outcomes/troms%C3%B8-recommendations-citation-research-data-linguistics)\n\nLanguage and linguistics datasets are often not cited, or cited imprecisely, \nbecause of confusion surrounding the proper methods for citing them. For the \nuse of researchers and scholars in the field working with datasets, the \nTromsø recommendations propose components of data citation for referencing \nlanguage data, both in the bibliography and in the text of linguistics \npublications. \n\n### Languages of the world {#languages-of-the-world}\n\n- [Glottolog](https://glottolog.org/): Comprehensive reference information for the world's languages, especially the lesser known languages. LDaCA uses Glottolog [language codes](../metadata/#glottologhttpsglottologorg) in our metadata. \n\n- Open Language Archives Community ([OLAC](http://www.language-archives.org/)) is an international partnership of institutions and individuals who are creating a worldwide virtual library of language resources by: (i) developing consensus on best current practice for the digital archiving of language resources, and (ii) developing a network of interoperating repositories and services for housing and accessing such resources. OLAC harvests metadata and their web site has a search facility to find resources for languages. OLAC [metadata recommendations](../metadata/#olachttpwwwlanguage-archivesorg) are the basis for some of LDaCA's metadata.\n\n- [Ethnologue](https://www.ethnologue.com/): *Ethnologue* provides information about the languages of the world, but operates on a subscription model. The information which is available without a subscription is very limited: the first three lines of individual language entries which include the ISO 639-3 code, the classification of the language into a language family, and a link to the language’s OLAC page.\n\n- The World Atlas of Linguistic Structures Online ([WALS](https://wals.info/)) is a large database of structural (phonological, grammatical, lexical) properties of languages gathered from descriptive materials (such as reference grammars) by a team of 55 authors.\n\n### Language Archives {#language-archives}\n\n- [LAAL](https://livingarchive.cdu.edu.au/): \nThe Living Archive of Aboriginal Languages is a digital archive of endangered literature in Australian Indigenous languages of the Northern Territory. It contains nearly 4000 books in 50 languages from 40 communities available to read online or download freely. This is a living archive, with connections to the people and communities where the books were created. This will allow for collaborative research work with the Indigenous authorities and communities.\n- [PARADISEC](https://www.paradisec.org.au/): The Pacific and Regional Archive for Digital Sources in Endangered Cultures holds 14,500 hours of audio recordings and 2,000 hours of video recordings that might otherwise have been lost. These recordings are of performance, narrative, singing, and other oral tradition. This amounts to 150 terabytes, and represents 1,315 languages, mainly from the Pacific region.\n- [ELAR](https://www.elararchive.org/): The Endangered Languages Archive is a digital repository for preserving multimedia collections of endangered languages from all over the world, making them available for future generations.\n- [The Language Archive](https://archive.mpi.nl/tla/): The Language Archive (TLA) is an integral part of the Max Planck Institute for Psycholinguistics in Nijmegen. It contains various types of materials, including: audio and video language corpus data from languages around the world; photographs, notes, experimental data, and other relevant information required to document and describe languages and how people use them; records of speech in everyday interactions in families and communities; naturalistic data from adult conversations from endangered and under-studied languages, and linguistic phenomena.\n- [Kaipuleohone Language Archive](http://ling.hawaii.edu/kaipuleohone-language-archive/): Kaipuleohone is the digital language archive of the University of Hawaiʻi. Founded in 2008, the archive houses texts, images, audio, and video collected from around the world by linguists, anthropologists, ethnomusicologists, and more. Our collection includes a wealth of photographs, notes, dictionaries, transcriptions, and other materials related to small and endangered languages.\n- [DELAMAN](https://www.delaman.org/): The Digital Endangered Languages and Musics Archive is an international network of archives of data on linguistic and cultural diversity, in particular on small languages and cultures under pressure.\n\n### Australian Organisations {#australian-organisations}\n\n- The Australian Linguistic Society ([ALS](https://als.asn.au/Home)) is the national organisation for linguists and linguistics in Australia. Its primary goal is to further interest in and support for linguistics research and teaching in Australia.\n\n- The Applied Linguistics Association of Australia ([ALAA](https://alaa.net.au/)) is the national professional organisation for applied linguistics in Australia. We welcome academics, teachers, researchers, students and members of the wider community to join us and become part of an active community interested in questions, issues and problems that can be understood and addressed through a focus on language in our world.\n\n- The Australasian Langauge Technology Association ([ALTA](https://www.alta.asn.au/index.html)) has the purpose of promoting language technology research and development in Australia and New Zealand.\n\n- The Australasian Speech Science and Technology Association ([ASSTA](https://assta.org/)) is a scientific association which aims to advance the understanding of speech science and its application to speech technology in a way that is appropriate for Australia and New Zealand.\n\n- The Australian Institute of Aboriginal and Torres Strait Islander Studies ([AIATSIS](https://aiatsis.gov.au/)) is an Indigenous-led, national institute that celebrates, educates and inspires people from all walks of life to connect with the knowledge, heritage and cultures of Australia’s First Peoples.\n\n### International Organisations {#international-organisations}\n\n- Common Language Resources and Technology Infrastructure ([CLARIN](https://www.clarin.eu/)) is a research infrastructure that was initiated from the vision that all digital language resources and tools from all over Europe and beyond are accessible through an online environment for the support of researchers in the humanities and social sciences.\n\n- Endangered Languages Documentation Program ([ELDP](https://www.eldp.net/)) supports the documentation and preservation of endangered languages through granting, training and outreach activities. The collections compiled through our funding are freely accessible at the [Endangered Languages Archive](#language-archives).\n\n- The Linguistic Data Consortium ([LDC](https://www.ldc.upenn.edu/)) is an open consortium of universities, libraries, corporations and government research laboratories. LDC was formed in 1992 to address the critical data shortage then facing language technology research and development. Initially, LDC's primary role was as a repository and distribution point for language resources. but with the help of its members, LDC has grown into an organization that creates and distributes a wide array of language resources. \n\n### Publications {#publications}\n\n- [The Open Handbook of Linguistic Data Management](https://direct.mit.edu/books/book/5244/The-Open-Handbook-of-Linguistic-Data-Management) \nEdited by Andrea L. Berez-Kroeker, Bradley McDonnell, Eve Koller, Lauren B. Collister. MIT Press 2022.\n\n    \"A guide to principles and methods for the management, archiving, sharing, and citing of linguistic research data, especially digital data. “Doing language science” depends on collecting, transcribing, annotating, analyzing, storing, and sharing linguistic research data. This volume offers a guide to linguistic data management, engaging with current trends toward the transformation of linguistics into a more data-driven and reproducible scientific endeavor. It offers both principles and methods, presenting the conceptual foundations of linguistic data management and a series of case studies, each of which demonstrates a concrete application of abstract principles in a current practice.\"\n\n    This material is Open Access.\n\n- [Language Documentation and Conservation](https://nflrc.hawaii.edu/ldc/)\n\n    “LD&C publishes papers on all topics related to language documentation and conservation, including, but not limited to, the goals of language documentation, data management, fieldwork methods, ethical issues, orthography design, reference grammar design, lexicography, methods of assessing ethnolinguistic vitality, archiving matters, language planning, areal survey reports, short field reports on endangered or underdocumented languages, reports on language maintenance, preservation, and revitalization efforts, plus software, hardware, and book reviews.”\n\n    LD&C is an Open Access journal.\n\n- [Living languages/Lenguas vivas/Línguas vivas journal](https://scholarworks.umass.edu/livinglanguages/)\n\n    “Living Languages is an international, multilingual journal dedicated to topics in language revitalization and sustainability. The goal of the journal is to promote scholarly work and experience-sharing in the field. The primary focus is on bringing together language revitalization practitioners from a diversity of backgrounds, whether academic or not, within a peer-reviewed publication venue that is not limited to academic contributions and is inclusive of a diversity of perspectives and forms of expression.”\n\n    *Living Languages* is an Open Access journal.\n\n- [Pacific Linguistics](http://sealang.net/archives/pl/): a digital archive of many Pacific Linguistics publications up to 2012.\n\n### Tools {#tools}\n\n- [Audacity](https://www.audacityteam.org/): Audacity is a free, easy-to-use, multi-track audio editor and recorder for Windows, macOS, GNU/Linux and other operating systems.\n- [ELAN](https://archive.mpi.nl/tla/elan): a tool for making time-aligned annotations on audio and video recordings.\n- [Praat](https://www.fon.hum.uva.nl/praat/): Free software for doing phonetics by computer.\n- [FieldWorks](https://software.sil.org/fieldworks/): Software tools for managing linguistic and cultural data. FieldWorks supports tasks ranging from the initial entry of collected data through to the preparation of data for publication, including dictionary development, interlinearization of texts, morphological analysis, and other publications.\n- [Toolbox](https://software.sil.org/toolbox/): Toolbox is a data management and analysis tool for field linguists. It is especially useful for maintaining lexical data, and for parsing and interlinearizing text, but it can be used to manage virtually any kind of data.\n-  [AntConc](https://www.laurenceanthony.net/software/antconc/): A freeware corpus analysis toolkit for concordancing and text analysis.\n\n"},{"title":"Technologies","slug":"technologies","content":"LDaCA is basing its data storage on the two technologies described below. The overall approach is informed by the [Arkisto](https://arkisto-platform.github.io/) platform, taking the view that research data has interest and value that extends beyond funding cycles and its long term preservation and accessibility must continue to be managed. [This presentation](https://ptsefton.com/2022/02/18/hass_rdc_tech_advisory/index.html) gives further details of the technical architecture.\n\n## Research Object Crates ([RO-Crate](https://www.researchobject.org/ro-crate/))\n\nA Research Object ([RO](https://www.researchobject.org/)) is a structured archive of all the items that contributed to the research outcome, including their identifiers, provenance, relations and annotations. [RO-Crate](https://www.researchobject.org/ro-crate/) is a lightweight approach to packaging research data with their metadata. It is based on [schema.org](https://schema.org/) annotations in [JSON-LD](https://json-ld.org/), and aims to make best-practice in formal metadata description accessible and practical for use in a wide variety of situations. While RO-Crates can be considered general-purpose containers of arbitrary data and open-ended metadata, in practical use within a particular domain, application or framework, it is beneficial to further constrain RO-Crate to a specific profile: a set of conventions, types and properties that one minimally can require and expect to be present in that subset of RO-Crates. LDaCA is developing such a profile to be used for language data.\n\n\n## Oxford Common File Layout ([OCFL](https://ocfl.io/))\n\nOCFL is a specification for laying out digital collections on file or object storage. It is designed with long-term preservation principles in mind and does not rely on specialised software. Amongst the benefits of using OCFL with RO-Crate objects are:\n- completeness: a repository can be re-indexed from the files it stores\n- versioning: repositories can make changes to objects and still allow their history to persist\n<br />\n<br />\n\n<br />\n<br />\n\nBack to [Background](../background/)\n"}]}},"__N_SSG":true}